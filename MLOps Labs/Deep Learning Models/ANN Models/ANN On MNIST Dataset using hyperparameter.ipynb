{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a2c007e4-1328-4e2e-985c-1965e46f9eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.datasets import mnist\n",
    "import keras_tuner as kt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7f4555e6-6ce6-42f6-8297-fd375939b8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Normalize the pixel values to be between 0 and 1\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "# Flatten the images for the ANN\n",
    "x_train = x_train.reshape(x_train.shape[0], 28 * 28)\n",
    "x_test = x_test.reshape(x_test.shape[0], 28 * 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d8cc61d6-6000-4bbb-b8fa-19fd4c52684e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a function to create the model for Keras Tuner\n",
    "def build_model(hp):\n",
    "    model = Sequential([\n",
    "        Dense(hp.Int('units_1', min_value=64, max_value=512, step=64), \n",
    "              activation='relu', kernel_regularizer=l2(0.01), input_shape=(28 * 28,)),\n",
    "        Dropout(hp.Float('dropout_1', min_value=0.2, max_value=0.5, step=0.1)),\n",
    "        Dense(hp.Int('units_2', min_value=64, max_value=512, step=64), activation='relu', kernel_regularizer=l2(0.01)),\n",
    "        Dropout(hp.Float('dropout_2', min_value=0.2, max_value=0.5, step=0.1)),\n",
    "        Dense(10, activation='softmax')  # 10 classes for MNIST digits\n",
    "    ])\n",
    "    \n",
    "    # Choose the optimizer and learning rate\n",
    "    optimizer = hp.Choice('optimizer', values=['adam', 'sgd'])\n",
    "    if optimizer == 'adam':\n",
    "        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=hp.Float('learning_rate', min_value=1e-5, max_value=1e-2, sampling='LOG')),\n",
    "                      loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    else:\n",
    "        model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=hp.Float('learning_rate', min_value=1e-5, max_value=1e-2, sampling='LOG')),\n",
    "                      loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7b2d188d-8a36-41ad-a45d-8ea8e4f51a5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloading Tuner from keras_tuner\\mnist_ANN\\tuner0.json\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the Keras Tuner RandomSearch object\n",
    "tuner = kt.RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=5,  # You can increase this to explore more combinations\n",
    "    executions_per_trial=3,  # Number of models to train for each trial\n",
    "    directory='keras_tuner',\n",
    "    project_name='mnist_ANN'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4a6f1ba2-23c8-46c5-a83e-711c8cd23881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: \n",
      "Units in Layer 1: 128\n",
      "Units in Layer 2: 512\n",
      "Dropout 1: 0.4\n",
      "Dropout 2: 0.2\n",
      "Optimizer: adam\n",
      "Learning rate: 0.00012532888305788302\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.9565 - loss: 0.2955\n",
      "Test accuracy of the best model: 0.9643999934196472\n"
     ]
    }
   ],
   "source": [
    "# Set up early stopping to avoid overfitting during hyperparameter search\n",
    "early_stopping_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "# Perform the hyperparameter search\n",
    "tuner.search(x_train, y_train, epochs=20, batch_size=32, validation_data=(x_test, y_test), callbacks=[early_stopping_callback])\n",
    "\n",
    "# Get the best model and its hyperparameters\n",
    "best_model = tuner.get_best_models(num_models=1)[0]\n",
    "best_hyperparameters = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "print(\"Best Hyperparameters: \")\n",
    "print(f\"Units in Layer 1: {best_hyperparameters.get('units_1')}\")\n",
    "print(f\"Units in Layer 2: {best_hyperparameters.get('units_2')}\")\n",
    "print(f\"Dropout 1: {best_hyperparameters.get('dropout_1')}\")\n",
    "print(f\"Dropout 2: {best_hyperparameters.get('dropout_2')}\")\n",
    "print(f\"Optimizer: {best_hyperparameters.get('optimizer')}\")\n",
    "print(f\"Learning rate: {best_hyperparameters.get('learning_rate')}\")\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "test_loss, test_accuracy = best_model.evaluate(x_test, y_test)\n",
    "print(f'Test accuracy of the best model: {test_accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3fdf506-4fb2-4064-84ef-e398a9437ae3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf_env)",
   "language": "python",
   "name": "tf_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
