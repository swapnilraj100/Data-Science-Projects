\documentclass[12pt,a4paper]{article}
\usepackage[margin=1in]{geometry}
\usepackage{setspace}
\usepackage{tocloft}
\usepackage[hidelinks]{hyperref}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{booktabs}

\setstretch{1.5}

\begin{document}
\begin{titlepage}
  \thispagestyle{empty}

  \begin{center}
    % Logo
    \includegraphics[height=3.4cm]{National_Institute_of_Technology,_Jamshedpur_Logo.png}\\[0.8cm]

    {\Large \textsc{National Institute of Technology, Jamshedpur}}\\[0.4cm]
    {\large Department of Computer Science Engineering}\\[1.0cm]

    \rule{\textwidth}{0.6pt}\\[0.9cm]

    {\LARGE \bfseries Semantic Analytics on Code-Mixed Text}\\[0.25cm]
    {\large \itshape M.Tech Major Project}\\[0.6cm]

    \rule{0.6\textwidth}{0.6pt}\\[1.2cm]
  \end{center}

  % Two neat blocks
  \vspace{0.3cm}
  \noindent
  \begin{minipage}[t]{0.48\textwidth}
    \textbf{Submitted By}\\
    Swapnil Raj\\
    Roll No: 2024PGCSDS01
  \end{minipage}
  \hfill
  \begin{minipage}[t]{0.48\textwidth}
    \raggedleft
    \textbf{Under the Guidance of}\\
    Dr.~Jitesh Pradhan\\
    Assistant Professor\\
    Department of CSE,
    NIT Jamshedpur
  \end{minipage}

  \vfill
  \begin{center}
    {\large \today}
  \end{center}
\end{titlepage}





% -------- Front matter in roman numerals --------
\pagenumbering{roman}

% Table of Contents
\tableofcontents
\clearpage

% Unnumbered sections
\section*{Declaration}
\addcontentsline{toc}{section}{Declaration}
I hereby declare that the work entitled \textbf{``Semantic Analytics on Code-Mixed Text''} is my original research work carried out under the supervision of Dr.~.Jitesh Pradhan, NIT Jamshedpur This report has not been submitted elsewhere for the award of any degree or diploma.

\vspace{2cm}
\hfill (Swapnil Raj)

\clearpage

\section*{Acknowledgment}
\addcontentsline{toc}{section}{Acknowledgment}
I would like to express my sincere gratitude to my supervisor, Dr.~Jitesh Pradhan, for his continuous support, guidance, and insightful feedback throughout this project. I also thank my peers and the Dravidian CodeMix research community for providing valuable datasets and baselines. Finally, I am indebted to my family and friends for their encouragement and motivation.

\clearpage

\section*{Abstract}
\addcontentsline{toc}{section}{Abstract}
This project investigates \textbf{sentiment analysis on code-mixed Tamil-English text}, a challenging task due to linguistic diversity, transliteration, and noisy social media data. 

We benchmarked multiple approaches:
\begin{enumerate}
    \item \textbf{Classical Models:} TF-IDF with Linear SVM and Logistic Regression.
    \item \textbf{Deep Models:} BiLSTM, CNN+BiLSTM.
    \item \textbf{Attention-based Hybrids:} Attention-BiLSTM and Attention-BiLSTM+CNN.
    \item \textbf{Transformer Models:} Google MuRIL, mBERT, IndicBERT, and XLM-RoBERTa.
\end{enumerate}

The dataset used was the FIRE 2021 Dravidian-CodeMix Tamil-English sentiment dataset. The primary evaluation metric was \textbf{Weighted F1-score}, chosen to handle class imbalance. Results show that transformer-based approaches significantly outperform classical and deep learning models, with MuRIL and XLM-RoBERTa achieving the highest Weighted F1-scores.

\clearpage

% -------- Main matter in arabic numerals --------
\pagenumbering{arabic}

\section{Introduction}
\subsection{Motivation}
In multilingual societies like India, social media users frequently mix languages (code-mixing) within a single sentence. This introduces challenges for natural language processing models. Sentiment analysis on code-mixed data is crucial for opinion mining, hate-speech detection, and customer feedback analysis.

\subsection{Objectives}
\begin{itemize}
    \item Build a standardized pipeline for sentiment analysis on Tamil-English code-mixed text.
    \item Compare the performance of classical, deep, hybrid, and transformer-based models.
    \item Report results using \textbf{Weighted F1-score}.
    \item Identify error patterns unique to code-mixed sentiment analysis.
\end{itemize}

\section{Related Work}
Previous research in sentiment analysis focused on monolingual English and Hindi corpora. With the introduction of the Dravidian-CodeMix shared tasks at FIRE 2020 and 2021, sentiment analysis of code-mixed Tamil, Malayalam, and Kannada gained momentum. Studies show that while classical models like SVM perform decently, transformer models such as XLM-RoBERTa and MuRIL set new benchmarks.

\section{Materials and Methods}
\subsection{Dataset Description}
We used the FIRE 2021 Dravidian-CodeMix Tamil-English YouTube comments dataset. It contains:
\begin{itemize}
    \item \textbf{Total samples:} 44,020 comments.
    \item \textbf{Classes:} Positive, Negative, Neutral, Mixed feelings, Not-in-intended-language.
    \item \textbf{Splits:} 90\% training, 5\% validation, 5\% testing.
\end{itemize}

\subsection{Methodology}
\textbf{Pipeline:}
\begin{enumerate}
    \item Data preprocessing: cleaning, normalization, preserving emojis and sentiment cues.
    \item Feature extraction: TF-IDF for classical models; embeddings for neural models.
    \item Model training: classical ML, BiLSTM/CNN hybrids, attention-based models, transformer fine-tuning.
    \item Evaluation: dev/test weighted F1, classification reports, confusion matrices.
\end{enumerate}

\subsection{Performance Evaluation}
The main metric used is the \textbf{Weighted F1-score}, defined as:
\[
F1_{weighted} = \sum_{i=1}^{L} \frac{n_i}{N} \cdot F1_i
\]
where $n_i$ is the number of samples in class $i$, and $N$ is the total number of samples.

\section{Results}
\subsection{Overall Comparison}
\begin{table}[h]
\centering
\caption{Weighted F1-scores of models (dev/test).}
\begin{tabular}{lcc}
\toprule
Model & Dev F1 & Test F1 \\
\midrule
TF-IDF + Linear SVM & 0.62 & 0.61 \\
TF-IDF + Logistic Regression & 0.60 & 0.59 \\
BiLSTM & 0.65 & 0.63 \\
CNN + BiLSTM & 0.66 & 0.64 \\
Attention-BiLSTM & 0.67 & 0.65 \\
Attention-BiLSTM + CNN & 0.68 & 0.66 \\
MuRIL & 0.72 & 0.71 \\
mBERT & 0.70 & 0.69 \\
IndicBERT & 0.69 & 0.68 \\
XLM-RoBERTa & 0.74 & 0.73 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Error Analysis}
Frequent errors include confusion between \textit{Positive} and \textit{Neutral}, and misclassification of code-switched Romanized Tamil words. Emojis and slang often mislead classical models, but transformers handle them better.

\section{Conclusion}
In this project, we built an end-to-end semantic analytics pipeline for Tamil-English code-mixed sentiment analysis. Our results confirm that multilingual transformers like MuRIL and XLM-RoBERTa outperform other approaches. Classical models remain viable for quick baselines, while attention-based BiLSTMs provide a strong middle ground. Future work includes training code-mix aware transformers, applying data augmentation, and extending experiments to other Dravidian languages.

\clearpage
\section*{Bibliography}
\addcontentsline{toc}{section}{Bibliography}
\begin{thebibliography}{9}
\bibitem{fire2021}
Chakravarthi, Bharathi Raja, et al. ``Findings of the Sentiment Analysis of Dravidian Languages in Code-Mixed Text.'' FIRE 2021.
\bibitem{muril}
Khanuja, Simran, et al. ``MuRIL: Multilingual Representations for Indian Languages.'' arXiv preprint arXiv:2103.10730, 2021.
\bibitem{xlmr}
Conneau, Alexis, et al. ``Unsupervised Cross-lingual Representation Learning at Scale.'' ACL 2020.
\end{thebibliography}

\end{document}
