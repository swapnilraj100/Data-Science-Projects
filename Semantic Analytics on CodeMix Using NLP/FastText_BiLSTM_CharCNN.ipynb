{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "fxk0fvTF8uEg",
        "outputId": "c2d6e0ac-057a-42d2-a19f-054e0160cf1d"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-f40cbd92-c5b1-43fa-8d41-fea94db3f12a\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-f40cbd92-c5b1-43fa-8d41-fea94db3f12a\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving tamil_sentiment_full.csv to tamil_sentiment_full (1).csv\n"
          ]
        }
      ],
      "source": [
        "# Cell 3 — upload local file via browser\n",
        "from google.colab import files\n",
        "uploaded = files.upload()   # choose your tamil_sentiment_full.csv from your machine\n",
        "# After upload, file will be in current working directory, e.g. '/content/tamil_sentiment_full.csv'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wMvxDakGDl7u"
      },
      "outputs": [],
      "source": [
        "# Step 1: Install necessary libraries\n",
        "!pip install torch scikit-learn gensim tensorboardX pandas tqdm -q\n",
        "\n",
        "# Step 2: The full Python script\n",
        "import os\n",
        "import json\n",
        "import random\n",
        "from collections import Counter, defaultdict\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Dict, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from gensim.models import FastText\n",
        "from tqdm import tqdm\n",
        "from tensorboardX import SummaryWriter"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# uninstall then install a fresh wheel\n",
        "!pip uninstall -y numpy\n",
        "!pip cache purge\n",
        "\n",
        "# install a stable release (choose one appropriate for your Python version)\n",
        "!pip install numpy==1.26.4 --no-cache-dir"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "Xs3Hyawg4CGc",
        "outputId": "562f672c-6415-4486-fe18-7bc67104dc98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: numpy 1.26.4\n",
            "Uninstalling numpy-1.26.4:\n",
            "  Successfully uninstalled numpy-1.26.4\n",
            "Files removed: 24\n",
            "Collecting numpy==1.26.4\n",
            "  Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m86.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m172.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "tsfresh 0.21.1 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.13.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.26.4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "8658b42f5f124355b55443b0b2716ca6"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------\n",
        "# Config - edit these\n",
        "# -------------------------\n",
        "@dataclass\n",
        "class CFG:\n",
        "    # Data / paths\n",
        "    # This path works directly with the file uploaded to Colab's root session storage\n",
        "    data_csv: str = \"tamil_sentiment_full.csv\"\n",
        "    labels_json: str = \"data/labels.json\"\n",
        "    output_dir: str = \"outputs_bilstm\"\n",
        "    fasttext_model_path: str = \"outputs_bilstm/fasttext_tamil.model\"\n",
        "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    # Model\n",
        "    max_length: int = 100\n",
        "    embedding_dim: int = 300\n",
        "    hidden_dim: int = 256\n",
        "    n_layers: int = 2\n",
        "    dropout: float = 0.4\n",
        "\n",
        "    # Training\n",
        "    epochs: int = 10\n",
        "    batch_size: int = 32\n",
        "    lr: float = 1e-3\n",
        "    weight_decay: float = 1e-5\n",
        "    warmup_steps: int = 100\n",
        "    seed: int = 42\n",
        "\n",
        "    # Loss options (Focal Loss is enabled by default)\n",
        "    use_class_weight: bool = False\n",
        "    use_oversample: bool = False\n",
        "    use_focal: bool = True\n",
        "    focal_gamma: float = 2.0\n",
        "    focal_alpha: List[float] = None\n",
        "\n",
        "    # FastText\n",
        "    ft_dim: int = 300\n",
        "    ft_window: int = 5\n",
        "    ft_min_count: int = 2\n",
        "    ft_epochs: int = 10\n",
        "\n",
        "cfg = CFG()\n",
        "\n",
        "# Create directories for outputs\n",
        "os.makedirs(cfg.output_dir, exist_ok=True)\n",
        "os.makedirs(\"data\", exist_ok=True)"
      ],
      "metadata": {
        "id": "k0fvzK9R4RKu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------\n",
        "# Utilities\n",
        "# -------------------------\n",
        "def seed_everything(seed=42):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "\n",
        "seed_everything(cfg.seed)\n",
        "\n",
        "def read_data(csv_path: str) -> Tuple[List[Dict], Dict[str, int]]:\n",
        "    \"\"\"\n",
        "    Reads the tab-separated CSV. Creates and saves a label_map if one isn't found.\n",
        "    \"\"\"\n",
        "    print(f\"Reading data from {csv_path}...\")\n",
        "    # Handle potential errors if the file is not found\n",
        "    if not os.path.exists(csv_path):\n",
        "        raise FileNotFoundError(f\"Dataset not found at {csv_path}. Please make sure you have uploaded it to the Colab session.\")\n",
        "\n",
        "    df = pd.read_csv(csv_path, sep='\\\\t', header=None, names=['label', 'text'], engine='python')\n",
        "    df.dropna(subset=['text', 'label'], inplace=True)\n",
        "    df['text'] = df['text'].astype(str)\n",
        "\n",
        "    unique_labels = sorted(df['label'].unique())\n",
        "    label_map = {label: i for i, label in enumerate(unique_labels)}\n",
        "\n",
        "    if not os.path.exists(cfg.labels_json):\n",
        "        with open(cfg.labels_json, 'w') as f:\n",
        "            json.dump(label_map, f, indent=2)\n",
        "        print(f\"Created and saved label map to {cfg.labels_json}\")\n",
        "\n",
        "    data = df.to_dict(orient=\"records\")\n",
        "    return data, label_map\n",
        "\n",
        "# -------------------------\n",
        "# Vocabulary Builder\n",
        "# -------------------------\n",
        "class Vocabulary:\n",
        "    def __init__(self, texts: List[str], min_freq=2):\n",
        "        self.word2idx = {'<pad>': 0, '<unk>': 1}\n",
        "        self.idx2word = {0: '<pad>', 1: '<unk>'}\n",
        "        self.pad_token_id = 0\n",
        "        self.unk_token_id = 1\n",
        "\n",
        "        word_counts = Counter(word for text in texts for word in text.split())\n",
        "\n",
        "        for word, count in word_counts.items():\n",
        "            if count >= min_freq:\n",
        "                idx = len(self.word2idx)\n",
        "                self.word2idx[word] = idx\n",
        "                self.idx2word[idx] = word\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.word2idx)\n",
        "\n",
        "    def text_to_sequence(self, text: str) -> List[int]:\n",
        "        return [self.word2idx.get(word, self.unk_token_id) for word in text.split()]\n",
        "\n",
        "# -------------------------\n",
        "# Dataset\n",
        "# -------------------------\n",
        "class CommentsDataset(Dataset):\n",
        "    def __init__(self, records: List[Dict], label_map: Dict[str,int], vocab: Vocabulary, max_len=128):\n",
        "        self.vocab = vocab\n",
        "        self.max_len = max_len\n",
        "        self.label_map = label_map\n",
        "        self.samples = []\n",
        "        for rec in records:\n",
        "            text = str(rec[\"text\"])\n",
        "            if text.strip()==\"\":\n",
        "                text = \"<pad>\"\n",
        "            label = self.label_map[rec[\"label\"]]\n",
        "            self.samples.append((text, label))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text, label = self.samples[idx]\n",
        "        seq = self.vocab.text_to_sequence(text)\n",
        "        if len(seq) < self.max_len:\n",
        "            seq.extend([self.vocab.pad_token_id] * (self.max_len - len(seq)))\n",
        "        else:\n",
        "            seq = seq[:self.max_len]\n",
        "        return {\"text\": torch.tensor(seq, dtype=torch.long), \"label\": torch.tensor(label, dtype=torch.long)}\n"
      ],
      "metadata": {
        "id": "7ikakMQu4qcg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------\n",
        "# FastText training helper\n",
        "# -------------------------\n",
        "def train_fasttext(sentences: List[str], save_path: str, dim=300, window=5, min_count=2, epochs=10):\n",
        "    if os.path.exists(save_path):\n",
        "        print(f\"FastText model found at {save_path}, loading...\")\n",
        "        return FastText.load(save_path)\n",
        "\n",
        "    print(\"Training new FastText model...\")\n",
        "    tokenized = [s.split() for s in sentences]\n",
        "    ft = FastText(sentences=tokenized, vector_size=dim, window=window, min_count=min_count, workers=os.cpu_count(), epochs=epochs)\n",
        "    ft.save(save_path)\n",
        "    print(f\"FastText model saved to {save_path}\")\n",
        "    return"
      ],
      "metadata": {
        "id": "xsgcFhWJ4wat"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------\n",
        "# Model: BiLSTM + Attention\n",
        "# -------------------------\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, hidden_dim):\n",
        "        super(Attention, self).__init__()\n",
        "        self.attn = nn.Linear(hidden_dim, 1, bias=False)\n",
        "\n",
        "    def forward(self, lstm_output):\n",
        "        attn_weights = F.softmax(self.attn(lstm_output), dim=1)\n",
        "        context_vector = torch.sum(attn_weights * lstm_output, dim=1)\n",
        "        return context_vector\n",
        "\n",
        "class BiLSTMAttentionClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, n_layers, num_labels, dropout):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim // 2, num_layers=n_layers,\n",
        "                            bidirectional=True, batch_first=True, dropout=dropout if n_layers > 1 else 0)\n",
        "        self.attention = Attention(hidden_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.fc = nn.Linear(hidden_dim, num_labels)\n",
        "\n",
        "    def init_weights_from_fasttext(self, ft_model, vocab):\n",
        "        print(\"Initializing embedding layer with FastText vectors...\")\n",
        "        weights_matrix = np.zeros((len(vocab), cfg.embedding_dim))\n",
        "        found_count = 0\n",
        "        for word, i in vocab.word2idx.items():\n",
        "            if word in ft_model.wv:\n",
        "                weights_matrix[i] = ft_model.wv[word]\n",
        "                found_count += 1\n",
        "\n",
        "        self.embedding.weight.data.copy_(torch.from_numpy(weights_matrix))\n",
        "        print(f\"Found {found_count}/{len(vocab)} words in FastText model.\")\n",
        "\n",
        "    def forward(self, text):\n",
        "        embedded = self.embedding(text)\n",
        "        lstm_output, _ = self.lstm(embedded)\n",
        "        context_vector = self.attention(lstm_output)\n",
        "        dropped_out = self.dropout(context_vector)\n",
        "        logits = self.fc(dropped_out)\n",
        "        return logits, None"
      ],
      "metadata": {
        "id": "Q15FG1Df41TL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------\n",
        "# Loss implementation (Focal Loss)\n",
        "# -------------------------\n",
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, gamma=2.0, alpha=None, reduction='mean', device='cpu'):\n",
        "        super().__init__()\n",
        "        self.gamma = gamma\n",
        "        if alpha is not None:\n",
        "            self.alpha = torch.tensor(alpha, dtype=torch.float32).to(device)\n",
        "        else:\n",
        "            self.alpha = None\n",
        "        self.reduction = reduction\n",
        "\n",
        "    def forward(self, logits, targets):\n",
        "        ce = F.cross_entropy(logits, targets, reduction='none')\n",
        "        pt = torch.exp(-ce)\n",
        "        loss = ((1 - pt) ** self.gamma) * ce\n",
        "        if self.alpha is not None:\n",
        "            a = self.alpha[targets]\n",
        "            loss = a * loss\n",
        "        return loss.mean() if self.reduction == 'mean' else loss.sum()\n"
      ],
      "metadata": {
        "id": "TC00F6W646hh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------\n",
        "# Training & evaluation loops\n",
        "# -------------------------\n",
        "def evaluate(model, dataloader, device, id2label):\n",
        "    model.eval()\n",
        "    y_true, y_pred = [], []\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(dataloader, desc=\"Eval\", leave=False):\n",
        "            text = batch['text'].to(device)\n",
        "            labels = batch['label'].cpu().numpy().tolist()\n",
        "            logits, _ = model(text)\n",
        "            preds = torch.argmax(logits, dim=1).cpu().numpy().tolist()\n",
        "            y_true.extend(labels)\n",
        "            y_pred.extend(preds)\n",
        "    report = classification_report(y_true, y_pred, output_dict=True, zero_division=0, target_names=list(id2label.values()))\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    return report, cm\n",
        "\n",
        "def train_loop(train_loader, val_loader, model, cfg, class_weights=None, id2label=None):\n",
        "    device = cfg.device\n",
        "    model.to(device)\n",
        "    print(f\"Training on {device}...\")\n",
        "\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)\n",
        "\n",
        "    alpha = class_weights / class_weights.sum()\n",
        "    criterion = FocalLoss(gamma=cfg.focal_gamma, alpha=alpha, device=cfg.device)\n",
        "\n",
        "    best_macro, best_ckpt = -1.0, None\n",
        "    for epoch in range(cfg.epochs):\n",
        "        model.train()\n",
        "        pbar = tqdm(train_loader, desc=f\"Train E{epoch+1}/{cfg.epochs}\")\n",
        "        for batch in pbar:\n",
        "            text, labels = batch['text'].to(device), batch['label'].to(device)\n",
        "            optimizer.zero_grad()\n",
        "            logits, _ = model(text)\n",
        "            loss = criterion(logits, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            pbar.set_postfix(loss=loss.item())\n",
        "\n",
        "        report, _ = evaluate(model, val_loader, device, id2label)\n",
        "        macro_f1 = report['macro avg']['f1-score']\n",
        "        print(f\"Epoch {epoch+1} -> Val Macro F1: {macro_f1:.4f}\")\n",
        "\n",
        "        if macro_f1 > best_macro:\n",
        "            best_macro = macro_f1\n",
        "            ckpt_path = os.path.join(cfg.output_dir, \"best_model.pt\")\n",
        "            torch.save({\"model_state_dict\": model.state_dict()}, ckpt_path)\n",
        "            print(f\"✅ New best model saved with Macro F1: {macro_f1:.4f}\")\n",
        "\n",
        "    return best_ckpt"
      ],
      "metadata": {
        "id": "TzPDaQFx5w4Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------\n",
        "# Main pipeline orchestration\n",
        "# -------------------------\n",
        "def main():\n",
        "    # Load the full dataset into a pandas DataFrame first\n",
        "    if not os.path.exists(cfg.data_csv):\n",
        "        raise FileNotFoundError(f\"Dataset not found at {cfg.data_csv}. Please make sure you have uploaded it.\")\n",
        "\n",
        "    df = pd.read_csv(cfg.data_csv, sep='\\\\t', header=None, names=['label', 'text'], engine='python')\n",
        "    df.dropna(subset=['text', 'label'], inplace=True)\n",
        "    df['text'] = df['text'].astype(str)\n",
        "\n",
        "    # --- FIX: Filter out classes with fewer than min_class_samples ---\n",
        "    print(\"Original dataset size:\", len(df))\n",
        "    label_counts = df['label'].value_counts()\n",
        "\n",
        "    # Define a minimum number of samples for a class to be included\n",
        "    MIN_SAMPLES = 3\n",
        "    classes_to_keep = label_counts[label_counts >= MIN_SAMPLES].index.tolist()\n",
        "\n",
        "    if len(classes_to_keep) < len(label_counts):\n",
        "        print(f\"Filtering out classes with less than {MIN_SAMPLES} samples.\")\n",
        "        removed_classes = label_counts[label_counts < MIN_SAMPLES].index.tolist()\n",
        "        print(\"--> Removed classes:\", removed_classes)\n",
        "        df = df[df['label'].isin(classes_to_keep)]\n",
        "        print(\"Cleaned dataset size:\", len(df))\n",
        "    else:\n",
        "        print(\"All classes have sufficient samples for a stratified split.\")\n",
        "    # --- END FIX ---\n",
        "\n",
        "    # Create a new, clean label map from the filtered data\n",
        "    unique_labels = sorted(df['label'].unique())\n",
        "    label_map = {label: i for i, label in enumerate(unique_labels)}\n",
        "    id2label = {v: k for k, v in label_map.items()}\n",
        "    num_classes = len(label_map)\n",
        "    print(f\"\\nProceeding with {num_classes} classes.\")\n",
        "\n",
        "    # Convert the cleaned DataFrame back to a list of records for the rest of the script\n",
        "    data = df.to_dict(orient=\"records\")\n",
        "    texts = [r['text'] for r in data]\n",
        "    labels = [label_map[r['label']] for r in data]\n",
        "\n",
        "    # Now, we can safely stratify the split on the cleaned data\n",
        "    train_idx, test_idx = train_test_split(range(len(data)), test_size=0.15, random_state=cfg.seed, stratify=labels)\n",
        "    train_idx, val_idx = train_test_split(train_idx, test_size=0.15, random_state=cfg.seed, stratify=[labels[i] for i in train_idx])\n",
        "\n",
        "    train_records = [data[i] for i in train_idx]\n",
        "    val_records = [data[i] for i in val_idx]\n",
        "    test_records = [data[i] for i in test_idx]\n",
        "\n",
        "    print(f\"Data split -> Train: {len(train_records)}, Val: {len(val_records)}, Test: {len(test_records)}\")\n",
        "\n",
        "    vocab = Vocabulary([r['text'] for r in train_records], min_freq=cfg.ft_min_count)\n",
        "    print(f\"Vocabulary size: {len(vocab)}\")\n",
        "\n",
        "    cnt = Counter([label_map[r['label']] for r in train_records])\n",
        "    class_counts = np.array([cnt.get(i, 0) for i in range(num_classes)])\n",
        "    class_weights = (1.0 / (class_counts + 1e-9))\n",
        "    class_weights = class_weights / class_weights.sum() * num_classes\n",
        "    print(\"Class counts (train set):\", dict(sorted(cnt.items())))\n",
        "\n",
        "    ft_model = train_fasttext([r['text'] for r in data], cfg.fasttext_model_path, dim=cfg.ft_dim, epochs=cfg.ft_epochs)\n",
        "\n",
        "    model = BiLSTMAttentionClassifier(vocab_size=len(vocab), embedding_dim=cfg.embedding_dim, hidden_dim=cfg.hidden_dim,\n",
        "                                      n_layers=cfg.n_layers, num_labels=num_classes, dropout=cfg.dropout)\n",
        "    model.init_weights_from_fasttext(ft_model, vocab)\n",
        "\n",
        "    train_loader, val_loader = build_dataloaders(train_records, val_records, label_map, vocab, cfg, use_sampler=cfg.use_oversample)\n",
        "    writer = SummaryWriter(logdir=os.path.join(cfg.output_dir, \"tb_logs\"))\n",
        "\n",
        "    print(\"\\n--- Starting Training ---\")\n",
        "    best_ckpt = train_loop(train_loader, val_loader, model, cfg, class_weights=class_weights, id2label=id2label, writer=writer)\n",
        "    writer.close()\n",
        "\n",
        "    print(\"\\n--- Final Test Set Evaluation ---\")\n",
        "    if best_ckpt:\n",
        "        print(f\"Loading best checkpoint from: {best_ckpt}\")\n",
        "        ckpt = torch.load(best_ckpt, map_location=cfg.device)\n",
        "        model.load_state_dict(ckpt['model_state_dict'])\n",
        "\n",
        "        test_ds = CommentsDataset(test_records, label_map, vocab, max_len=cfg.max_length)\n",
        "        test_loader = DataLoader(test_ds, batch_size=cfg.batch_size*2)\n",
        "        report, cm = evaluate(model, test_loader, cfg.device, id2label)\n",
        "\n",
        "        print(\"\\nTest Report:\")\n",
        "        print(json.dumps(report, indent=2))\n",
        "        print(\"\\nConfusion Matrix:\\n\", cm)\n",
        "    else:\n",
        "        print(\"Training did not complete successfully, no checkpoint to evaluate.\")"
      ],
      "metadata": {
        "id": "GyaqSXwh490q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------\n",
        "# Training & evaluation loops\n",
        "# -------------------------\n",
        "def evaluate(model, dataloader, device, id2label):\n",
        "    model.eval()\n",
        "    y_true, y_pred = [], []\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(dataloader, desc=\"Eval\", leave=False):\n",
        "            text, labels = batch['text'].to(device), batch['label'].cpu().numpy().tolist()\n",
        "            logits, _ = model(text)\n",
        "            preds = torch.argmax(logits, dim=1).cpu().numpy().tolist()\n",
        "            y_true.extend(labels)\n",
        "            y_pred.extend(preds)\n",
        "    report = classification_report(y_true, y_pred, output_dict=True, zero_division=0, target_names=list(id2label.values()))\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    return report, cm\n",
        "\n",
        "def train_loop(train_loader, val_loader, model, cfg, class_weights=None, id2label=None, writer=None):\n",
        "    device = cfg.device\n",
        "    model.to(device)\n",
        "    print(f\"Training on {device}...\")\n",
        "\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)\n",
        "\n",
        "    criterion = None\n",
        "    if cfg.use_focal:\n",
        "        alpha = class_weights / class_weights.sum()\n",
        "        criterion = FocalLoss(gamma=cfg.focal_gamma, alpha=alpha, device=cfg.device)\n",
        "    else: # Default to CrossEntropy\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    best_macro, best_ckpt = -1.0, None\n",
        "    for epoch in range(cfg.epochs):\n",
        "        model.train()\n",
        "        pbar = tqdm(train_loader, desc=f\"Train E{epoch+1}/{cfg.epochs}\")\n",
        "        for batch in pbar:\n",
        "            text, labels = batch['text'].to(device), batch['label'].to(device)\n",
        "            optimizer.zero_grad()\n",
        "            logits, _ = model(text)\n",
        "            loss = criterion(logits, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            pbar.set_postfix(loss=loss.item())\n",
        "\n",
        "        report, _ = evaluate(model, val_loader, device, id2label)\n",
        "        macro_f1 = report['macro avg']['f1-score']\n",
        "        print(f\"Epoch {epoch+1} -> Val Macro F1: {macro_f1:.4f}\")\n",
        "\n",
        "        if writer: writer.add_scalar(\"eval/macro_f1\", macro_f1, epoch)\n",
        "\n",
        "        if macro_f1 > best_macro:\n",
        "            best_macro = macro_f1\n",
        "            ckpt_path = os.path.join(cfg.output_dir, \"best_model.pt\")\n",
        "            torch.save({\"model_state_dict\": model.state_dict()}, ckpt_path)\n",
        "            print(f\"✅ New best model saved with Macro F1: {macro_f1:.4f}\")\n",
        "\n",
        "    return best_ckpt\n",
        "\n",
        "def build_dataloaders(train_records, val_records, label_map, vocab, cfg):\n",
        "    train_ds = CommentsDataset(train_records, label_map, vocab, max_len=cfg.max_length)\n",
        "    val_ds = CommentsDataset(val_records, label_map, vocab, max_len=cfg.max_length)\n",
        "\n",
        "    train_loader_args = {\"batch_size\": cfg.batch_size, \"num_workers\": 2, \"pin_memory\": True}\n",
        "    if cfg.use_oversample:\n",
        "        print(\"Using WeightedRandomSampler for oversampling minority classes.\")\n",
        "        labels = [label_map[rec['label']] for rec in train_records]\n",
        "        cnt = Counter(labels)\n",
        "        sample_weights = [1.0 / cnt[lbl] for lbl in labels]\n",
        "        sampler = WeightedRandomSampler(weights=sample_weights, num_samples=len(sample_weights), replacement=True)\n",
        "        train_loader_args[\"sampler\"] = sampler\n",
        "    else:\n",
        "        train_loader_args[\"shuffle\"] = True\n",
        "\n",
        "    train_loader = DataLoader(train_ds, **train_loader_args)\n",
        "    val_loader = DataLoader(val_ds, batch_size=cfg.batch_size*2, shuffle=False, num_workers=2)\n",
        "    return train_loader, val_loader\n",
        "\n",
        "# -------------------------\n",
        "# Main pipeline orchestration\n",
        "# -------------------------\n",
        "def main():\n",
        "    if not os.path.exists(cfg.data_csv):\n",
        "        raise FileNotFoundError(f\"Dataset not found at {cfg.data_csv}. Please upload it to the Colab session first.\")\n",
        "\n",
        "    df = pd.read_csv(cfg.data_csv, sep='\\\\t', header=None, names=['label', 'text'], engine='python')\n",
        "    df.dropna(subset=['text', 'label'], inplace=True)\n",
        "    df['text'] = df['text'].astype(str)\n",
        "\n",
        "    # --- NEW ROBUST FIX STARTS HERE ---\n",
        "    print(\"Original dataset size:\", len(df))\n",
        "\n",
        "    # Convert value counts to a standard Python dictionary to avoid pandas indexing issues\n",
        "    label_counts_dict = df['label'].value_counts().to_dict()\n",
        "\n",
        "    # Identify classes to keep and remove by iterating through the dictionary\n",
        "    classes_to_keep = [label for label, count in label_counts_dict.items() if count >= cfg.min_class_samples]\n",
        "\n",
        "    if len(classes_to_keep) < len(label_counts_dict):\n",
        "        removed_classes = [label for label, count in label_counts_dict.items() if count < cfg.min_class_samples]\n",
        "        print(f\"\\nFiltering out classes with less than {cfg.min_class_samples} samples.\")\n",
        "        print(\"Removed classes:\", removed_classes)\n",
        "\n",
        "        # Filter the DataFrame using the list of classes to keep\n",
        "        df = df[df['label'].isin(classes_to_keep)]\n",
        "        print(\"Cleaned dataset size:\", len(df))\n",
        "    else:\n",
        "        print(\"All classes have sufficient samples.\")\n",
        "    # --- NEW ROBUST FIX ENDS HERE ---\n",
        "\n",
        "    # Create a new, clean label map from the filtered data\n",
        "    unique_labels = sorted(df['label'].unique())\n",
        "    label_map = {label: i for i, label in enumerate(unique_labels)}\n",
        "    id2label = {v: k for k, v in label_map.items()}\n",
        "    num_classes = len(label_map)\n",
        "\n",
        "    print(f\"\\nProcessing {num_classes} classes for training.\")\n",
        "\n",
        "    data = df.to_dict(orient=\"records\")\n",
        "    texts, labels = [r['text'] for r in data], [label_map[r['label']] for r in data]\n",
        "\n",
        "    # Now, we can safely stratify the split on the cleaned data\n",
        "    train_idx, test_idx = train_test_split(range(len(data)), test_size=0.15, random_state=cfg.seed, stratify=labels)\n",
        "    train_idx, val_idx = train_test_split(train_idx, test_size=0.15, random_state=cfg.seed, stratify=[labels[i] for i in train_idx])\n",
        "\n",
        "    train_records, val_records, test_records = [data[i] for i in train_idx], [data[i] for i in val_idx], [data[i] for i in test_idx]\n",
        "    print(f\"Data split -> Train: {len(train_records)}, Val: {len(val_records)}, Test: {len(test_records)}\")\n",
        "\n",
        "    vocab = Vocabulary([r['text'] for r in train_records], min_freq=cfg.ft_min_count)\n",
        "\n",
        "    cnt = Counter([label_map[r['label']] for r in train_records])\n",
        "    class_counts = np.array([cnt.get(i, 0) for i in range(num_classes)])\n",
        "    class_weights = 1.0 / (class_counts + 1e-9)\n",
        "\n",
        "    ft_model = train_fasttext([r['text'] for r in data], cfg.fasttext_model_path, dim=cfg.ft_dim, min_count=cfg.ft_min_count, epochs=cfg.ft_epochs)\n",
        "\n",
        "    model = BiLSTMAttentionClassifier(vocab_size=len(vocab), embedding_dim=cfg.embedding_dim, hidden_dim=cfg.hidden_dim,\n",
        "                                      n_layers=cfg.n_layers, num_labels=num_classes, dropout=cfg.dropout)\n",
        "    model.init_weights_from_fasttext(ft_model, vocab)\n",
        "\n",
        "    train_loader, val_loader = build_dataloaders(train_records, val_records, label_map, vocab, cfg)\n",
        "\n",
        "    writer = SummaryWriter(logdir=os.path.join(cfg.output_dir, \"tb_logs\"))\n",
        "\n",
        "    print(\"\\n--- Starting Training ---\")\n",
        "    best_ckpt = train_loop(train_loader, val_loader, model, cfg, class_weights=class_weights, id2label=id2label, writer=writer)\n",
        "    writer.close()\n",
        "\n",
        "    print(\"\\n--- Final Test Set Evaluation ---\")\n",
        "    if best_ckpt:\n",
        "        ckpt = torch.load(best_ckpt, map_location=cfg.device)\n",
        "        model.load_state_dict(ckpt['model_state_dict'])\n",
        "        test_ds = CommentsDataset(test_records, label_map, vocab, max_len=cfg.max_length)\n",
        "        test_loader = DataLoader(test_ds, batch_size=cfg.batch_size*2)\n",
        "        report, cm = evaluate(model, test_loader, cfg.device, id2label)\n",
        "\n",
        "        print(\"\\nTest Report:\")\n",
        "        print(json.dumps(report, indent=2))\n",
        "        print(\"\\nConfusion Matrix:\\n\", cm)\n",
        "    else:\n",
        "        print(\"Training did not complete successfully, no checkpoint to evaluate.\")"
      ],
      "metadata": {
        "id": "xI1oyKj15HD2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Single Colab cell: full pipeline (BiLSTM + FastText) with corrected evaluate()\n",
        "# Step 1: Install necessary libraries (uncomment if running fresh)\n",
        "# !pip install torch scikit-learn gensim tensorboardX pandas tqdm -q\n",
        "\n",
        "import os\n",
        "import json\n",
        "import random\n",
        "from collections import Counter\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Dict, Tuple\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from gensim.models import FastText\n",
        "from tqdm import tqdm\n",
        "from tensorboardX import SummaryWriter\n",
        "\n",
        "# -------------------------\n",
        "# Config - edit these\n",
        "# -------------------------\n",
        "@dataclass\n",
        "class CFG:\n",
        "    data_csv: str = \"tamil_sentiment_full.csv\"\n",
        "    output_dir: str = \"outputs_bilstm\"\n",
        "    fasttext_model_path: str = \"outputs_bilstm/fasttext_tamil.model\"\n",
        "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    min_class_samples: int = 3\n",
        "\n",
        "    # Model\n",
        "    max_length: int = 100\n",
        "    embedding_dim: int = 300\n",
        "    hidden_dim: int = 256\n",
        "    n_layers: int = 2\n",
        "    dropout: float = 0.4\n",
        "\n",
        "    # Training\n",
        "    epochs: int = 10\n",
        "    batch_size: int = 64\n",
        "    lr: float = 1e-3\n",
        "    seed: int = 42\n",
        "\n",
        "    # Loss options\n",
        "    use_focal: bool = True\n",
        "    focal_gamma: float = 2.0  # fixed\n",
        "\n",
        "    # FastText\n",
        "    ft_dim: int = 300\n",
        "    ft_min_count: int = 2\n",
        "    ft_epochs: int = 10\n",
        "\n",
        "cfg = CFG()\n",
        "\n",
        "os.makedirs(cfg.output_dir, exist_ok=True)\n",
        "os.makedirs(\"data\", exist_ok=True)\n",
        "\n",
        "# -------------------------\n",
        "# Utilities & helpers\n",
        "# -------------------------\n",
        "def seed_everything(seed=42):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "\n",
        "seed_everything(cfg.seed)\n",
        "\n",
        "class Vocabulary:\n",
        "    def __init__(self, texts: List[str], min_freq=2):\n",
        "        self.word2idx = {'<pad>': 0, '<unk>': 1}\n",
        "        self.pad_token_id = 0; self.unk_token_id = 1\n",
        "        word_counts = Counter(word for text in texts for word in text.split())\n",
        "        for word, count in word_counts.items():\n",
        "            if count >= min_freq:\n",
        "                self.word2idx[word] = len(self.word2idx)\n",
        "    def __len__(self): return len(self.word2idx)\n",
        "    def text_to_sequence(self, text: str) -> List[int]:\n",
        "        return [self.word2idx.get(word, self.unk_token_id) for word in text.split()]\n",
        "\n",
        "class CommentsDataset(Dataset):\n",
        "    def __init__(self, records: List[Dict], label_map: Dict[str,int], vocab: Vocabulary, max_len=128):\n",
        "        self.vocab = vocab; self.max_len = max_len; self.label_map = label_map\n",
        "        self.samples = [(str(rec[\"text\"]), self.label_map[rec[\"label\"]]) for rec in records]\n",
        "    def __len__(self): return len(self.samples)\n",
        "    def __getitem__(self, idx):\n",
        "        text, label = self.samples[idx]\n",
        "        seq = self.vocab.text_to_sequence(text)\n",
        "        if len(seq) < self.max_len:\n",
        "            seq.extend([self.vocab.pad_token_id] * (self.max_len - len(seq)))\n",
        "        else:\n",
        "            seq = seq[:self.max_len]\n",
        "        return {\"text\": torch.tensor(seq, dtype=torch.long), \"label\": torch.tensor(label, dtype=torch.long)}\n",
        "\n",
        "def train_fasttext(sentences: List[str], save_path: str, dim=300, min_count=2, epochs=10):\n",
        "    if os.path.exists(save_path):\n",
        "        print(f\"✅ FastText model found at {save_path}, loading...\")\n",
        "        return FastText.load(save_path)\n",
        "    print(f\"⏳ Training new FastText model for {epochs} epochs... (This can take 5-10 minutes)\")\n",
        "    tokenized = [s.split() for s in sentences]\n",
        "    ft = FastText(sentences=tokenized, vector_size=dim, window=5, min_count=min_count, workers=os.cpu_count(), epochs=epochs)\n",
        "    ft.save(save_path)\n",
        "    print(\"✅ FastText model training complete.\")\n",
        "    return ft\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, hidden_dim):\n",
        "        super(Attention, self).__init__()\n",
        "        self.attn = nn.Linear(hidden_dim, 1, bias=False)\n",
        "    def forward(self, lstm_output):\n",
        "        attn_weights = F.softmax(self.attn(lstm_output), dim=1)\n",
        "        return torch.sum(attn_weights * lstm_output, dim=1)\n",
        "\n",
        "class BiLSTMAttentionClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, n_layers, num_labels, dropout):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim // 2, num_layers=n_layers, bidirectional=True, batch_first=True, dropout=dropout if n_layers > 1 else 0)\n",
        "        self.attention = Attention(hidden_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.fc = nn.Linear(hidden_dim, num_labels)\n",
        "    def init_weights_from_fasttext(self, ft_model, vocab):\n",
        "        print(\"⏳ Initializing embedding layer with FastText vectors...\")\n",
        "        weights_matrix = np.random.normal(size=(len(vocab), cfg.embedding_dim)).astype(np.float32)\n",
        "        found_count = 0\n",
        "        for word, i in vocab.word2idx.items():\n",
        "            if word in ft_model.wv:\n",
        "                weights_matrix[i] = ft_model.wv[word]\n",
        "                found_count += 1\n",
        "        self.embedding.weight.data.copy_(torch.from_numpy(weights_matrix))\n",
        "        print(f\"✅ Found {found_count}/{len(vocab)} words in FastText model.\")\n",
        "    def forward(self, text):\n",
        "        embedded = self.embedding(text)\n",
        "        lstm_output, _ = self.lstm(embedded)\n",
        "        context_vector = self.attention(lstm_output)\n",
        "        return self.fc(self.dropout(context_vector)), None\n",
        "\n",
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, gamma=2.0, alpha=None, device='cpu'):\n",
        "        super().__init__()\n",
        "        self.gamma = gamma\n",
        "        self.alpha = torch.tensor(alpha, dtype=torch.float32).to(device) if alpha is not None else None\n",
        "    def forward(self, logits, targets):\n",
        "        ce = F.cross_entropy(logits, targets, reduction='none')\n",
        "        pt = torch.exp(-ce)\n",
        "        loss = ((1 - pt) ** self.gamma) * ce\n",
        "        if self.alpha is not None:\n",
        "            loss = self.alpha[targets] * loss\n",
        "        return loss.mean()\n",
        "\n",
        "# -------------------------\n",
        "# Corrected evaluate() -> returns 4 items\n",
        "# -------------------------\n",
        "def evaluate(model, dataloader, device, id2label):\n",
        "    \"\"\"\n",
        "    Returns:\n",
        "      - report (dict): sklearn classification_report as dict (output_dict=True)\n",
        "      - cm (np.array): confusion matrix\n",
        "      - y_true (list): ground-truth labels\n",
        "      - y_pred (list): predicted labels\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    y_true, y_pred = [], []\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(dataloader, desc=\"Eval\", leave=False):\n",
        "            text = batch['text'].to(device)\n",
        "            labels = batch['label'].cpu().numpy().tolist()\n",
        "            logits, _ = model(text)\n",
        "            preds = torch.argmax(logits, dim=1).cpu().numpy().tolist()\n",
        "            y_true.extend(labels)\n",
        "            y_pred.extend(preds)\n",
        "\n",
        "    report = classification_report(\n",
        "        y_true,\n",
        "        y_pred,\n",
        "        target_names=list(id2label.values()),\n",
        "        digits=4,\n",
        "        output_dict=True,\n",
        "        zero_division=0\n",
        "    )\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    return report, cm, y_true, y_pred\n",
        "\n",
        "# -------------------------\n",
        "# Training loop\n",
        "# -------------------------\n",
        "def train_loop(train_loader, val_loader, model, cfg, class_weights, id2label):\n",
        "    device = cfg.device\n",
        "    model.to(device)\n",
        "    print(f\"✅ All setup complete. Starting training on {device}...\")\n",
        "    time.sleep(1)\n",
        "\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=cfg.lr)\n",
        "    alpha = class_weights / class_weights.sum()\n",
        "    criterion = FocalLoss(gamma=cfg.focal_gamma, alpha=alpha, device=cfg.device)\n",
        "\n",
        "    best_macro = -1.0\n",
        "    for epoch in range(cfg.epochs):\n",
        "        model.train()\n",
        "        pbar = tqdm(train_loader, desc=f\"Train E{epoch+1}/{cfg.epochs}\")\n",
        "        for batch in pbar:\n",
        "            text, labels = batch['text'].to(device), batch['label'].to(device)\n",
        "            optimizer.zero_grad()\n",
        "            logits, _ = model(text)\n",
        "            loss = criterion(logits, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            pbar.set_postfix(loss=loss.item())\n",
        "\n",
        "        report, _, _, _ = evaluate(model, val_loader, device, id2label)\n",
        "        macro_f1 = report['macro avg']['f1-score']\n",
        "        print(f\"Epoch {epoch+1} -> Val Macro F1: {macro_f1:.4f}\")\n",
        "        if macro_f1 > best_macro:\n",
        "            best_macro = macro_f1\n",
        "            torch.save({\"model_state_dict\": model.state_dict()}, os.path.join(cfg.output_dir, \"best_model.pt\"))\n",
        "            print(f\"🚀 New best model saved with Macro F1: {macro_f1:.4f}\")\n",
        "    return os.path.join(cfg.output_dir, \"best_model.pt\")\n",
        "\n",
        "# -------------------------\n",
        "# Main pipeline\n",
        "# -------------------------\n",
        "def main():\n",
        "    start_time = time.time()\n",
        "    print(\"--- Step 1: Loading and Cleaning Data ---\")\n",
        "    df = pd.read_csv(cfg.data_csv, sep='\\\\t', header=None, names=['label', 'text'], engine='python')\n",
        "    df.dropna(subset=['text', 'label'], inplace=True)\n",
        "    df['text'] = df['text'].astype(str)\n",
        "\n",
        "    label_counts = df['label'].value_counts()\n",
        "    classes_to_keep = label_counts[label_counts >= cfg.min_class_samples].index.tolist()\n",
        "    if len(classes_to_keep) < len(label_counts):\n",
        "        print(f\"Filtering out classes with less than {cfg.min_class_samples} samples.\")\n",
        "        df = df[df['label'].isin(classes_to_keep)]\n",
        "    print(f\"✅ Data loading complete. Kept {len(df)} records.\")\n",
        "\n",
        "    print(\"\\n--- Step 2: Splitting Data and Building Vocabulary ---\")\n",
        "    unique_labels = sorted(df['label'].unique())\n",
        "    label_map = {label: i for i, label in enumerate(unique_labels)}\n",
        "    id2label = {v: k for k, v in label_map.items()}\n",
        "\n",
        "    data = df.to_dict(orient=\"records\")\n",
        "    labels = [label_map[r['label']] for r in data]\n",
        "    train_idx, test_idx = train_test_split(range(len(data)), test_size=0.15, random_state=cfg.seed, stratify=labels)\n",
        "    train_idx, val_idx = train_test_split(train_idx, test_size=0.15, random_state=cfg.seed, stratify=[labels[i] for i in train_idx])\n",
        "    train_records, val_records, test_records = [data[i] for i in train_idx], [data[i] for i in val_idx], [data[i] for i in test_idx]\n",
        "\n",
        "    vocab = Vocabulary([r['text'] for r in train_records], min_freq=cfg.ft_min_count)\n",
        "    print(f\"✅ Vocabulary built with {len(vocab)} unique tokens.\")\n",
        "\n",
        "    print(\"\\n--- Step 3: Training FastText Embeddings ---\")\n",
        "    ft_model = train_fasttext([r['text'] for r in data], cfg.fasttext_model_path, dim=cfg.ft_dim, min_count=cfg.ft_min_count, epochs=cfg.ft_epochs)\n",
        "\n",
        "    print(\"\\n--- Step 4: Building Model and DataLoaders ---\")\n",
        "    model = BiLSTMAttentionClassifier(vocab_size=len(vocab), embedding_dim=cfg.embedding_dim, hidden_dim=cfg.hidden_dim, n_layers=cfg.n_layers, num_labels=len(label_map), dropout=cfg.dropout)\n",
        "    model.init_weights_from_fasttext(ft_model, vocab)\n",
        "\n",
        "    train_ds = CommentsDataset(train_records, label_map, vocab, max_len=cfg.max_length)\n",
        "    val_ds = CommentsDataset(val_records, label_map, vocab, max_len=cfg.max_length)\n",
        "    train_loader = DataLoader(train_ds, batch_size=cfg.batch_size, shuffle=True, num_workers=0)\n",
        "    val_loader = DataLoader(val_ds, batch_size=cfg.batch_size*2, shuffle=False, num_workers=0)\n",
        "\n",
        "    cnt = Counter([label_map[r['label']] for r in train_records])\n",
        "    class_weights = np.array([cnt.get(i, 0) for i in range(len(label_map))])\n",
        "    class_weights = 1.0 / (class_weights + 1e-9)\n",
        "\n",
        "    # --- Step 5: Starting the Training Loop ---\n",
        "    best_ckpt = train_loop(train_loader, val_loader, model, cfg, class_weights=class_weights, id2label=id2label)\n",
        "\n",
        "    # --- Step 6: Final Evaluation ---\n",
        "    print(\"\\n--- Final Test Set Evaluation ---\")\n",
        "    if best_ckpt and os.path.exists(best_ckpt):\n",
        "        ckpt = torch.load(best_ckpt, map_location=cfg.device)\n",
        "        model.load_state_dict(ckpt['model_state_dict'])\n",
        "        test_ds = CommentsDataset(test_records, label_map, vocab, max_len=cfg.max_length)\n",
        "        test_loader = DataLoader(test_ds, batch_size=cfg.batch_size*2, num_workers=0)\n",
        "        report, cm, y_true, y_pred = evaluate(model, test_loader, cfg.device, id2label)\n",
        "\n",
        "        print(\"\\nClassification Report:\\n\")\n",
        "        print(classification_report(y_true, y_pred,target_names=list(id2label.values()),digits=4))\n",
        "\n",
        "        print(\"\\nConfusion Matrix:\\n\", cm)\n",
        "\n",
        "    total_time = time.time() - start_time\n",
        "    print(f\"\\nTotal execution time: {total_time/60:.2f} minutes.\")\n",
        "\n",
        "# --- Run the main function ---\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AJzkfmd08OJ5",
        "outputId": "a8b5a345-392c-485a-de0d-16017b2c6768"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Step 1: Loading and Cleaning Data ---\n",
            "Filtering out classes with less than 3 samples.\n",
            "✅ Data loading complete. Kept 44019 records.\n",
            "\n",
            "--- Step 2: Splitting Data and Building Vocabulary ---\n",
            "✅ Vocabulary built with 21993 unique tokens.\n",
            "\n",
            "--- Step 3: Training FastText Embeddings ---\n",
            "✅ FastText model found at outputs_bilstm/fasttext_tamil.model, loading...\n",
            "\n",
            "--- Step 4: Building Model and DataLoaders ---\n",
            "⏳ Initializing embedding layer with FastText vectors...\n",
            "✅ Found 21993/21993 words in FastText model.\n",
            "✅ All setup complete. Starting training on cuda...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train E1/10: 100%|██████████| 497/497 [00:11<00:00, 44.67it/s, loss=0.108]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 -> Val Macro F1: 0.4199\n",
            "🚀 New best model saved with Macro F1: 0.4199\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train E2/10: 100%|██████████| 497/497 [00:11<00:00, 44.07it/s, loss=0.0611]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 -> Val Macro F1: 0.4175\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train E3/10: 100%|██████████| 497/497 [00:11<00:00, 43.93it/s, loss=0.0434]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 -> Val Macro F1: 0.4253\n",
            "🚀 New best model saved with Macro F1: 0.4253\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train E4/10: 100%|██████████| 497/497 [00:11<00:00, 44.61it/s, loss=0.0374]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 -> Val Macro F1: 0.4415\n",
            "🚀 New best model saved with Macro F1: 0.4415\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train E5/10: 100%|██████████| 497/497 [00:11<00:00, 44.75it/s, loss=0.0207]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5 -> Val Macro F1: 0.4274\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train E6/10: 100%|██████████| 497/497 [00:10<00:00, 45.49it/s, loss=0.0185]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6 -> Val Macro F1: 0.4371\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train E7/10: 100%|██████████| 497/497 [00:10<00:00, 45.47it/s, loss=0.0124]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7 -> Val Macro F1: 0.4397\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train E8/10: 100%|██████████| 497/497 [00:11<00:00, 44.80it/s, loss=0.0178]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8 -> Val Macro F1: 0.4332\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train E9/10: 100%|██████████| 497/497 [00:11<00:00, 44.72it/s, loss=0.0145]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9 -> Val Macro F1: 0.4212\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train E10/10: 100%|██████████| 497/497 [00:11<00:00, 44.82it/s, loss=0.00613]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10 -> Val Macro F1: 0.4240\n",
            "\n",
            "--- Final Test Set Evaluation ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Classification Report:\n",
            "\n",
            "                precision    recall  f1-score   support\n",
            "\n",
            "Mixed_feelings     0.1996    0.3938    0.2649       739\n",
            "      Negative     0.3410    0.3954    0.3662       784\n",
            "      Positive     0.8133    0.5162    0.6316      3731\n",
            "     not-Tamil     0.3870    0.6837    0.4942       313\n",
            " unknown_state     0.3863    0.4903    0.4322      1036\n",
            "\n",
            "      accuracy                         0.4920      6603\n",
            "     macro avg     0.4255    0.4959    0.4378      6603\n",
            "  weighted avg     0.6014    0.4920    0.5212      6603\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            " [[ 291  145  144   23  136]\n",
            " [ 202  310  115   25  132]\n",
            " [ 752  323 1926  234  496]\n",
            " [  20   10   26  214   43]\n",
            " [ 193  121  157   57  508]]\n",
            "\n",
            "Total execution time: 2.09 minutes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Full corrected script: CharCNN + FastText + BiLSTM + Attention + Aux features\n",
        "# Requirements:\n",
        "# pip install torch scikit-learn gensim pandas tqdm\n",
        "\n",
        "import os, time, random, json\n",
        "from collections import Counter\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Dict\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from gensim.models import FastText\n",
        "from tqdm import tqdm\n",
        "\n",
        "# -------------------------\n",
        "# Config\n",
        "# -------------------------\n",
        "@dataclass\n",
        "class CFG:\n",
        "    data_csv: str = \"tamil_sentiment_full.csv\"   # tab-separated file: label \\t text\n",
        "    output_dir: str = \"outputs_char_bilstm\"\n",
        "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    min_class_samples: int = 3\n",
        "\n",
        "    # vocab / chars\n",
        "    min_token_freq: int = 2\n",
        "    max_chars_per_token: int = 12\n",
        "    min_char_freq: int = 1\n",
        "\n",
        "    # embedding / ft\n",
        "    ft_dim: int = 300\n",
        "    ft_min_count: int = 2\n",
        "    ft_epochs: int = 8\n",
        "    embedding_trainable: bool = True\n",
        "\n",
        "    # model\n",
        "    hidden_dim: int = 256\n",
        "    lstm_layers: int = 1\n",
        "    char_emb_dim: int = 50\n",
        "    char_out: int = 100\n",
        "    attn_dim: int = 128\n",
        "    aux_dim: int = 8      # number of auxiliary features\n",
        "    dropout: float = 0.3\n",
        "\n",
        "    # training\n",
        "    epochs: int = 6\n",
        "    batch_size: int = 64\n",
        "    lr_emb: float = 5e-5\n",
        "    lr_head: float = 1e-3\n",
        "    weight_decay: float = 1e-5\n",
        "    use_sampler: bool = False\n",
        "    use_focal: bool = True\n",
        "    focal_gamma: float = 2.0\n",
        "    seed: int = 42\n",
        "\n",
        "cfg = CFG()\n",
        "os.makedirs(cfg.output_dir, exist_ok=True)\n",
        "\n",
        "# -------------------------\n",
        "# Utilities\n",
        "# -------------------------\n",
        "def seed_everything(seed=42):\n",
        "    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed)\n",
        "seed_everything(cfg.seed)\n",
        "\n",
        "def read_data(path):\n",
        "    # expects tab separated with label \\t text\n",
        "    df = pd.read_csv(path, sep='\\t', header=None, names=['label','text'], engine='python')\n",
        "    df.dropna(subset=['text','label'], inplace=True)\n",
        "    df['text'] = df['text'].astype(str)\n",
        "    return df\n",
        "\n",
        "# -------------------------\n",
        "# Preprocessing / vocabs\n",
        "# -------------------------\n",
        "def build_token_vocab(texts: List[str], min_freq=2):\n",
        "    cnt = Counter()\n",
        "    for t in texts:\n",
        "        for w in t.split():\n",
        "            cnt[w] += 1\n",
        "    word2idx = {'<pad>':0, '<unk>':1}\n",
        "    for w,c in cnt.items():\n",
        "        if c>=min_freq:\n",
        "            word2idx[w] = len(word2idx)\n",
        "    return word2idx\n",
        "\n",
        "def build_char_vocab(texts: List[str], min_freq=1, max_chars=12):\n",
        "    cnt = Counter()\n",
        "    for t in texts:\n",
        "        for tok in t.split():\n",
        "            for ch in list(tok)[:max_chars]:\n",
        "                cnt[ch] += 1\n",
        "    char2idx = {'<pad>':0, '<unk>':1}\n",
        "    for ch,c in cnt.items():\n",
        "        if c>=min_freq:\n",
        "            char2idx[ch] = len(char2idx)\n",
        "    return char2idx\n",
        "\n",
        "def text_to_token_ids(text, word2idx, max_len):\n",
        "    ids = [word2idx.get(w, word2idx['<unk>']) for w in text.split()]\n",
        "    if len(ids) < max_len: ids += [word2idx['<pad>']] * (max_len - len(ids))\n",
        "    else: ids = ids[:max_len]\n",
        "    return ids\n",
        "\n",
        "def text_to_char_ids(text, char2idx, max_len_tokens, max_chars_per_token):\n",
        "    toks = text.split()\n",
        "    char_ids = []\n",
        "    for i in range(max_len_tokens):\n",
        "        if i < len(toks):\n",
        "            tok = toks[i][:max_chars_per_token]\n",
        "            ids = [char2idx.get(ch, char2idx['<unk>']) for ch in tok]\n",
        "            if len(ids) < max_chars_per_token:\n",
        "                ids += [char2idx['<pad>']] * (max_chars_per_token - len(ids))\n",
        "        else:\n",
        "            ids = [char2idx['<pad>']] * max_chars_per_token\n",
        "        char_ids.append(ids)\n",
        "    return char_ids  # shape (max_len_tokens, max_chars_per_token)\n",
        "\n",
        "# Auxiliary features generator (simple, extensible)\n",
        "def compute_aux_features(text):\n",
        "    toks = text.split()\n",
        "    num_tokens = len(toks)\n",
        "    num_chars = len(text)\n",
        "    emoji_count = sum(1 for ch in text if ord(ch) > 10000)  # crude heuristic\n",
        "    punct_count = sum(1 for ch in text if ch in '?!.,;:')\n",
        "    has_english = 1.0 if any('a' <= ch.lower() <= 'z' for ch in text) else 0.0\n",
        "    has_tamil = 1.0 if any('\\u0B80' <= ch <= '\\u0BFF' for ch in text) else 0.0\n",
        "    avg_token_len = (sum(len(t) for t in toks)/num_tokens) if num_tokens>0 else 0.0\n",
        "    cap_ratio = sum(1 for ch in text if ch.isupper()) / (num_chars+1)\n",
        "    return [num_tokens, num_chars, emoji_count, punct_count, has_english, has_tamil, avg_token_len, cap_ratio]\n",
        "\n",
        "# -------------------------\n",
        "# Dataset\n",
        "# -------------------------\n",
        "class CharTokenDataset(Dataset):\n",
        "    def __init__(self, records, label_map, word2idx, char2idx, max_len_tokens, max_chars_per_token, aux_dim):\n",
        "        self.records = records\n",
        "        self.label_map = label_map\n",
        "        self.word2idx = word2idx\n",
        "        self.char2idx = char2idx\n",
        "        self.max_len_tokens = max_len_tokens\n",
        "        self.max_chars_per_token = max_chars_per_token\n",
        "        self.aux_dim = aux_dim\n",
        "\n",
        "        self.samples = []\n",
        "        for r in records:\n",
        "            text = str(r['text'])\n",
        "            label = label_map[r['label']]\n",
        "            token_ids = text_to_token_ids(text, word2idx, max_len_tokens)\n",
        "            char_ids = text_to_char_ids(text, char2idx, max_len_tokens, max_chars_per_token)\n",
        "            aux = compute_aux_features(text)\n",
        "            aux = (aux + [0.0]*aux_dim)[:aux_dim]\n",
        "            self.samples.append((token_ids, char_ids, aux, label))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        token_ids, char_ids, aux, label = self.samples[idx]\n",
        "        token_ids = torch.tensor(token_ids, dtype=torch.long)\n",
        "        char_ids = torch.tensor(char_ids, dtype=torch.long)  # shape (T, C)\n",
        "        aux = torch.tensor(aux, dtype=torch.float32)\n",
        "        label = torch.tensor(label, dtype=torch.long)\n",
        "        return {\"token_ids\": token_ids, \"char_ids\": char_ids, \"aux\": aux, \"label\": label}\n",
        "\n",
        "# -------------------------\n",
        "# FastText training / loading\n",
        "# -------------------------\n",
        "def train_or_load_fasttext(sentences, path, dim=300, min_count=2, epochs=6):\n",
        "    if os.path.exists(path):\n",
        "        print(\"Loading FastText from\", path)\n",
        "        return FastText.load(path)\n",
        "    print(\"Training FastText...\")\n",
        "    tokenized = [s.split() for s in sentences]\n",
        "    ft = FastText(vector_size=dim, window=5, min_count=min_count, workers=os.cpu_count(), epochs=epochs)\n",
        "    ft.build_vocab(tokenized)\n",
        "    ft.train(tokenized, total_examples=len(tokenized), epochs=epochs)\n",
        "    ft.save(path)\n",
        "    print(\"Saved FastText at\", path)\n",
        "    return ft\n",
        "\n",
        "def build_embedding_matrix(word2idx, ft_model, dim):\n",
        "    V = len(word2idx)\n",
        "    mat = np.random.normal(scale=0.01, size=(V, dim)).astype(np.float32)\n",
        "    found = 0\n",
        "    for w,i in word2idx.items():\n",
        "        if w in ft_model.wv:\n",
        "            mat[i] = ft_model.wv[w]\n",
        "            found += 1\n",
        "    print(f\"Found {found}/{V} tokens in FastText.\")\n",
        "    return mat\n",
        "\n",
        "# -------------------------\n",
        "# Model components (robust CharCNN & BiLSTM)\n",
        "# -------------------------\n",
        "class CharCNN(nn.Module):\n",
        "    def __init__(self, char_vocab_size, char_emb_dim=50, out_dim=100, kernel_sizes=(3,4,5), dropout=0.1, max_chars=12):\n",
        "        super().__init__()\n",
        "        self.char_emb = nn.Embedding(char_vocab_size, char_emb_dim, padding_idx=0)\n",
        "        k = len(kernel_sizes)\n",
        "        base = out_dim // k\n",
        "        extras = out_dim - (base * k)\n",
        "        out_channels_list = [base + (1 if i < extras else 0) for i in range(k)]\n",
        "        self.convs = nn.ModuleList([\n",
        "            nn.Conv2d(in_channels=1, out_channels=out_channels_list[i], kernel_size=(kernel_sizes[i], char_emb_dim))\n",
        "            for i in range(k)\n",
        "        ])\n",
        "        self.out_dim_actual = sum(out_channels_list)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.max_chars = max_chars\n",
        "\n",
        "    def forward(self, x_char):\n",
        "        # x_char: (B, T, C)\n",
        "        B,T,C = x_char.size()\n",
        "        x = self.char_emb(x_char)           # (B, T, C, E)\n",
        "        x = x.view(B*T, C, -1).unsqueeze(1) # (B*T, 1, C, E)\n",
        "        conv_outs = []\n",
        "        for conv in self.convs:\n",
        "            o = conv(x)                     # (B*T, out_ch, L, 1)\n",
        "            o = F.relu(o.squeeze(-1))       # (B*T, out_ch, L)\n",
        "            o = F.max_pool1d(o, o.size(2)).squeeze(2)  # (B*T, out_ch)\n",
        "            conv_outs.append(o)\n",
        "        out = torch.cat(conv_outs, dim=1)   # (B*T, out_dim_actual)\n",
        "        out = out.view(B, T, -1)            # (B, T, out_dim_actual)\n",
        "        out = self.dropout(out)\n",
        "        return out\n",
        "\n",
        "class BiLSTMCharFastText(nn.Module):\n",
        "    def __init__(self, emb_matrix, char_vocab_size, cfg, num_labels):\n",
        "        super().__init__()\n",
        "        emb_matrix = torch.tensor(emb_matrix)\n",
        "        V, E = emb_matrix.shape\n",
        "        self.embedding = nn.Embedding.from_pretrained(emb_matrix, freeze=not cfg.embedding_trainable, padding_idx=0)\n",
        "        self.char_cnn = CharCNN(char_vocab_size, char_emb_dim=cfg.char_emb_dim, out_dim=cfg.char_out, dropout=cfg.dropout, max_chars=cfg.max_chars_per_token)\n",
        "        token_in_dim = E + self.char_cnn.out_dim_actual\n",
        "        self.bilstm = nn.LSTM(token_in_dim, cfg.hidden_dim//2, num_layers=cfg.lstm_layers, bidirectional=True, batch_first=True, dropout=cfg.dropout if cfg.lstm_layers>1 else 0)\n",
        "        self.attn_proj = nn.Linear(cfg.hidden_dim, cfg.attn_dim)\n",
        "        self.attn_v = nn.Linear(cfg.attn_dim, 1, bias=False)\n",
        "        self.aux_proj = nn.Linear(cfg.aux_dim, 32)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(cfg.hidden_dim + 32, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(cfg.dropout),\n",
        "            nn.Linear(256, num_labels)\n",
        "        )\n",
        "\n",
        "    def forward(self, token_ids, char_ids, aux):\n",
        "        emb = self.embedding(token_ids)                   # (B, T, E)\n",
        "        char_vec = self.char_cnn(char_ids)                # (B, T, char_out_actual)\n",
        "        x = torch.cat([emb, char_vec], dim=-1)            # (B, T, E + char_out_actual)\n",
        "        h, _ = self.bilstm(x)                             # (B, T, H)\n",
        "        a = torch.tanh(self.attn_proj(h))                 # (B, T, attn_dim)\n",
        "        scores = self.attn_v(a).squeeze(-1)               # (B, T)\n",
        "        mask = (token_ids != 0).float()                   # pad mask\n",
        "        scores = scores.masked_fill(mask==0, -1e9)\n",
        "        alpha = torch.softmax(scores, dim=1).unsqueeze(-1) # (B, T, 1)\n",
        "        pooled = (h * alpha).sum(dim=1)                   # (B, H)\n",
        "        aux_p = torch.relu(self.aux_proj(aux))            # (B, 32)\n",
        "        cat = torch.cat([pooled, aux_p], dim=1)           # (B, H+32)\n",
        "        logits = self.classifier(cat)                     # (B, num_labels)\n",
        "        return logits\n",
        "\n",
        "# -------------------------\n",
        "# Loss: Focal\n",
        "# -------------------------\n",
        "# Replace your current FocalLoss class with this version\n",
        "\n",
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, gamma=2.0, alpha=None):\n",
        "        super().__init__()\n",
        "        self.gamma = gamma\n",
        "        # store alpha as tensor on CPU for now; will move in forward\n",
        "        if alpha is not None:\n",
        "            self.alpha = torch.tensor(alpha, dtype=torch.float32)\n",
        "        else:\n",
        "            self.alpha = None\n",
        "\n",
        "    def forward(self, logits, targets):\n",
        "        \"\"\"\n",
        "        logits: (B, C)\n",
        "        targets: (B,) long, on some device (cpu/cuda)\n",
        "        \"\"\"\n",
        "        ce = F.cross_entropy(logits, targets, reduction='none')  # (B,)\n",
        "        pt = torch.exp(-ce)\n",
        "        loss = ((1 - pt) ** self.gamma) * ce\n",
        "\n",
        "        if self.alpha is not None:\n",
        "            # ensure alpha is on same device as targets before indexing\n",
        "            if self.alpha.device != targets.device:\n",
        "                self.alpha = self.alpha.to(targets.device)\n",
        "            loss = self.alpha[targets] * loss\n",
        "\n",
        "        return loss.mean()\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# Evaluate (returns y_true, y_pred)\n",
        "# -------------------------\n",
        "def evaluate(model, dataloader, device, id2label):\n",
        "    model.eval()\n",
        "    y_true, y_pred = [], []\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(dataloader, desc=\"Eval\", leave=False):\n",
        "            tokens = batch['token_ids'].to(device)\n",
        "            chars  = batch['char_ids'].to(device)\n",
        "            aux    = batch['aux'].to(device)\n",
        "            labels = batch['label'].cpu().numpy().tolist()\n",
        "            logits = model(tokens, chars, aux)\n",
        "            preds = torch.argmax(logits, dim=1).cpu().numpy().tolist()\n",
        "            y_true.extend(labels); y_pred.extend(preds)\n",
        "    report = classification_report(y_true, y_pred, target_names=list(id2label.values()), digits=4, output_dict=True, zero_division=0)\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    return report, cm, y_true, y_pred\n",
        "\n",
        "# -------------------------\n",
        "# Training loop\n",
        "# -------------------------\n",
        "def train(train_loader, val_loader, model, cfg, class_weights, id2label):\n",
        "    device = cfg.device\n",
        "    model.to(device)\n",
        "    emb_params = list(model.embedding.parameters())\n",
        "    other_params = [p for n,p in model.named_parameters() if not n.startswith('embedding.')]\n",
        "    optimizer = torch.optim.AdamW([\n",
        "        {\"params\": emb_params, \"lr\": cfg.lr_emb},\n",
        "        {\"params\": other_params, \"lr\": cfg.lr_head}\n",
        "    ], weight_decay=cfg.weight_decay)\n",
        "\n",
        "    if cfg.use_focal:\n",
        "        alpha = (1.0 / (class_weights + 1e-9))\n",
        "        alpha = alpha / alpha.sum()\n",
        "        criterion = FocalLoss(cfg.focal_gamma, alpha=alpha)\n",
        "    else:\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    best_ckpt = None\n",
        "    best_macro = -1.0\n",
        "    for epoch in range(cfg.epochs):\n",
        "        model.train()\n",
        "        pbar = tqdm(train_loader, desc=f\"Train E{epoch+1}/{cfg.epochs}\")\n",
        "        for batch in pbar:\n",
        "            tokens = batch['token_ids'].to(device)\n",
        "            chars  = batch['char_ids'].to(device)\n",
        "            aux    = batch['aux'].to(device)\n",
        "            labels = batch['label'].to(device)\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(tokens, chars, aux)\n",
        "            loss = criterion(logits, labels)\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "            pbar.set_postfix(loss=loss.item())\n",
        "        report, cm, _, _ = evaluate(model, val_loader, cfg.device, id2label)\n",
        "        macro_f1 = report['macro avg']['f1-score']\n",
        "        print(f\"Epoch {epoch+1} -> Val Macro F1: {macro_f1:.4f}\")\n",
        "        if macro_f1 > best_macro:\n",
        "            best_macro = macro_f1\n",
        "            best_ckpt = os.path.join(cfg.output_dir, f\"best_macro_{macro_f1:.4f}.pt\")\n",
        "            torch.save({\"model_state_dict\": model.state_dict(), \"cfg\": cfg.__dict__}, best_ckpt)\n",
        "            print(\"Saved\", best_ckpt)\n",
        "    return best_ckpt\n",
        "\n",
        "# -------------------------\n",
        "# Pipeline orchestration\n",
        "# -------------------------\n",
        "def main():\n",
        "    # load\n",
        "    df = read_data(cfg.data_csv)\n",
        "    # filter tiny classes\n",
        "    cnt = df['label'].value_counts()\n",
        "    keep = cnt[cnt >= cfg.min_class_samples].index.tolist()\n",
        "    if len(keep) < len(cnt):\n",
        "        df = df[df['label'].isin(keep)].reset_index(drop=True)\n",
        "    print(\"Records:\", len(df), \"labels:\", df['label'].nunique())\n",
        "\n",
        "    # label mapping\n",
        "    labels_unique = sorted(df['label'].unique())\n",
        "    label_map = {lab:i for i,lab in enumerate(labels_unique)}\n",
        "    id2label = {v:k for k,v in label_map.items()}\n",
        "\n",
        "    # splits\n",
        "    data = df.to_dict(orient='records')\n",
        "    lablist = [label_map[r['label']] for r in data]\n",
        "    train_idx, test_idx = train_test_split(range(len(data)), test_size=0.15, random_state=cfg.seed, stratify=lablist)\n",
        "    train_idx, val_idx = train_test_split(train_idx, test_size=0.15, random_state=cfg.seed, stratify=[lablist[i] for i in train_idx])\n",
        "    train_records = [data[i] for i in train_idx]\n",
        "    val_records   = [data[i] for i in val_idx]\n",
        "    test_records  = [data[i] for i in test_idx]\n",
        "    print(\"split sizes:\", len(train_records), len(val_records), len(test_records))\n",
        "\n",
        "    # vocabs\n",
        "    word2idx = build_token_vocab([r['text'] for r in train_records], cfg.min_token_freq)\n",
        "    char2idx = build_char_vocab([r['text'] for r in train_records], cfg.min_char_freq, cfg.max_chars_per_token)\n",
        "    print(\"Vocab sizes: tokens\", len(word2idx), \"chars\", len(char2idx))\n",
        "\n",
        "    # fasttext\n",
        "    ft_path = os.path.join(cfg.output_dir, \"fasttext.model\")\n",
        "    ft = train_or_load_fasttext([r['text'] for r in data], ft_path, dim=cfg.ft_dim, min_count=cfg.ft_min_count, epochs=cfg.ft_epochs)\n",
        "    emb_matrix = build_embedding_matrix(word2idx, ft, cfg.ft_dim)\n",
        "\n",
        "    # datasets\n",
        "    max_len = 64  # token length cap - tune as needed\n",
        "    train_ds = CharTokenDataset(train_records, label_map, word2idx, char2idx, max_len, cfg.max_chars_per_token, cfg.aux_dim)\n",
        "    val_ds   = CharTokenDataset(val_records, label_map, word2idx, char2idx, max_len, cfg.max_chars_per_token, cfg.aux_dim)\n",
        "    test_ds  = CharTokenDataset(test_records, label_map, word2idx, char2idx, max_len, cfg.max_chars_per_token, cfg.aux_dim)\n",
        "\n",
        "    if cfg.use_sampler:\n",
        "        labels = [s[3] for s in train_ds.samples]\n",
        "        cnts = Counter(labels)\n",
        "        sample_weights = [1.0 / cnts[l] for l in labels]\n",
        "        sampler = WeightedRandomSampler(sample_weights, num_samples=len(sample_weights), replacement=True)\n",
        "        train_loader = DataLoader(train_ds, batch_size=cfg.batch_size, sampler=sampler, num_workers=2)\n",
        "    else:\n",
        "        train_loader = DataLoader(train_ds, batch_size=cfg.batch_size, shuffle=True, num_workers=2)\n",
        "\n",
        "    val_loader = DataLoader(val_ds, batch_size=cfg.batch_size*2, shuffle=False, num_workers=2)\n",
        "    test_loader = DataLoader(test_ds, batch_size=cfg.batch_size*2, shuffle=False, num_workers=2)\n",
        "\n",
        "    # class weight vector (counts)\n",
        "    train_labels = [s[3] for s in train_ds.samples]\n",
        "    cnts = np.array([Counter(train_labels).get(i,0) for i in range(len(label_map))])\n",
        "    class_weights = cnts.astype(np.float32)\n",
        "\n",
        "    # model\n",
        "    model = BiLSTMCharFastText(emb_matrix, char_vocab_size=len(char2idx), cfg=cfg, num_labels=len(label_map))\n",
        "    # quick shape sanity check\n",
        "    print(\"embedding dim:\", emb_matrix.shape[1])\n",
        "    print(\"char_cnn out dim actual:\", model.char_cnn.out_dim_actual)\n",
        "    sample_token_ids = torch.zeros((2, 8), dtype=torch.long)\n",
        "    sample_char_ids = torch.zeros((2, 8, cfg.max_chars_per_token), dtype=torch.long)\n",
        "    sample_aux = torch.zeros((2, cfg.aux_dim), dtype=torch.float)\n",
        "    with torch.no_grad():\n",
        "        logits_shape = model(sample_token_ids, sample_char_ids, sample_aux).shape\n",
        "    print(\"logits shape (sanity):\", logits_shape)\n",
        "    print(\"Model trainable params:\", sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
        "\n",
        "    # train\n",
        "    best_ckpt = train(train_loader, val_loader, model, cfg, class_weights, id2label)\n",
        "\n",
        "    # test eval\n",
        "    if best_ckpt and os.path.exists(best_ckpt):\n",
        "        ckpt = torch.load(best_ckpt, map_location=cfg.device)\n",
        "        model.load_state_dict(ckpt['model_state_dict'])\n",
        "    report, cm, y_true, y_pred = evaluate(model, test_loader, cfg.device, id2label)\n",
        "    print(\"\\nFinal Test Report:\")\n",
        "    print(classification_report(y_true, y_pred, target_names=list(id2label.values()), digits=4))\n",
        "    print(\"\\nConfusion Matrix:\\n\", cm)\n",
        "\n",
        "    # save predictions csv for inspection\n",
        "    rows = []\n",
        "    for rec, yt, yp in zip(test_records, y_true, y_pred):\n",
        "        rows.append({\"text\": rec['text'], \"label\": id2label[yt], \"pred\": id2label[yp]})\n",
        "    pd.DataFrame(rows).to_csv(os.path.join(cfg.output_dir, \"test_preds.csv\"), index=False)\n",
        "    print(\"Saved test_preds.csv\")\n",
        "    print(\"Done.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X710ooN0-az_",
        "outputId": "9f3fa997-2d2b-448f-afc4-b236a12ac4b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Records: 44019 labels: 5\n",
            "split sizes: 31803 5613 6603\n",
            "Vocab sizes: tokens 21997 chars 657\n",
            "Loading FastText from outputs_char_bilstm/fasttext.model\n",
            "Found 21997/21997 tokens in FastText.\n",
            "embedding dim: 300\n",
            "char_cnn out dim actual: 100\n",
            "logits shape (sanity): torch.Size([2, 5])\n",
            "Model trainable params: 7303301\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train E1/6: 100%|██████████| 497/497 [07:03<00:00,  1.17it/s, loss=0.0879]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 -> Val Macro F1: 0.3860\n",
            "Saved outputs_char_bilstm/best_macro_0.3860.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train E2/6: 100%|██████████| 497/497 [07:08<00:00,  1.16it/s, loss=0.0687]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 -> Val Macro F1: 0.3646\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train E3/6: 100%|██████████| 497/497 [07:08<00:00,  1.16it/s, loss=0.0893]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 -> Val Macro F1: 0.4286\n",
            "Saved outputs_char_bilstm/best_macro_0.4286.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train E4/6: 100%|██████████| 497/497 [07:08<00:00,  1.16it/s, loss=0.0952]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 -> Val Macro F1: 0.4014\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train E5/6: 100%|██████████| 497/497 [07:07<00:00,  1.16it/s, loss=0.0822]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5 -> Val Macro F1: 0.4619\n",
            "Saved outputs_char_bilstm/best_macro_0.4619.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train E6/6: 100%|██████████| 497/497 [07:08<00:00,  1.16it/s, loss=0.0595]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6 -> Val Macro F1: 0.4359\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Final Test Report:\n",
            "                precision    recall  f1-score   support\n",
            "\n",
            "Mixed_feelings     0.2225    0.2842    0.2496       739\n",
            "      Negative     0.3159    0.5536    0.4022       784\n",
            "      Positive     0.8604    0.5090    0.6396      3731\n",
            "     not-Tamil     0.4488    0.6997    0.5468       313\n",
            " unknown_state     0.3535    0.5425    0.4280      1036\n",
            "\n",
            "      accuracy                         0.5034      6603\n",
            "     macro avg     0.4402    0.5178    0.4532      6603\n",
            "  weighted avg     0.6253    0.5034    0.5302      6603\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            " [[ 210  251  131   19  128]\n",
            " [ 110  434   73   23  144]\n",
            " [ 481  479 1899  169  703]\n",
            " [   7   13   21  219   53]\n",
            " [ 136  197   83   58  562]]\n",
            "Saved test_preds.csv\n",
            "Done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Full corrected script: CharCNN + FastText + BiLSTM + Attention + Aux features + TF-IDF\n",
        "# Requirements:\n",
        "# pip install torch scikit-learn gensim pandas tqdm\n",
        "\n",
        "import os, time, random, json\n",
        "from collections import Counter\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Dict\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer ### NEW ###\n",
        "from gensim.models import FastText\n",
        "from tqdm import tqdm\n",
        "\n",
        "# -------------------------\n",
        "# Config\n",
        "# -------------------------\n",
        "@dataclass\n",
        "class CFG:\n",
        "    data_csv: str = \"tamil_sentiment_full.csv\"  # tab-separated file: label \\t text\n",
        "    output_dir: str = \"outputs_char_bilstm_tfidf\"\n",
        "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    min_class_samples: int = 3\n",
        "\n",
        "    # vocab / chars\n",
        "    min_token_freq: int = 2\n",
        "    max_chars_per_token: int = 12\n",
        "    min_char_freq: int = 1\n",
        "\n",
        "    # embedding / ft\n",
        "    ft_dim: int = 300\n",
        "    ft_min_count: int = 2\n",
        "    ft_epochs: int = 8\n",
        "    embedding_trainable: bool = True\n",
        "\n",
        "    # model\n",
        "    hidden_dim: int = 256\n",
        "    lstm_layers: int = 1\n",
        "    char_emb_dim: int = 50\n",
        "    char_out: int = 100\n",
        "    attn_dim: int = 128\n",
        "    aux_dim: int = 8      # number of auxiliary features\n",
        "    tfidf_dim: int = 5000 ### NEW ###: Max features for TF-IDF\n",
        "    tfidf_proj_dim: int = 64 ### NEW ###: Dimension to project TF-IDF features to\n",
        "    dropout: float = 0.3\n",
        "\n",
        "    # training\n",
        "    epochs: int = 6\n",
        "    batch_size: int = 64\n",
        "    lr_emb: float = 5e-5\n",
        "    lr_head: float = 1e-3\n",
        "    weight_decay: float = 1e-5\n",
        "    use_sampler: bool = False\n",
        "    use_focal: bool = True\n",
        "    focal_gamma: float = 2.0\n",
        "    seed: int = 42\n",
        "\n",
        "cfg = CFG()\n",
        "os.makedirs(cfg.output_dir, exist_ok=True)\n",
        "\n",
        "# -------------------------\n",
        "# Utilities\n",
        "# -------------------------\n",
        "def seed_everything(seed=42):\n",
        "    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed)\n",
        "seed_everything(cfg.seed)\n",
        "\n",
        "def read_data(path):\n",
        "    # expects tab separated with label \\t text\n",
        "    df = pd.read_csv(path, sep='\\t', header=None, names=['label','text'], engine='python')\n",
        "    df.dropna(subset=['text','label'], inplace=True)\n",
        "    df['text'] = df['text'].astype(str)\n",
        "    return df\n",
        "\n",
        "# -------------------------\n",
        "# Preprocessing / vocabs\n",
        "# -------------------------\n",
        "def build_token_vocab(texts: List[str], min_freq=2):\n",
        "    cnt = Counter()\n",
        "    for t in texts:\n",
        "        for w in t.split():\n",
        "            cnt[w] += 1\n",
        "    word2idx = {'<pad>':0, '<unk>':1}\n",
        "    for w,c in cnt.items():\n",
        "        if c>=min_freq:\n",
        "            word2idx[w] = len(word2idx)\n",
        "    return word2idx\n",
        "\n",
        "def build_char_vocab(texts: List[str], min_freq=1, max_chars=12):\n",
        "    cnt = Counter()\n",
        "    for t in texts:\n",
        "        for tok in t.split():\n",
        "            for ch in list(tok)[:max_chars]:\n",
        "                cnt[ch] += 1\n",
        "    char2idx = {'<pad>':0, '<unk>':1}\n",
        "    for ch,c in cnt.items():\n",
        "        if c>=min_freq:\n",
        "            char2idx[ch] = len(char2idx)\n",
        "    return char2idx\n",
        "\n",
        "def text_to_token_ids(text, word2idx, max_len):\n",
        "    ids = [word2idx.get(w, word2idx['<unk>']) for w in text.split()]\n",
        "    if len(ids) < max_len: ids += [word2idx['<pad>']] * (max_len - len(ids))\n",
        "    else: ids = ids[:max_len]\n",
        "    return ids\n",
        "\n",
        "def text_to_char_ids(text, char2idx, max_len_tokens, max_chars_per_token):\n",
        "    toks = text.split()\n",
        "    char_ids = []\n",
        "    for i in range(max_len_tokens):\n",
        "        if i < len(toks):\n",
        "            tok = toks[i][:max_chars_per_token]\n",
        "            ids = [char2idx.get(ch, char2idx['<unk>']) for ch in tok]\n",
        "            if len(ids) < max_chars_per_token:\n",
        "                ids += [char2idx['<pad>']] * (max_chars_per_token - len(ids))\n",
        "        else:\n",
        "            ids = [char2idx['<pad>']] * max_chars_per_token\n",
        "        char_ids.append(ids)\n",
        "    return char_ids  # shape (max_len_tokens, max_chars_per_token)\n",
        "\n",
        "# Auxiliary features generator (simple, extensible)\n",
        "def compute_aux_features(text):\n",
        "    toks = text.split()\n",
        "    num_tokens = len(toks)\n",
        "    num_chars = len(text)\n",
        "    emoji_count = sum(1 for ch in text if ord(ch) > 10000)  # crude heuristic\n",
        "    punct_count = sum(1 for ch in text if ch in '?!.,;:')\n",
        "    has_english = 1.0 if any('a' <= ch.lower() <= 'z' for ch in text) else 0.0\n",
        "    has_tamil = 1.0 if any('\\u0B80' <= ch <= '\\u0BFF' for ch in text) else 0.0\n",
        "    avg_token_len = (sum(len(t) for t in toks)/num_tokens) if num_tokens>0 else 0.0\n",
        "    cap_ratio = sum(1 for ch in text if ch.isupper()) / (num_chars+1)\n",
        "    return [num_tokens, num_chars, emoji_count, punct_count, has_english, has_tamil, avg_token_len, cap_ratio]\n",
        "\n",
        "# -------------------------\n",
        "# Dataset\n",
        "# -------------------------\n",
        "class CharTokenDataset(Dataset): ### MODIFIED to include TF-IDF ###\n",
        "    def __init__(self, records, tfidf_matrix, label_map, word2idx, char2idx, max_len_tokens, max_chars_per_token, aux_dim):\n",
        "        self.records = records\n",
        "        self.tfidf_matrix = tfidf_matrix\n",
        "        self.label_map = label_map\n",
        "        self.word2idx = word2idx\n",
        "        self.char2idx = char2idx\n",
        "        self.max_len_tokens = max_len_tokens\n",
        "        self.max_chars_per_token = max_chars_per_token\n",
        "        self.aux_dim = aux_dim\n",
        "\n",
        "        self.samples = []\n",
        "        for i, r in enumerate(records):\n",
        "            text = str(r['text'])\n",
        "            label = label_map[r['label']]\n",
        "            token_ids = text_to_token_ids(text, word2idx, max_len_tokens)\n",
        "            char_ids = text_to_char_ids(text, char2idx, max_len_tokens, max_chars_per_token)\n",
        "            aux = compute_aux_features(text)\n",
        "            aux = (aux + [0.0]*aux_dim)[:aux_dim]\n",
        "            tfidf_vec = self.tfidf_matrix[i].toarray().squeeze()\n",
        "            self.samples.append((token_ids, char_ids, aux, tfidf_vec, label))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        token_ids, char_ids, aux, tfidf, label = self.samples[idx]\n",
        "        token_ids = torch.tensor(token_ids, dtype=torch.long)\n",
        "        char_ids = torch.tensor(char_ids, dtype=torch.long)\n",
        "        aux = torch.tensor(aux, dtype=torch.float32)\n",
        "        tfidf = torch.tensor(tfidf, dtype=torch.float32)\n",
        "        label = torch.tensor(label, dtype=torch.long)\n",
        "        return {\"token_ids\": token_ids, \"char_ids\": char_ids, \"aux\": aux, \"tfidf\": tfidf, \"label\": label}\n",
        "\n",
        "# -------------------------\n",
        "# FastText training / loading\n",
        "# -------------------------\n",
        "def train_or_load_fasttext(sentences, path, dim=300, min_count=2, epochs=6):\n",
        "    if os.path.exists(path):\n",
        "        print(\"Loading FastText from\", path)\n",
        "        return FastText.load(path)\n",
        "    print(\"Training FastText...\")\n",
        "    tokenized = [s.split() for s in sentences]\n",
        "    ft = FastText(vector_size=dim, window=5, min_count=min_count, workers=os.cpu_count(), epochs=epochs)\n",
        "    ft.build_vocab(tokenized)\n",
        "    ft.train(tokenized, total_examples=len(tokenized), epochs=epochs)\n",
        "    ft.save(path)\n",
        "    print(\"Saved FastText at\", path)\n",
        "    return ft\n",
        "\n",
        "def build_embedding_matrix(word2idx, ft_model, dim):\n",
        "    V = len(word2idx)\n",
        "    mat = np.random.normal(scale=0.01, size=(V, dim)).astype(np.float32)\n",
        "    found = 0\n",
        "    for w,i in word2idx.items():\n",
        "        if w in ft_model.wv:\n",
        "            mat[i] = ft_model.wv[w]\n",
        "            found += 1\n",
        "    print(f\"Found {found}/{V} tokens in FastText.\")\n",
        "    return mat\n",
        "\n",
        "# -------------------------\n",
        "# Model components (robust CharCNN & BiLSTM)\n",
        "# -------------------------\n",
        "class CharCNN(nn.Module):\n",
        "    def __init__(self, char_vocab_size, char_emb_dim=50, out_dim=100, kernel_sizes=(3,4,5), dropout=0.1, max_chars=12):\n",
        "        super().__init__()\n",
        "        self.char_emb = nn.Embedding(char_vocab_size, char_emb_dim, padding_idx=0)\n",
        "        k = len(kernel_sizes)\n",
        "        base = out_dim // k\n",
        "        extras = out_dim - (base * k)\n",
        "        out_channels_list = [base + (1 if i < extras else 0) for i in range(k)]\n",
        "        self.convs = nn.ModuleList([\n",
        "            nn.Conv2d(in_channels=1, out_channels=out_channels_list[i], kernel_size=(kernel_sizes[i], char_emb_dim))\n",
        "            for i in range(k)\n",
        "        ])\n",
        "        self.out_dim_actual = sum(out_channels_list)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.max_chars = max_chars\n",
        "\n",
        "    def forward(self, x_char):\n",
        "        # x_char: (B, T, C)\n",
        "        B,T,C = x_char.size()\n",
        "        x = self.char_emb(x_char)            # (B, T, C, E)\n",
        "        x = x.view(B*T, C, -1).unsqueeze(1) # (B*T, 1, C, E)\n",
        "        conv_outs = []\n",
        "        for conv in self.convs:\n",
        "            o = conv(x)                      # (B*T, out_ch, L, 1)\n",
        "            o = F.relu(o.squeeze(-1))      # (B*T, out_ch, L)\n",
        "            o = F.max_pool1d(o, o.size(2)).squeeze(2)  # (B*T, out_ch)\n",
        "            conv_outs.append(o)\n",
        "        out = torch.cat(conv_outs, dim=1)  # (B*T, out_dim_actual)\n",
        "        out = out.view(B, T, -1)           # (B, T, out_dim_actual)\n",
        "        out = self.dropout(out)\n",
        "        return out\n",
        "\n",
        "class BiLSTMCharFastText(nn.Module): ### MODIFIED to include TF-IDF ###\n",
        "    def __init__(self, emb_matrix, char_vocab_size, cfg, num_labels):\n",
        "        super().__init__()\n",
        "        emb_matrix = torch.tensor(emb_matrix)\n",
        "        V, E = emb_matrix.shape\n",
        "        self.embedding = nn.Embedding.from_pretrained(emb_matrix, freeze=not cfg.embedding_trainable, padding_idx=0)\n",
        "        self.char_cnn = CharCNN(char_vocab_size, char_emb_dim=cfg.char_emb_dim, out_dim=cfg.char_out, dropout=cfg.dropout, max_chars=cfg.max_chars_per_token)\n",
        "        token_in_dim = E + self.char_cnn.out_dim_actual\n",
        "        self.bilstm = nn.LSTM(token_in_dim, cfg.hidden_dim//2, num_layers=cfg.lstm_layers, bidirectional=True, batch_first=True, dropout=cfg.dropout if cfg.lstm_layers>1 else 0)\n",
        "        self.attn_proj = nn.Linear(cfg.hidden_dim, cfg.attn_dim)\n",
        "        self.attn_v = nn.Linear(cfg.attn_dim, 1, bias=False)\n",
        "        self.aux_proj = nn.Linear(cfg.aux_dim, 32)\n",
        "        self.tfidf_proj = nn.Linear(cfg.tfidf_dim, cfg.tfidf_proj_dim) ### NEW ###\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(cfg.hidden_dim + 32 + cfg.tfidf_proj_dim, 256), ### MODIFIED ###\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(cfg.dropout),\n",
        "            nn.Linear(256, num_labels)\n",
        "        )\n",
        "\n",
        "    def forward(self, token_ids, char_ids, aux, tfidf): ### MODIFIED ###\n",
        "        emb = self.embedding(token_ids)              # (B, T, E)\n",
        "        char_vec = self.char_cnn(char_ids)           # (B, T, char_out_actual)\n",
        "        x = torch.cat([emb, char_vec], dim=-1)       # (B, T, E + char_out_actual)\n",
        "        h, _ = self.bilstm(x)                        # (B, T, H)\n",
        "        a = torch.tanh(self.attn_proj(h))            # (B, T, attn_dim)\n",
        "        scores = self.attn_v(a).squeeze(-1)          # (B, T)\n",
        "        mask = (token_ids != 0).float()              # pad mask\n",
        "        scores = scores.masked_fill(mask==0, -1e9)\n",
        "        alpha = torch.softmax(scores, dim=1).unsqueeze(-1) # (B, T, 1)\n",
        "        pooled = (h * alpha).sum(dim=1)              # (B, H)\n",
        "        aux_p = torch.relu(self.aux_proj(aux))       # (B, 32)\n",
        "        tfidf_p = torch.relu(self.tfidf_proj(tfidf)) ### NEW ### (B, tfidf_proj_dim)\n",
        "        cat = torch.cat([pooled, aux_p, tfidf_p], dim=1)  ### MODIFIED ### (B, H + 32 + tfidf_proj_dim)\n",
        "        logits = self.classifier(cat)                # (B, num_labels)\n",
        "        return logits\n",
        "\n",
        "# -------------------------\n",
        "# Loss: Focal\n",
        "# -------------------------\n",
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, gamma=2.0, alpha=None):\n",
        "        super().__init__()\n",
        "        self.gamma = gamma\n",
        "        if alpha is not None:\n",
        "            self.alpha = torch.tensor(alpha, dtype=torch.float32)\n",
        "        else:\n",
        "            self.alpha = None\n",
        "\n",
        "    def forward(self, logits, targets):\n",
        "        ce = F.cross_entropy(logits, targets, reduction='none')\n",
        "        pt = torch.exp(-ce)\n",
        "        loss = ((1 - pt) ** self.gamma) * ce\n",
        "        if self.alpha is not None:\n",
        "            if self.alpha.device != targets.device:\n",
        "                self.alpha = self.alpha.to(targets.device)\n",
        "            loss = self.alpha[targets] * loss\n",
        "        return loss.mean()\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# Evaluate (returns y_true, y_pred)\n",
        "# -------------------------\n",
        "def evaluate(model, dataloader, device, id2label): ### MODIFIED to include TF-IDF ###\n",
        "    model.eval()\n",
        "    y_true, y_pred = [], []\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(dataloader, desc=\"Eval\", leave=False):\n",
        "            tokens = batch['token_ids'].to(device)\n",
        "            chars  = batch['char_ids'].to(device)\n",
        "            aux    = batch['aux'].to(device)\n",
        "            tfidf  = batch['tfidf'].to(device)\n",
        "            labels = batch['label'].cpu().numpy().tolist()\n",
        "            logits = model(tokens, chars, aux, tfidf)\n",
        "            preds = torch.argmax(logits, dim=1).cpu().numpy().tolist()\n",
        "            y_true.extend(labels); y_pred.extend(preds)\n",
        "    report = classification_report(y_true, y_pred, target_names=list(id2label.values()), digits=4, output_dict=True, zero_division=0)\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    return report, cm, y_true, y_pred\n",
        "\n",
        "# -------------------------\n",
        "# Training loop\n",
        "# -------------------------\n",
        "def train(train_loader, val_loader, model, cfg, class_weights, id2label): ### MODIFIED to include TF-IDF ###\n",
        "    device = cfg.device\n",
        "    model.to(device)\n",
        "    emb_params = list(model.embedding.parameters())\n",
        "    other_params = [p for n,p in model.named_parameters() if not n.startswith('embedding.')]\n",
        "    optimizer = torch.optim.AdamW([\n",
        "        {\"params\": emb_params, \"lr\": cfg.lr_emb},\n",
        "        {\"params\": other_params, \"lr\": cfg.lr_head}\n",
        "    ], weight_decay=cfg.weight_decay)\n",
        "\n",
        "    if cfg.use_focal:\n",
        "        alpha = (1.0 / (class_weights + 1e-9))\n",
        "        alpha = alpha / alpha.sum()\n",
        "        criterion = FocalLoss(cfg.focal_gamma, alpha=alpha)\n",
        "    else:\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    best_ckpt = None\n",
        "    best_macro = -1.0\n",
        "    for epoch in range(cfg.epochs):\n",
        "        model.train()\n",
        "        pbar = tqdm(train_loader, desc=f\"Train E{epoch+1}/{cfg.epochs}\")\n",
        "        for batch in pbar:\n",
        "            tokens = batch['token_ids'].to(device)\n",
        "            chars  = batch['char_ids'].to(device)\n",
        "            aux    = batch['aux'].to(device)\n",
        "            tfidf  = batch['tfidf'].to(device)\n",
        "            labels = batch['label'].to(device)\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(tokens, chars, aux, tfidf)\n",
        "            loss = criterion(logits, labels)\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "            pbar.set_postfix(loss=loss.item())\n",
        "        report, cm, _, _ = evaluate(model, val_loader, cfg.device, id2label)\n",
        "        macro_f1 = report['macro avg']['f1-score']\n",
        "        print(f\"Epoch {epoch+1} -> Val Macro F1: {macro_f1:.4f}\")\n",
        "        if macro_f1 > best_macro:\n",
        "            best_macro = macro_f1\n",
        "            best_ckpt = os.path.join(cfg.output_dir, f\"best_macro_{macro_f1:.4f}.pt\")\n",
        "            torch.save({\"model_state_dict\": model.state_dict(), \"cfg\": cfg.__dict__}, best_ckpt)\n",
        "            print(\"Saved\", best_ckpt)\n",
        "    return best_ckpt\n",
        "\n",
        "# -------------------------\n",
        "# Pipeline orchestration\n",
        "# -------------------------\n",
        "def main():\n",
        "    # load\n",
        "    df = read_data(cfg.data_csv)\n",
        "    # filter tiny classes\n",
        "    cnt = df['label'].value_counts()\n",
        "    keep = cnt[cnt >= cfg.min_class_samples].index.tolist()\n",
        "    if len(keep) < len(cnt):\n",
        "        df = df[df['label'].isin(keep)].reset_index(drop=True)\n",
        "    print(\"Records:\", len(df), \"labels:\", df['label'].nunique())\n",
        "\n",
        "    # label mapping\n",
        "    labels_unique = sorted(df['label'].unique())\n",
        "    label_map = {lab:i for i,lab in enumerate(labels_unique)}\n",
        "    id2label = {v:k for k,v in label_map.items()}\n",
        "\n",
        "    # splits\n",
        "    data = df.to_dict(orient='records')\n",
        "    lablist = [label_map[r['label']] for r in data]\n",
        "    train_idx, test_idx = train_test_split(range(len(data)), test_size=0.15, random_state=cfg.seed, stratify=lablist)\n",
        "    train_idx, val_idx = train_test_split(train_idx, test_size=0.15, random_state=cfg.seed, stratify=[lablist[i] for i in train_idx])\n",
        "    train_records = [data[i] for i in train_idx]\n",
        "    val_records   = [data[i] for i in val_idx]\n",
        "    test_records  = [data[i] for i in test_idx]\n",
        "    print(\"split sizes:\", len(train_records), len(val_records), len(test_records))\n",
        "\n",
        "    ### NEW ###: Compute TF-IDF features\n",
        "    print(\"Computing TF-IDF features...\")\n",
        "    tfidf_vectorizer = TfidfVectorizer(\n",
        "        max_features=cfg.tfidf_dim,\n",
        "        ngram_range=(1, 2),\n",
        "        token_pattern=r'(?u)\\b\\w+\\b'\n",
        "    )\n",
        "    train_texts = [r['text'] for r in train_records]\n",
        "    train_tfidf = tfidf_vectorizer.fit_transform(train_texts)\n",
        "    val_tfidf = tfidf_vectorizer.transform([r['text'] for r in val_records])\n",
        "    test_tfidf = tfidf_vectorizer.transform([r['text'] for r in test_records])\n",
        "    print(\"TF-IDF matrix shape (train):\", train_tfidf.shape)\n",
        "\n",
        "    # vocabs\n",
        "    word2idx = build_token_vocab([r['text'] for r in train_records], cfg.min_token_freq)\n",
        "    char2idx = build_char_vocab([r['text'] for r in train_records], cfg.min_char_freq, cfg.max_chars_per_token)\n",
        "    print(\"Vocab sizes: tokens\", len(word2idx), \"chars\", len(char2idx))\n",
        "\n",
        "    # fasttext\n",
        "    ft_path = os.path.join(cfg.output_dir, \"fasttext.model\")\n",
        "    ft = train_or_load_fasttext([r['text'] for r in data], ft_path, dim=cfg.ft_dim, min_count=cfg.ft_min_count, epochs=cfg.ft_epochs)\n",
        "    emb_matrix = build_embedding_matrix(word2idx, ft, cfg.ft_dim)\n",
        "\n",
        "    # datasets ### MODIFIED to pass TF-IDF matrices ###\n",
        "    max_len = 64  # token length cap - tune as needed\n",
        "    train_ds = CharTokenDataset(train_records, train_tfidf, label_map, word2idx, char2idx, max_len, cfg.max_chars_per_token, cfg.aux_dim)\n",
        "    val_ds   = CharTokenDataset(val_records, val_tfidf, label_map, word2idx, char2idx, max_len, cfg.max_chars_per_token, cfg.aux_dim)\n",
        "    test_ds  = CharTokenDataset(test_records, test_tfidf, label_map, word2idx, char2idx, max_len, cfg.max_chars_per_token, cfg.aux_dim)\n",
        "\n",
        "    if cfg.use_sampler:\n",
        "        labels = [s[4] for s in train_ds.samples] ### MODIFIED ###: Label is now the 5th element (index 4)\n",
        "        cnts = Counter(labels)\n",
        "        sample_weights = [1.0 / cnts[l] for l in labels]\n",
        "        sampler = WeightedRandomSampler(sample_weights, num_samples=len(sample_weights), replacement=True)\n",
        "        train_loader = DataLoader(train_ds, batch_size=cfg.batch_size, sampler=sampler, num_workers=2)\n",
        "    else:\n",
        "        train_loader = DataLoader(train_ds, batch_size=cfg.batch_size, shuffle=True, num_workers=2)\n",
        "\n",
        "    val_loader = DataLoader(val_ds, batch_size=cfg.batch_size*2, shuffle=False, num_workers=2)\n",
        "    test_loader = DataLoader(test_ds, batch_size=cfg.batch_size*2, shuffle=False, num_workers=2)\n",
        "\n",
        "    # class weight vector (counts)\n",
        "    train_labels = [s[4] for s in train_ds.samples] ### MODIFIED ###: Label is now the 5th element (index 4)\n",
        "    cnts = np.array([Counter(train_labels).get(i,0) for i in range(len(label_map))])\n",
        "    class_weights = cnts.astype(np.float32)\n",
        "\n",
        "    # model\n",
        "    model = BiLSTMCharFastText(emb_matrix, char_vocab_size=len(char2idx), cfg=cfg, num_labels=len(label_map))\n",
        "    # quick shape sanity check\n",
        "    print(\"embedding dim:\", emb_matrix.shape[1])\n",
        "    print(\"char_cnn out dim actual:\", model.char_cnn.out_dim_actual)\n",
        "    sample_token_ids = torch.zeros((2, 8), dtype=torch.long)\n",
        "    sample_char_ids = torch.zeros((2, 8, cfg.max_chars_per_token), dtype=torch.long)\n",
        "    sample_aux = torch.zeros((2, cfg.aux_dim), dtype=torch.float)\n",
        "    sample_tfidf = torch.zeros((2, cfg.tfidf_dim), dtype=torch.float) ### NEW ###\n",
        "    with torch.no_grad():\n",
        "        logits_shape = model(sample_token_ids, sample_char_ids, sample_aux, sample_tfidf).shape ### MODIFIED ###\n",
        "    print(\"logits shape (sanity):\", logits_shape)\n",
        "    print(\"Model trainable params:\", sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
        "\n",
        "    # train\n",
        "    best_ckpt = train(train_loader, val_loader, model, cfg, class_weights, id2label)\n",
        "\n",
        "    # test eval\n",
        "    if best_ckpt and os.path.exists(best_ckpt):\n",
        "        ckpt = torch.load(best_ckpt, map_location=cfg.device)\n",
        "        model.load_state_dict(ckpt['model_state_dict'])\n",
        "        report, cm, y_true, y_pred = evaluate(model, test_loader, cfg.device, id2label)\n",
        "        print(\"\\nFinal Test Report:\")\n",
        "        print(classification_report(y_true, y_pred, target_names=list(id2label.values()), digits=4))\n",
        "        print(\"\\nConfusion Matrix:\\n\", cm)\n",
        "\n",
        "        # save predictions csv for inspection\n",
        "        rows = []\n",
        "        for rec, yt, yp in zip(test_records, y_true, y_pred):\n",
        "            rows.append({\"text\": rec['text'], \"label\": id2label[yt], \"pred\": id2label[yp]})\n",
        "        pd.DataFrame(rows).to_csv(os.path.join(cfg.output_dir, \"test_preds.csv\"), index=False)\n",
        "        print(\"Saved test_preds.csv\")\n",
        "    print(\"Done.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Y4KP5JwBc7f",
        "outputId": "aaaa85fc-c968-4155-f814-4e246bd62d99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Records: 44019 labels: 5\n",
            "split sizes: 31803 5613 6603\n",
            "Computing TF-IDF features...\n",
            "TF-IDF matrix shape (train): (31803, 5000)\n",
            "Vocab sizes: tokens 21997 chars 657\n",
            "Training FastText...\n",
            "Saved FastText at outputs_char_bilstm_tfidf/fasttext.model\n",
            "Found 21997/21997 tokens in FastText.\n",
            "embedding dim: 300\n",
            "char_cnn out dim actual: 100\n",
            "logits shape (sanity): torch.Size([2, 5])\n",
            "Model trainable params: 7639749\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train E1/6: 100%|██████████| 497/497 [07:02<00:00,  1.17it/s, loss=0.0734]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 -> Val Macro F1: 0.4474\n",
            "Saved outputs_char_bilstm_tfidf/best_macro_0.4474.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train E2/6: 100%|██████████| 497/497 [07:09<00:00,  1.16it/s, loss=0.0515]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 -> Val Macro F1: 0.4585\n",
            "Saved outputs_char_bilstm_tfidf/best_macro_0.4585.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train E3/6: 100%|██████████| 497/497 [07:09<00:00,  1.16it/s, loss=0.0418]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 -> Val Macro F1: 0.4445\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train E4/6: 100%|██████████| 497/497 [07:09<00:00,  1.16it/s, loss=0.0526]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 -> Val Macro F1: 0.4382\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train E5/6: 100%|██████████| 497/497 [07:09<00:00,  1.16it/s, loss=0.0431]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5 -> Val Macro F1: 0.4228\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train E6/6: 100%|██████████| 497/497 [07:08<00:00,  1.16it/s, loss=0.0288]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6 -> Val Macro F1: 0.4244\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Final Test Report:\n",
            "                precision    recall  f1-score   support\n",
            "\n",
            "Mixed_feelings     0.2392    0.3288    0.2769       739\n",
            "      Negative     0.3090    0.6199    0.4124       784\n",
            "      Positive     0.8576    0.5615    0.6787      3731\n",
            "     not-Tamil     0.3902    0.7668    0.5172       313\n",
            " unknown_state     0.4372    0.4035    0.4197      1036\n",
            "\n",
            "      accuracy                         0.5273      6603\n",
            "     macro avg     0.4466    0.5361    0.4610      6603\n",
            "  weighted avg     0.6351    0.5273    0.5538      6603\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            " [[ 243  275  130   22   69]\n",
            " [ 118  486   76   23   81]\n",
            " [ 503  557 2095  213  363]\n",
            " [   7   15   26  240   25]\n",
            " [ 145  240  116  117  418]]\n",
            "Saved test_preds.csv\n",
            "Done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "p9HdzDK2PNAA"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}