{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "FastText Embedding + BiLSTM + CharCNN Hybrid model"
      ],
      "metadata": {
        "id": "uK1y_pTxxHkc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Step 1: Install necessary libraries ---\n",
        "!pip install torch scikit-learn gensim pandas tqdm -q\n",
        "\n",
        "import os\n",
        "import time\n",
        "import random\n",
        "import json\n",
        "from collections import Counter\n",
        "from dataclasses import dataclass\n",
        "from typing import List\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from gensim.models import FastText\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ------------------------\n",
        "# Configuration\n",
        "# ------------------------\n",
        "@dataclass\n",
        "class CFG:\n",
        "    # --- ADAPTED FOR MALAYALAM ---\n",
        "    data_csv: str = \"mal_full_sentiment.tsv\"\n",
        "    output_dir: str = \"outputs_malayalam_hybrid\"\n",
        "    # ---------------------------\n",
        "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    min_class_samples: int = 3\n",
        "\n",
        "    # Vocab / Chars / Padding\n",
        "    min_token_freq: int = 2\n",
        "    max_chars_per_token: int = 20 # Max characters to consider in a single token\n",
        "    max_length: int = 128         # <<--- FIX: ADDED THIS MISSING ATTRIBUTE\n",
        "\n",
        "    # FastText\n",
        "    ft_dim: int = 300\n",
        "    ft_min_count: int = 2\n",
        "    ft_epochs: int = 10\n",
        "\n",
        "    # Model Architecture\n",
        "    hidden_dim: int = 256\n",
        "    lstm_layers: int = 1\n",
        "    char_emb_dim: int = 50\n",
        "    char_out_dim: int = 100\n",
        "    aux_dim: int = 8\n",
        "    dropout: float = 0.4\n",
        "\n",
        "    # Training\n",
        "    epochs: int = 8\n",
        "    batch_size: int = 64\n",
        "    lr: float = 1e-3\n",
        "    seed: int = 42\n",
        "\n",
        "cfg = CFG()\n",
        "os.makedirs(cfg.output_dir, exist_ok=True)\n",
        "\n",
        "# ------------------------\n",
        "# Utilities\n",
        "# ------------------------\n",
        "def seed_everything(seed=42):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "seed_everything(cfg.seed)\n",
        "\n",
        "def normalize_label(l):\n",
        "    s = str(l).lower()\n",
        "    if 'posit' in s: return 'Positive'\n",
        "    if 'negat' in s: return 'Negative'\n",
        "    if 'neu' in s or 'normal' in s: return 'Neutral'\n",
        "    return 'Other'\n",
        "\n",
        "# ------------------------\n",
        "# Preprocessing & Vocabs\n",
        "# ------------------------\n",
        "def build_vocab(texts: List[str], min_freq: int, vocab_type: str):\n",
        "    print(f\"Building {vocab_type} vocabulary...\")\n",
        "    counts = Counter()\n",
        "    for text in texts:\n",
        "        items = text.split() if vocab_type == 'token' else list(\"\".join(text.split()))\n",
        "        counts.update(items)\n",
        "\n",
        "    vocab = {'<pad>': 0, '<unk>': 1}\n",
        "    for item, count in counts.items():\n",
        "        if count >= min_freq:\n",
        "            vocab[item] = len(vocab)\n",
        "    return vocab\n",
        "\n",
        "def compute_aux_features(text: str) -> List[float]:\n",
        "    toks = text.split()\n",
        "    num_tokens = len(toks)\n",
        "    num_chars = len(text)\n",
        "    has_malayalam = 1.0 if any('\\u0D00' <= ch <= '\\u0D7F' for ch in text) else 0.0\n",
        "    has_english = 1.0 if any('a' <= ch.lower() <= 'z' for ch in text) else 0.0\n",
        "    avg_token_len = (sum(len(t) for t in toks) / num_tokens) if num_tokens > 0 else 0.0\n",
        "    cap_ratio = sum(1 for ch in text if ch.isupper()) / (num_chars + 1e-6)\n",
        "    punct_count = sum(1 for ch in text if ch in '?!.,;:')\n",
        "    emoji_count = sum(1 for ch in text if ord(ch) > 10000)\n",
        "    return [num_tokens, num_chars, has_malayalam, has_english, avg_token_len, cap_ratio, punct_count, emoji_count]\n",
        "\n",
        "# ------------------------\n",
        "# Dataset Class\n",
        "# ------------------------\n",
        "class SentimentDataset(Dataset):\n",
        "    def __init__(self, records, label_map, word2idx, char2idx, cfg):\n",
        "        self.samples = []\n",
        "        for r in records:\n",
        "            text = str(r['text'])\n",
        "            token_ids = [word2idx.get(w, word2idx['<unk>']) for w in text.split()]\n",
        "            char_ids = [char2idx.get(c, char2idx['<unk>']) for c in text]\n",
        "            aux = compute_aux_features(text)\n",
        "            label = label_map[r['label']]\n",
        "            self.samples.append({\"token_ids\": token_ids, \"char_ids\": char_ids, \"aux\": aux, \"label\": label})\n",
        "        self.cfg = cfg\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.samples[idx]\n",
        "        token_ids, char_ids = sample[\"token_ids\"], sample[\"char_ids\"]\n",
        "\n",
        "        # <<--- FIX: Use self.cfg.max_length for padding tokens ---\n",
        "        padded_tokens = token_ids[:self.cfg.max_length] + [0] * (self.cfg.max_length - len(token_ids))\n",
        "        # <<--- FIX: Use self.cfg.max_chars_per_token for padding chars ---\n",
        "        padded_chars = char_ids[:self.cfg.max_chars_per_token] + [0] * (self.cfg.max_chars_per_token - len(char_ids))\n",
        "\n",
        "        return {\n",
        "            \"token_ids\": torch.tensor(padded_tokens, dtype=torch.long),\n",
        "            \"char_ids\": torch.tensor(padded_chars, dtype=torch.long),\n",
        "            \"aux\": torch.tensor(sample[\"aux\"], dtype=torch.float32),\n",
        "            \"label\": torch.tensor(sample[\"label\"], dtype=torch.long)\n",
        "        }\n",
        "\n",
        "# ------------------------\n",
        "# Model Architecture\n",
        "# ------------------------\n",
        "class CharCNN(nn.Module):\n",
        "    def __init__(self, char_vocab_size, char_emb_dim, out_dim):\n",
        "        super().__init__()\n",
        "        self.char_emb = nn.Embedding(char_vocab_size, char_emb_dim, padding_idx=0)\n",
        "        self.conv = nn.Conv1d(char_emb_dim, out_dim, kernel_size=3, padding=1)\n",
        "\n",
        "    def forward(self, x_char):\n",
        "        x = self.char_emb(x_char)\n",
        "        x = x.transpose(1, 2)\n",
        "        x = self.conv(x)\n",
        "        x = F.max_pool1d(x, x.size(2)).squeeze(2)\n",
        "        return x\n",
        "\n",
        "class HybridSentimentClassifier(nn.Module):\n",
        "    def __init__(self, emb_matrix, char_vocab_size, num_labels, cfg):\n",
        "        super().__init__()\n",
        "        emb_matrix = torch.tensor(emb_matrix, dtype=torch.float32)\n",
        "        self.token_embedding = nn.Embedding.from_pretrained(emb_matrix, freeze=False, padding_idx=0)\n",
        "        self.char_cnn = CharCNN(char_vocab_size, cfg.char_emb_dim, cfg.char_out_dim)\n",
        "\n",
        "        lstm_input_dim = emb_matrix.shape[1]\n",
        "        self.bilstm = nn.LSTM(lstm_input_dim, cfg.hidden_dim // 2, num_layers=cfg.lstm_layers,\n",
        "                              bidirectional=True, batch_first=True)\n",
        "\n",
        "        classifier_input_dim = cfg.hidden_dim + cfg.char_out_dim + cfg.aux_dim\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(classifier_input_dim, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(cfg.dropout),\n",
        "            nn.Linear(256, num_labels)\n",
        "        )\n",
        "\n",
        "    def forward(self, token_ids, char_ids, aux):\n",
        "        emb = self.token_embedding(token_ids)\n",
        "        lstm_out, (h_n, _) = self.bilstm(emb)\n",
        "        hidden = torch.cat((h_n[-2,:,:], h_n[-1,:,:]), dim=1)\n",
        "        char_vec = self.char_cnn(char_ids)\n",
        "        combined_features = torch.cat([hidden, char_vec, aux], dim=1)\n",
        "        logits = self.classifier(combined_features)\n",
        "        return logits\n",
        "\n",
        "# ------------------------\n",
        "# Training & Evaluation\n",
        "# ------------------------\n",
        "def evaluate(model, dataloader, device):\n",
        "    model.eval()\n",
        "    y_true, y_pred = [], []\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            token_ids = batch['token_ids'].to(device)\n",
        "            char_ids = batch['char_ids'].to(device)\n",
        "            aux = batch['aux'].to(device)\n",
        "            labels = batch['label'].cpu().numpy()\n",
        "            logits = model(token_ids, char_ids, aux)\n",
        "            preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
        "            y_true.extend(labels)\n",
        "            y_pred.extend(preds)\n",
        "    return y_true, y_pred\n",
        "\n",
        "def train_loop(train_loader, val_loader, model, cfg, label_map):\n",
        "    device = cfg.device\n",
        "    model.to(device)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=cfg.lr)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    best_macro_f1 = -1.0\n",
        "\n",
        "    print(f\"\\n--- Starting Training on {device} ---\")\n",
        "    for epoch in range(cfg.epochs):\n",
        "        model.train()\n",
        "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{cfg.epochs}\")\n",
        "        for batch in pbar:\n",
        "            token_ids = batch['token_ids'].to(device)\n",
        "            char_ids = batch['char_ids'].to(device)\n",
        "            aux = batch['aux'].to(device)\n",
        "            labels = batch['label'].to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(token_ids, char_ids, aux)\n",
        "            loss = criterion(logits, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            pbar.set_postfix(loss=loss.item())\n",
        "\n",
        "        val_true, val_pred = evaluate(model, val_loader, device)\n",
        "        report = classification_report(val_true, val_pred, target_names=label_map.keys(), output_dict=True, zero_division=0)\n",
        "        macro_f1 = report['macro avg']['f1-score']\n",
        "        print(f\"Epoch {epoch+1} -> Val Macro F1: {macro_f1:.4f}\")\n",
        "\n",
        "        if macro_f1 > best_macro_f1:\n",
        "            best_macro_f1 = macro_f1\n",
        "            torch.save(model.state_dict(), os.path.join(cfg.output_dir, \"best_model.pt\"))\n",
        "            print(f\"ðŸš€ New best model saved with Macro F1: {macro_f1:.4f}\")\n",
        "\n",
        "# ------------------------\n",
        "# Main Orchestration\n",
        "# ------------------------\n",
        "def main():\n",
        "    df = pd.read_csv(cfg.data_csv, sep='\\t', header=None, names=['label', 'text'], engine='python')\n",
        "    df.dropna(subset=['text', 'label'], inplace=True)\n",
        "    df['text'] = df['text'].astype(str)\n",
        "    df['label'] = df['label'].apply(normalize_label)\n",
        "    df = df[df['label'] != 'Other']\n",
        "\n",
        "    label_counts = df['label'].value_counts()\n",
        "    classes_to_keep = label_counts[label_counts >= cfg.min_class_samples].index\n",
        "    df = df[df['label'].isin(classes_to_keep)].reset_index(drop=True)\n",
        "\n",
        "    label_map = {label: i for i, label in enumerate(df['label'].unique())}\n",
        "\n",
        "    data = df.to_dict(orient=\"records\")\n",
        "    labels = [label_map[r['label']] for r in data]\n",
        "\n",
        "    train_records, test_records, _, _ = train_test_split(data, labels, test_size=0.2, random_state=cfg.seed, stratify=labels)\n",
        "    train_records, val_records = train_test_split(train_records, test_size=0.15, random_state=cfg.seed, stratify=[label_map[r['label']] for r in train_records])\n",
        "\n",
        "    train_texts = [r['text'] for r in train_records]\n",
        "    word2idx = build_vocab(train_texts, cfg.min_token_freq, 'token')\n",
        "    char2idx = build_vocab(train_texts, 1, 'char')\n",
        "\n",
        "    ft_path = os.path.join(cfg.output_dir, \"fasttext.model\")\n",
        "    if not os.path.exists(ft_path):\n",
        "        print(\"Training FastText model...\")\n",
        "        all_texts_for_ft = [r['text'].split() for r in data]\n",
        "        ft_model = FastText(sentences=all_texts_for_ft, vector_size=cfg.ft_dim, window=5, min_count=cfg.ft_min_count, workers=4, sg=1, epochs=cfg.ft_epochs)\n",
        "        ft_model.save(ft_path)\n",
        "    else:\n",
        "        print(\"Loading existing FastText model...\")\n",
        "        ft_model = FastText.load(ft_path)\n",
        "\n",
        "    embedding_matrix = np.random.normal(scale=0.02, size=(len(word2idx), cfg.ft_dim))\n",
        "    for word, i in word2idx.items():\n",
        "        if word in ft_model.wv:\n",
        "            embedding_matrix[i] = ft_model.wv[word]\n",
        "\n",
        "    train_ds = SentimentDataset(train_records, label_map, word2idx, char2idx, cfg)\n",
        "    val_ds = SentimentDataset(val_records, label_map, word2idx, char2idx, cfg)\n",
        "    test_ds = SentimentDataset(test_records, label_map, word2idx, char2idx, cfg)\n",
        "\n",
        "    train_loader = DataLoader(train_ds, batch_size=cfg.batch_size, shuffle=True, num_workers=2)\n",
        "    val_loader = DataLoader(val_ds, batch_size=cfg.batch_size * 2, shuffle=False, num_workers=2)\n",
        "    test_loader = DataLoader(test_ds, batch_size=cfg.batch_size * 2, shuffle=False, num_workers=2)\n",
        "\n",
        "    model = HybridSentimentClassifier(embedding_matrix, len(char2idx), len(label_map), cfg)\n",
        "    train_loop(train_loader, val_loader, model, cfg, label_map)\n",
        "\n",
        "    print(\"\\n--- Final Test Set Evaluation ---\")\n",
        "    model.load_state_dict(torch.load(os.path.join(cfg.output_dir, \"best_model.pt\")))\n",
        "    test_true, test_pred = evaluate(model, test_loader, cfg.device)\n",
        "    print(classification_report(test_true, test_pred, target_names=label_map.keys()))\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2L7AzPpuI9_l",
        "outputId": "b45a0d2f-673b-42e9-ad3a-0594f6c9c5e9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Building token vocabulary...\n",
            "Building char vocabulary...\n",
            "Loading existing FastText model...\n",
            "\n",
            "--- Starting Training on cuda ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/8: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 112/112 [00:03<00:00, 34.47it/s, loss=0.492]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 -> Val Macro F1: 0.7226\n",
            "ðŸš€ New best model saved with Macro F1: 0.7226\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/8: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 112/112 [00:01<00:00, 67.88it/s, loss=0.347]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 -> Val Macro F1: 0.7711\n",
            "ðŸš€ New best model saved with Macro F1: 0.7711\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/8: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 112/112 [00:01<00:00, 61.65it/s, loss=0.233]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 -> Val Macro F1: 0.7966\n",
            "ðŸš€ New best model saved with Macro F1: 0.7966\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/8: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 112/112 [00:02<00:00, 55.16it/s, loss=0.153]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 -> Val Macro F1: 0.7789\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/8: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 112/112 [00:01<00:00, 67.12it/s, loss=0.135]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5 -> Val Macro F1: 0.7641\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/8: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 112/112 [00:01<00:00, 67.72it/s, loss=0.028]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6 -> Val Macro F1: 0.7535\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/8: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 112/112 [00:01<00:00, 67.57it/s, loss=0.0539]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7 -> Val Macro F1: 0.7583\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8/8: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 112/112 [00:01<00:00, 67.50it/s, loss=0.0406]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8 -> Val Macro F1: 0.7751\n",
            "\n",
            "--- Final Test Set Evaluation ---\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Positive       0.91      0.86      0.88      1582\n",
            "    Negative       0.62      0.73      0.67       520\n",
            "\n",
            "    accuracy                           0.82      2102\n",
            "   macro avg       0.76      0.79      0.78      2102\n",
            "weighted avg       0.84      0.82      0.83      2102\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "FastText Embedding + BiGRU + CharCNN Hybrid Model"
      ],
      "metadata": {
        "id": "AXwIf2TZxAix"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Step 1: Install necessary libraries ---\n",
        "!pip install torch scikit-learn gensim pandas tqdm -q\n",
        "\n",
        "import os\n",
        "import time\n",
        "import random\n",
        "import json\n",
        "from collections import Counter\n",
        "from dataclasses import dataclass\n",
        "from typing import List\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from gensim.models import FastText\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ------------------------\n",
        "# Configuration\n",
        "# ------------------------\n",
        "@dataclass\n",
        "class CFG:\n",
        "    data_csv: str = \"mal_full_sentiment.tsv\"\n",
        "    output_dir: str = \"outputs_malayalam_hybrid_bigru\"\n",
        "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    min_class_samples: int = 3\n",
        "\n",
        "    # Vocab / Chars / Padding\n",
        "    min_token_freq: int = 2\n",
        "    max_chars_per_token: int = 20\n",
        "    max_length: int = 128\n",
        "\n",
        "    # FastText\n",
        "    ft_dim: int = 300\n",
        "    ft_min_count: int = 2\n",
        "    ft_epochs: int = 10\n",
        "\n",
        "    # Model Architecture\n",
        "    hidden_dim: int = 256\n",
        "    # --- MODIFIED: This now controls the GRU layers ---\n",
        "    gru_layers: int = 1\n",
        "    char_emb_dim: int = 50\n",
        "    char_out_dim: int = 100\n",
        "    aux_dim: int = 8\n",
        "    dropout: float = 0.4\n",
        "\n",
        "    # Training\n",
        "    epochs: int = 8\n",
        "    batch_size: int = 64\n",
        "    lr: float = 1e-3\n",
        "    seed: int = 42\n",
        "\n",
        "cfg = CFG()\n",
        "os.makedirs(cfg.output_dir, exist_ok=True)\n",
        "\n",
        "# ------------------------\n",
        "# Utilities\n",
        "# ------------------------\n",
        "def seed_everything(seed=42):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "seed_everything(cfg.seed)\n",
        "\n",
        "def normalize_label(l):\n",
        "    s = str(l).lower()\n",
        "    if 'posit' in s: return 'Positive'\n",
        "    if 'negat' in s: return 'Negative'\n",
        "    if 'neu' in s or 'normal' in s: return 'Neutral'\n",
        "    return 'Other'\n",
        "\n",
        "# ------------------------\n",
        "# Preprocessing & Vocabs\n",
        "# ------------------------\n",
        "def build_vocab(texts: List[str], min_freq: int, vocab_type: str):\n",
        "    print(f\"Building {vocab_type} vocabulary...\")\n",
        "    counts = Counter()\n",
        "    for text in texts:\n",
        "        items = text.split() if vocab_type == 'token' else list(\"\".join(text.split()))\n",
        "        counts.update(items)\n",
        "\n",
        "    vocab = {'<pad>': 0, '<unk>': 1}\n",
        "    for item, count in counts.items():\n",
        "        if count >= min_freq:\n",
        "            vocab[item] = len(vocab)\n",
        "    return vocab\n",
        "\n",
        "def compute_aux_features(text: str) -> List[float]:\n",
        "    toks = text.split()\n",
        "    num_tokens = len(toks)\n",
        "    num_chars = len(text)\n",
        "    has_malayalam = 1.0 if any('\\u0D00' <= ch <= '\\u0D7F' for ch in text) else 0.0\n",
        "    has_english = 1.0 if any('a' <= ch.lower() <= 'z' for ch in text) else 0.0\n",
        "    avg_token_len = (sum(len(t) for t in toks) / num_tokens) if num_tokens > 0 else 0.0\n",
        "    cap_ratio = sum(1 for ch in text if ch.isupper()) / (num_chars + 1e-6)\n",
        "    punct_count = sum(1 for ch in text if ch in '?!.,;:')\n",
        "    emoji_count = sum(1 for ch in text if ord(ch) > 10000)\n",
        "    return [num_tokens, num_chars, has_malayalam, has_english, avg_token_len, cap_ratio, punct_count, emoji_count]\n",
        "\n",
        "# ------------------------\n",
        "# Dataset Class\n",
        "# ------------------------\n",
        "class SentimentDataset(Dataset):\n",
        "    def __init__(self, records, label_map, word2idx, char2idx, cfg):\n",
        "        self.samples = []\n",
        "        for r in records:\n",
        "            text = str(r['text'])\n",
        "            token_ids = [word2idx.get(w, word2idx['<unk>']) for w in text.split()]\n",
        "            char_ids = [char2idx.get(c, char2idx['<unk>']) for c in text]\n",
        "            aux = compute_aux_features(text)\n",
        "            label = label_map[r['label']]\n",
        "            self.samples.append({\"token_ids\": token_ids, \"char_ids\": char_ids, \"aux\": aux, \"label\": label})\n",
        "        self.cfg = cfg\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.samples[idx]\n",
        "        token_ids, char_ids = sample[\"token_ids\"], sample[\"char_ids\"]\n",
        "        padded_tokens = token_ids[:self.cfg.max_length] + [0] * (self.cfg.max_length - len(token_ids))\n",
        "        padded_chars = char_ids[:self.cfg.max_chars_per_token] + [0] * (self.cfg.max_chars_per_token - len(char_ids))\n",
        "        return {\n",
        "            \"token_ids\": torch.tensor(padded_tokens, dtype=torch.long),\n",
        "            \"char_ids\": torch.tensor(padded_chars, dtype=torch.long),\n",
        "            \"aux\": torch.tensor(sample[\"aux\"], dtype=torch.float32),\n",
        "            \"label\": torch.tensor(sample[\"label\"], dtype=torch.long)\n",
        "        }\n",
        "\n",
        "# ------------------------\n",
        "# Model Architecture\n",
        "# ------------------------\n",
        "class CharCNN(nn.Module):\n",
        "    def __init__(self, char_vocab_size, char_emb_dim, out_dim):\n",
        "        super().__init__()\n",
        "        self.char_emb = nn.Embedding(char_vocab_size, char_emb_dim, padding_idx=0)\n",
        "        self.conv = nn.Conv1d(char_emb_dim, out_dim, kernel_size=3, padding=1)\n",
        "\n",
        "    def forward(self, x_char):\n",
        "        x = self.char_emb(x_char)\n",
        "        x = x.transpose(1, 2)\n",
        "        x = self.conv(x)\n",
        "        x = F.max_pool1d(x, x.size(2)).squeeze(2)\n",
        "        return x\n",
        "\n",
        "class HybridSentimentClassifier(nn.Module):\n",
        "    def __init__(self, emb_matrix, char_vocab_size, num_labels, cfg):\n",
        "        super().__init__()\n",
        "        emb_matrix = torch.tensor(emb_matrix, dtype=torch.float32)\n",
        "        self.token_embedding = nn.Embedding.from_pretrained(emb_matrix, freeze=False, padding_idx=0)\n",
        "        self.char_cnn = CharCNN(char_vocab_size, cfg.char_emb_dim, cfg.char_out_dim)\n",
        "\n",
        "        recurrent_input_dim = emb_matrix.shape[1]\n",
        "\n",
        "        # --- MODIFIED: Using BiGRU instead of BiLSTM ---\n",
        "        self.bigru = nn.GRU(\n",
        "            recurrent_input_dim,\n",
        "            cfg.hidden_dim // 2,\n",
        "            num_layers=cfg.gru_layers,\n",
        "            bidirectional=True,\n",
        "            batch_first=True\n",
        "        )\n",
        "        # ---------------------------------------------\n",
        "\n",
        "        classifier_input_dim = cfg.hidden_dim + cfg.char_out_dim + cfg.aux_dim\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(classifier_input_dim, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(cfg.dropout),\n",
        "            nn.Linear(256, num_labels)\n",
        "        )\n",
        "\n",
        "    def forward(self, token_ids, char_ids, aux):\n",
        "        emb = self.token_embedding(token_ids)\n",
        "\n",
        "        # --- MODIFIED: Using BiGRU instead of BiLSTM ---\n",
        "        # A GRU returns (output, h_n) instead of (output, (h_n, c_n))\n",
        "        _, h_n = self.bigru(emb)\n",
        "        # ---------------------------------------------\n",
        "\n",
        "        hidden = torch.cat((h_n[-2,:,:], h_n[-1,:,:]), dim=1)\n",
        "        char_vec = self.char_cnn(char_ids)\n",
        "        combined_features = torch.cat([hidden, char_vec, aux], dim=1)\n",
        "        logits = self.classifier(combined_features)\n",
        "        return logits\n",
        "\n",
        "# ------------------------\n",
        "# Training & Evaluation\n",
        "# ------------------------\n",
        "def evaluate(model, dataloader, device):\n",
        "    model.eval()\n",
        "    y_true, y_pred = [], []\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            token_ids = batch['token_ids'].to(device)\n",
        "            char_ids = batch['char_ids'].to(device)\n",
        "            aux = batch['aux'].to(device)\n",
        "            labels = batch['label'].cpu().numpy()\n",
        "            logits = model(token_ids, char_ids, aux)\n",
        "            preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
        "            y_true.extend(labels)\n",
        "            y_pred.extend(preds)\n",
        "    return y_true, y_pred\n",
        "\n",
        "def train_loop(train_loader, val_loader, model, cfg, label_map):\n",
        "    device = cfg.device\n",
        "    model.to(device)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=cfg.lr)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    best_macro_f1 = -1.0\n",
        "\n",
        "    print(f\"\\n--- Starting Training on {device} ---\")\n",
        "    for epoch in range(cfg.epochs):\n",
        "        model.train()\n",
        "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{cfg.epochs}\")\n",
        "        for batch in pbar:\n",
        "            token_ids = batch['token_ids'].to(device)\n",
        "            char_ids = batch['char_ids'].to(device)\n",
        "            aux = batch['aux'].to(device)\n",
        "            labels = batch['label'].to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(token_ids, char_ids, aux)\n",
        "            loss = criterion(logits, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            pbar.set_postfix(loss=loss.item())\n",
        "\n",
        "        val_true, val_pred = evaluate(model, val_loader, device)\n",
        "        report = classification_report(val_true, val_pred, target_names=label_map.keys(), output_dict=True, zero_division=0)\n",
        "        macro_f1 = report['macro avg']['f1-score']\n",
        "        print(f\"Epoch {epoch+1} -> Val Macro F1: {macro_f1:.4f}\")\n",
        "\n",
        "        if macro_f1 > best_macro_f1:\n",
        "            best_macro_f1 = macro_f1\n",
        "            torch.save(model.state_dict(), os.path.join(cfg.output_dir, \"best_model.pt\"))\n",
        "            print(f\"ðŸš€ New best model saved with Macro F1: {macro_f1:.4f}\")\n",
        "\n",
        "# ------------------------\n",
        "# Main Orchestration\n",
        "# ------------------------\n",
        "def main():\n",
        "    df = pd.read_csv(cfg.data_csv, sep='\\t', header=None, names=['label', 'text'], engine='python')\n",
        "    df.dropna(subset=['text', 'label'], inplace=True)\n",
        "    df['text'] = df['text'].astype(str)\n",
        "    df['label'] = df['label'].apply(normalize_label)\n",
        "    df = df[df['label'] != 'Other']\n",
        "\n",
        "    label_counts = df['label'].value_counts()\n",
        "    classes_to_keep = label_counts[label_counts >= cfg.min_class_samples].index\n",
        "    df = df[df['label'].isin(classes_to_keep)].reset_index(drop=True)\n",
        "\n",
        "    label_map = {label: i for i, label in enumerate(df['label'].unique())}\n",
        "\n",
        "    data = df.to_dict(orient=\"records\")\n",
        "    labels = [label_map[r['label']] for r in data]\n",
        "\n",
        "    train_records, test_records, _, _ = train_test_split(data, labels, test_size=0.2, random_state=cfg.seed, stratify=labels)\n",
        "    train_records, val_records = train_test_split(train_records, test_size=0.15, random_state=cfg.seed, stratify=[label_map[r['label']] for r in train_records])\n",
        "\n",
        "    train_texts = [r['text'] for r in train_records]\n",
        "    word2idx = build_vocab(train_texts, cfg.min_token_freq, 'token')\n",
        "    char2idx = build_vocab(train_texts, 1, 'char')\n",
        "\n",
        "    ft_path = os.path.join(cfg.output_dir, \"fasttext.model\")\n",
        "    if not os.path.exists(ft_path):\n",
        "        print(\"Training FastText model...\")\n",
        "        all_texts_for_ft = [r['text'].split() for r in data]\n",
        "        ft_model = FastText(sentences=all_texts_for_ft, vector_size=cfg.ft_dim, window=5, min_count=cfg.ft_min_count, workers=4, sg=1, epochs=cfg.ft_epochs)\n",
        "        ft_model.save(ft_path)\n",
        "    else:\n",
        "        print(\"Loading existing FastText model...\")\n",
        "        ft_model = FastText.load(ft_path)\n",
        "\n",
        "    embedding_matrix = np.random.normal(scale=0.02, size=(len(word2idx), cfg.ft_dim))\n",
        "    for word, i in word2idx.items():\n",
        "        if word in ft_model.wv:\n",
        "            embedding_matrix[i] = ft_model.wv[word]\n",
        "\n",
        "    train_ds = SentimentDataset(train_records, label_map, word2idx, char2idx, cfg)\n",
        "    val_ds = SentimentDataset(val_records, label_map, word2idx, char2idx, cfg)\n",
        "    test_ds = SentimentDataset(test_records, label_map, word2idx, char2idx, cfg)\n",
        "\n",
        "    train_loader = DataLoader(train_ds, batch_size=cfg.batch_size, shuffle=True, num_workers=2)\n",
        "    val_loader = DataLoader(val_ds, batch_size=cfg.batch_size * 2, shuffle=False, num_workers=2)\n",
        "    test_loader = DataLoader(test_ds, batch_size=cfg.batch_size * 2, shuffle=False, num_workers=2)\n",
        "\n",
        "    model = HybridSentimentClassifier(embedding_matrix, len(char2idx), len(label_map), cfg)\n",
        "    train_loop(train_loader, val_loader, model, cfg, label_map)\n",
        "\n",
        "    print(\"\\n--- Final Test Set Evaluation ---\")\n",
        "    model.load_state_dict(torch.load(os.path.join(cfg.output_dir, \"best_model.pt\")))\n",
        "    test_true, test_pred = evaluate(model, test_loader, cfg.device)\n",
        "    print(classification_report(test_true, test_pred, target_names=label_map.keys()))\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F_pgaa6sKgkh",
        "outputId": "46f70cad-113a-4089-afee-32476c5e27a5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Building token vocabulary...\n",
            "Building char vocabulary...\n",
            "Training FastText model...\n",
            "\n",
            "--- Starting Training on cuda ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/8: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 112/112 [00:01<00:00, 75.63it/s, loss=0.302]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 -> Val Macro F1: 0.7114\n",
            "ðŸš€ New best model saved with Macro F1: 0.7114\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/8: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 112/112 [00:01<00:00, 77.29it/s, loss=0.264]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 -> Val Macro F1: 0.7914\n",
            "ðŸš€ New best model saved with Macro F1: 0.7914\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/8: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 112/112 [00:01<00:00, 61.25it/s, loss=0.158]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 -> Val Macro F1: 0.7873\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/8: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 112/112 [00:01<00:00, 68.87it/s, loss=0.151]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 -> Val Macro F1: 0.7732\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/8: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 112/112 [00:01<00:00, 83.16it/s, loss=0.0421]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5 -> Val Macro F1: 0.7853\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/8: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 112/112 [00:01<00:00, 82.44it/s, loss=0.0054]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6 -> Val Macro F1: 0.7839\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/8: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 112/112 [00:01<00:00, 83.21it/s, loss=0.0219]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7 -> Val Macro F1: 0.7770\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8/8: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 112/112 [00:01<00:00, 81.38it/s, loss=0.139]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8 -> Val Macro F1: 0.7687\n",
            "\n",
            "--- Final Test Set Evaluation ---\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Positive       0.89      0.89      0.89      1582\n",
            "    Negative       0.66      0.67      0.66       520\n",
            "\n",
            "    accuracy                           0.83      2102\n",
            "   macro avg       0.77      0.78      0.78      2102\n",
            "weighted avg       0.83      0.83      0.83      2102\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "FastText Embedding + BiLSTM + CharCNN + TF-IDF Hybrid Model"
      ],
      "metadata": {
        "id": "IiP80JinwXvo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Step 1: Install necessary libraries ---\n",
        "!pip install torch scikit-learn gensim pandas tqdm -q\n",
        "\n",
        "import os\n",
        "import time\n",
        "import random\n",
        "import json\n",
        "from collections import Counter\n",
        "from dataclasses import dataclass\n",
        "from typing import List\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer # <-- NEW IMPORT\n",
        "from gensim.models import FastText\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ------------------------\n",
        "# Configuration\n",
        "# ------------------------\n",
        "@dataclass\n",
        "class CFG:\n",
        "    data_csv: str = \"mal_full_sentiment.tsv\"\n",
        "    output_dir: str = \"outputs_malayalam_final_hybrid\"\n",
        "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    min_class_samples: int = 3\n",
        "\n",
        "    # Vocab / Chars / Padding\n",
        "    min_token_freq: int = 2\n",
        "    max_chars_per_token: int = 20\n",
        "    max_length: int = 128\n",
        "\n",
        "    # --- NEW: TF-IDF Config ---\n",
        "    tfidf_max_features: int = 5000\n",
        "    tfidf_proj_dim: int = 64\n",
        "    # --------------------------\n",
        "\n",
        "    # FastText\n",
        "    ft_dim: int = 300\n",
        "    ft_min_count: int = 2\n",
        "    ft_epochs: int = 10\n",
        "\n",
        "    # Model Architecture\n",
        "    hidden_dim: int = 256\n",
        "    lstm_layers: int = 1\n",
        "    char_emb_dim: int = 50\n",
        "    char_out_dim: int = 100\n",
        "    aux_dim: int = 8\n",
        "    dropout: float = 0.4\n",
        "\n",
        "    # Training\n",
        "    epochs: int = 8\n",
        "    batch_size: int = 64\n",
        "    lr: float = 1e-3\n",
        "    seed: int = 42\n",
        "\n",
        "cfg = CFG()\n",
        "os.makedirs(cfg.output_dir, exist_ok=True)\n",
        "\n",
        "# ------------------------\n",
        "# Utilities\n",
        "# ------------------------\n",
        "def seed_everything(seed=42):\n",
        "    random.seed(seed); os.environ['PYTHONHASHSEED'] = str(seed); np.random.seed(seed); torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "seed_everything(cfg.seed)\n",
        "\n",
        "def normalize_label(l):\n",
        "    s = str(l).lower()\n",
        "    if 'posit' in s: return 'Positive'\n",
        "    if 'negat' in s: return 'Negative'\n",
        "    if 'neu' in s or 'normal' in s: return 'Neutral'\n",
        "    return 'Other'\n",
        "\n",
        "# ------------------------\n",
        "# Preprocessing & Vocabs\n",
        "# ------------------------\n",
        "def build_vocab(texts: List[str], min_freq: int, vocab_type: str):\n",
        "    counts = Counter(item for text in texts for item in (text.split() if vocab_type == 'token' else list(\"\".join(text.split()))))\n",
        "    vocab = {'<pad>': 0, '<unk>': 1}\n",
        "    for item, count in counts.items():\n",
        "        if count >= min_freq: vocab[item] = len(vocab)\n",
        "    return vocab\n",
        "\n",
        "def compute_aux_features(text: str) -> List[float]:\n",
        "    toks = text.split(); num_tokens = len(toks); num_chars = len(text)\n",
        "    return [\n",
        "        num_tokens, num_chars,\n",
        "        1.0 if any('\\u0D00' <= ch <= '\\u0D7F' for ch in text) else 0.0, # has_malayalam\n",
        "        1.0 if any('a' <= ch.lower() <= 'z' for ch in text) else 0.0, # has_english\n",
        "        (sum(len(t) for t in toks) / num_tokens) if num_tokens > 0 else 0.0, # avg_token_len\n",
        "        sum(1 for ch in text if ch.isupper()) / (num_chars + 1e-6), # cap_ratio\n",
        "        sum(1 for ch in text if ch in '?!.,;:'), # punct_count\n",
        "        sum(1 for ch in text if ord(ch) > 10000) # emoji_count\n",
        "    ]\n",
        "\n",
        "# ------------------------\n",
        "# Dataset Class - MODIFIED for TF-IDF\n",
        "# ------------------------\n",
        "class SentimentDataset(Dataset):\n",
        "    def __init__(self, records, tfidf_vectors, label_map, word2idx, char2idx, cfg):\n",
        "        self.samples = []\n",
        "        for i, r in enumerate(records):\n",
        "            text = str(r['text'])\n",
        "            self.samples.append({\n",
        "                \"token_ids\": [word2idx.get(w, word2idx['<unk>']) for w in text.split()],\n",
        "                \"char_ids\": [char2idx.get(c, char2idx['<unk>']) for c in text],\n",
        "                \"aux\": compute_aux_features(text),\n",
        "                \"tfidf\": tfidf_vectors[i].toarray().squeeze(), # <-- NEW\n",
        "                \"label\": label_map[r['label']]\n",
        "            })\n",
        "        self.cfg = cfg\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.samples[idx]\n",
        "        token_ids = sample[\"token_ids\"][:self.cfg.max_length]\n",
        "        char_ids = sample[\"char_ids\"][:self.cfg.max_chars_per_token]\n",
        "        return {\n",
        "            \"token_ids\": torch.tensor(token_ids + [0] * (self.cfg.max_length - len(token_ids)), dtype=torch.long),\n",
        "            \"char_ids\": torch.tensor(char_ids + [0] * (self.cfg.max_chars_per_token - len(char_ids)), dtype=torch.long),\n",
        "            \"aux\": torch.tensor(sample[\"aux\"], dtype=torch.float32),\n",
        "            \"tfidf\": torch.tensor(sample[\"tfidf\"], dtype=torch.float32), # <-- NEW\n",
        "            \"label\": torch.tensor(sample[\"label\"], dtype=torch.long)\n",
        "        }\n",
        "\n",
        "# ------------------------\n",
        "# Model Architecture - MODIFIED for TF-IDF\n",
        "# ------------------------\n",
        "class CharCNN(nn.Module):\n",
        "    def __init__(self, char_vocab_size, char_emb_dim, out_dim):\n",
        "        super().__init__(); self.char_emb = nn.Embedding(char_vocab_size, char_emb_dim, padding_idx=0); self.conv = nn.Conv1d(char_emb_dim, out_dim, kernel_size=3, padding=1)\n",
        "    def forward(self, x_char):\n",
        "        x = self.char_emb(x_char).transpose(1, 2); x = self.conv(x); return F.max_pool1d(x, x.size(2)).squeeze(2)\n",
        "\n",
        "class HybridSentimentClassifier(nn.Module):\n",
        "    def __init__(self, emb_matrix, char_vocab_size, num_labels, cfg):\n",
        "        super().__init__()\n",
        "        self.token_embedding = nn.Embedding.from_pretrained(torch.tensor(emb_matrix, dtype=torch.float32), freeze=False, padding_idx=0)\n",
        "        self.char_cnn = CharCNN(char_vocab_size, cfg.char_emb_dim, cfg.char_out_dim)\n",
        "        self.bilstm = nn.LSTM(emb_matrix.shape[1], cfg.hidden_dim // 2, num_layers=cfg.lstm_layers, bidirectional=True, batch_first=True)\n",
        "        # --- NEW: TF-IDF and Aux projections ---\n",
        "        self.tfidf_proj = nn.Linear(cfg.tfidf_max_features, cfg.tfidf_proj_dim)\n",
        "        self.aux_proj = nn.Linear(cfg.aux_dim, cfg.aux_dim)\n",
        "        # -----------------------------------\n",
        "        classifier_input_dim = cfg.hidden_dim + cfg.char_out_dim + cfg.aux_dim + cfg.tfidf_proj_dim\n",
        "        self.classifier = nn.Sequential(nn.Linear(classifier_input_dim, 256), nn.ReLU(), nn.Dropout(cfg.dropout), nn.Linear(256, num_labels))\n",
        "\n",
        "    def forward(self, token_ids, char_ids, aux, tfidf):\n",
        "        emb = self.token_embedding(token_ids)\n",
        "        _, (h_n, _) = self.bilstm(emb)\n",
        "        hidden = torch.cat((h_n[-2,:,:], h_n[-1,:,:]), dim=1)\n",
        "        char_vec = self.char_cnn(char_ids)\n",
        "        # --- NEW: Project and combine all features ---\n",
        "        aux_vec = F.relu(self.aux_proj(aux))\n",
        "        tfidf_vec = F.relu(self.tfidf_proj(tfidf))\n",
        "        combined_features = torch.cat([hidden, char_vec, aux_vec, tfidf_vec], dim=1)\n",
        "        # -------------------------------------------\n",
        "        return self.classifier(combined_features)\n",
        "\n",
        "# ------------------------\n",
        "# Training & Evaluation - MODIFIED for TF-IDF\n",
        "# ------------------------\n",
        "def evaluate(model, dataloader, device):\n",
        "    model.eval(); y_true, y_pred = [], []\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            logits = model(batch['token_ids'].to(device), batch['char_ids'].to(device), batch['aux'].to(device), batch['tfidf'].to(device))\n",
        "            y_true.extend(batch['label'].numpy()); y_pred.extend(torch.argmax(logits, dim=1).cpu().numpy())\n",
        "    return y_true, y_pred\n",
        "\n",
        "def train_loop(train_loader, val_loader, model, cfg, label_map):\n",
        "    device = cfg.device; model.to(device); optimizer = torch.optim.AdamW(model.parameters(), lr=cfg.lr); criterion = nn.CrossEntropyLoss(); best_macro_f1 = -1.0\n",
        "    print(f\"\\n--- Starting Training on {device} ---\")\n",
        "    for epoch in range(cfg.epochs):\n",
        "        model.train()\n",
        "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{cfg.epochs}\")\n",
        "        for batch in pbar:\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(batch['token_ids'].to(device), batch['char_ids'].to(device), batch['aux'].to(device), batch['tfidf'].to(device))\n",
        "            loss = criterion(logits, batch['label'].to(device))\n",
        "            loss.backward(); optimizer.step(); pbar.set_postfix(loss=loss.item())\n",
        "        val_true, val_pred = evaluate(model, val_loader, device)\n",
        "        report = classification_report(val_true, val_pred, target_names=label_map.keys(), output_dict=True, zero_division=0)\n",
        "        macro_f1 = report['macro avg']['f1-score']\n",
        "        print(f\"Epoch {epoch+1} -> Val Macro F1: {macro_f1:.4f}\")\n",
        "        if macro_f1 > best_macro_f1:\n",
        "            best_macro_f1 = macro_f1; torch.save(model.state_dict(), os.path.join(cfg.output_dir, \"best_model.pt\")); print(f\"ðŸš€ New best model saved with Macro F1: {macro_f1:.4f}\")\n",
        "\n",
        "# ------------------------\n",
        "# Main Orchestration\n",
        "# ------------------------\n",
        "def main():\n",
        "    df = pd.read_csv(cfg.data_csv, sep='\\t', header=None, names=['label', 'text'], engine='python')\n",
        "    df.dropna(subset=['text', 'label'], inplace=True); df['text'] = df['text'].astype(str); df['label'] = df['label'].apply(normalize_label); df = df[df['label'] != 'Other']\n",
        "    label_counts = df['label'].value_counts(); classes_to_keep = label_counts[label_counts >= cfg.min_class_samples].index; df = df[df['label'].isin(classes_to_keep)].reset_index(drop=True)\n",
        "    label_map = {label: i for i, label in enumerate(df['label'].unique())}\n",
        "    data = df.to_dict(orient=\"records\"); labels = [label_map[r['label']] for r in data]\n",
        "    train_records, test_records, train_labels, _ = train_test_split(data, labels, test_size=0.2, random_state=cfg.seed, stratify=labels)\n",
        "    train_records, val_records = train_test_split(train_records, test_size=0.15, random_state=cfg.seed, stratify=[label_map[r['label']] for r in train_records])\n",
        "\n",
        "    # --- NEW: Fit TF-IDF on training data only ---\n",
        "    print(\"Fitting TF-IDF Vectorizer...\")\n",
        "    train_texts = [r['text'] for r in train_records]\n",
        "    tfidf_vectorizer = TfidfVectorizer(max_features=cfg.tfidf_max_features, ngram_range=(1, 2))\n",
        "    train_tfidf = tfidf_vectorizer.fit_transform(train_texts)\n",
        "    val_tfidf = tfidf_vectorizer.transform([r['text'] for r in val_records])\n",
        "    test_tfidf = tfidf_vectorizer.transform([r['text'] for r in test_records])\n",
        "    # ---------------------------------------------\n",
        "\n",
        "    train_texts = [r['text'] for r in train_records]\n",
        "    word2idx = build_vocab(train_texts, cfg.min_token_freq, 'token'); char2idx = build_vocab(train_texts, 1, 'char')\n",
        "\n",
        "    ft_path = os.path.join(cfg.output_dir, \"fasttext.model\")\n",
        "    if not os.path.exists(ft_path):\n",
        "        print(\"Training FastText model...\"); all_texts_for_ft = [r['text'].split() for r in data]; ft_model = FastText(sentences=all_texts_for_ft, vector_size=cfg.ft_dim, window=5, min_count=cfg.ft_min_count, workers=4, sg=1, epochs=cfg.ft_epochs); ft_model.save(ft_path)\n",
        "    else: print(\"Loading existing FastText model...\"); ft_model = FastText.load(ft_path)\n",
        "\n",
        "    embedding_matrix = np.random.normal(scale=0.02, size=(len(word2idx), cfg.ft_dim))\n",
        "    for word, i in word2idx.items():\n",
        "        if word in ft_model.wv: embedding_matrix[i] = ft_model.wv[word]\n",
        "\n",
        "    train_ds = SentimentDataset(train_records, train_tfidf, label_map, word2idx, char2idx, cfg)\n",
        "    val_ds = SentimentDataset(val_records, val_tfidf, label_map, word2idx, char2idx, cfg)\n",
        "    test_ds = SentimentDataset(test_records, test_tfidf, label_map, word2idx, char2idx, cfg)\n",
        "\n",
        "    train_loader = DataLoader(train_ds, batch_size=cfg.batch_size, shuffle=True, num_workers=2)\n",
        "    val_loader = DataLoader(val_ds, batch_size=cfg.batch_size * 2, shuffle=False, num_workers=2)\n",
        "    test_loader = DataLoader(test_ds, batch_size=cfg.batch_size * 2, shuffle=False, num_workers=2)\n",
        "\n",
        "    model = HybridSentimentClassifier(embedding_matrix, len(char2idx), len(label_map), cfg)\n",
        "    train_loop(train_loader, val_loader, model, cfg, label_map)\n",
        "\n",
        "    print(\"\\n--- Final Test Set Evaluation ---\")\n",
        "    model.load_state_dict(torch.load(os.path.join(cfg.output_dir, \"best_model.pt\")))\n",
        "    test_true, test_pred = evaluate(model, test_loader, cfg.device)\n",
        "    print(classification_report(test_true, test_pred, target_names=label_map.keys()))\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V04XgvK7LOMw",
        "outputId": "d55c5564-815b-4837-8119-4a71a253a282"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting TF-IDF Vectorizer...\n",
            "Training FastText model...\n",
            "\n",
            "--- Starting Training on cuda ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/8: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 112/112 [00:02<00:00, 52.22it/s, loss=0.392]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 -> Val Macro F1: 0.7964\n",
            "ðŸš€ New best model saved with Macro F1: 0.7964\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/8: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 112/112 [00:02<00:00, 55.61it/s, loss=0.0998]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 -> Val Macro F1: 0.8092\n",
            "ðŸš€ New best model saved with Macro F1: 0.8092\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/8: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 112/112 [00:02<00:00, 55.41it/s, loss=0.265]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 -> Val Macro F1: 0.8231\n",
            "ðŸš€ New best model saved with Macro F1: 0.8231\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/8: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 112/112 [00:02<00:00, 50.82it/s, loss=0.0791]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 -> Val Macro F1: 0.8177\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/8: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 112/112 [00:02<00:00, 44.89it/s, loss=0.0179]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5 -> Val Macro F1: 0.8044\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/8: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 112/112 [00:02<00:00, 55.54it/s, loss=0.0145]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6 -> Val Macro F1: 0.8164\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/8: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 112/112 [00:02<00:00, 55.21it/s, loss=0.00149]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7 -> Val Macro F1: 0.8145\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8/8: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 112/112 [00:02<00:00, 55.05it/s, loss=0.00281]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8 -> Val Macro F1: 0.7946\n",
            "\n",
            "--- Final Test Set Evaluation ---\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Positive       0.91      0.90      0.90      1582\n",
            "    Negative       0.70      0.71      0.70       520\n",
            "\n",
            "    accuracy                           0.85      2102\n",
            "   macro avg       0.80      0.81      0.80      2102\n",
            "weighted avg       0.85      0.85      0.85      2102\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "MuRIL Embeddings + CharCNN + BiLSTM + TF-IDF Hybrid Model"
      ],
      "metadata": {
        "id": "OHSoeBEfv1QQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Step 1: Install necessary libraries, including Hugging Face Transformers ---\n",
        "!pip install torch scikit-learn gensim pandas tqdm transformers sentencepiece -q\n",
        "\n",
        "import os\n",
        "import time\n",
        "import random\n",
        "import json\n",
        "from collections import Counter\n",
        "from dataclasses import dataclass\n",
        "from typing import List\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "# ------------------------\n",
        "# Configuration\n",
        "# ------------------------\n",
        "@dataclass\n",
        "class CFG:\n",
        "    data_csv: str = \"mal_full_sentiment.tsv\"\n",
        "    output_dir: str = \"outputs_malayalam_muril_hybrid\"\n",
        "    model_name: str = \"google/muril-base-cased\"\n",
        "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    min_class_samples: int = 3\n",
        "    max_token_len: int = 128\n",
        "    max_char_len: int = 256\n",
        "    tfidf_max_features: int = 5000\n",
        "    tfidf_proj_dim: int = 64\n",
        "    muril_hidden_size: int = 768\n",
        "    lstm_hidden_dim: int = 256\n",
        "    lstm_layers: int = 1\n",
        "    char_emb_dim: int = 50\n",
        "    char_out_dim: int = 100\n",
        "    aux_dim: int = 8\n",
        "    dropout: float = 0.4\n",
        "    epochs: int = 5\n",
        "    batch_size: int = 32\n",
        "    lr_muril: float = 2e-5\n",
        "    lr_head: float = 1e-3\n",
        "    seed: int = 42\n",
        "\n",
        "cfg = CFG()\n",
        "os.makedirs(cfg.output_dir, exist_ok=True)\n",
        "\n",
        "# ------------------------\n",
        "# Utilities & Preprocessing\n",
        "# ------------------------\n",
        "def seed_everything(seed=42):\n",
        "    random.seed(seed); os.environ['PYTHONHASHSEED'] = str(seed); np.random.seed(seed); torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "seed_everything(cfg.seed)\n",
        "\n",
        "def normalize_label(l):\n",
        "    s = str(l).lower()\n",
        "    if 'posit' in s: return 'Positive'\n",
        "    if 'negat' in s: return 'Negative'\n",
        "    if 'neu' in s or 'normal' in s: return 'Neutral'\n",
        "    return 'Other'\n",
        "\n",
        "def build_char_vocab(texts: List[str]):\n",
        "    counts = Counter(c for text in texts for c in text)\n",
        "    vocab = {'<pad>': 0, '<unk>': 1}\n",
        "    for char, count in counts.items():\n",
        "        vocab[char] = len(vocab)\n",
        "    return vocab\n",
        "\n",
        "def compute_aux_features(text: str) -> List[float]:\n",
        "    toks = text.split(); num_tokens = len(toks); num_chars = len(text)\n",
        "    return [num_tokens, num_chars, 1.0 if any('\\u0D00' <= ch <= '\\u0D7F' for ch in text) else 0.0, 1.0 if any('a' <= ch.lower() <= 'z' for ch in text) else 0.0, (sum(len(t) for t in toks) / num_tokens) if num_tokens > 0 else 0.0, sum(1 for ch in text if ch.isupper()) / (num_chars + 1e-6), sum(1 for ch in text if ch in '?!.,;:'), sum(1 for ch in text if ord(ch) > 10000)]\n",
        "\n",
        "# ------------------------\n",
        "# Dataset Class\n",
        "# ------------------------\n",
        "class MurilHybridDataset(Dataset):\n",
        "    def __init__(self, records, tfidf_vectors, label_map, char2idx, tokenizer, cfg):\n",
        "        self.records = records; self.tfidf_vectors = tfidf_vectors; self.label_map = label_map; self.char2idx = char2idx; self.tokenizer = tokenizer; self.cfg = cfg\n",
        "    def __len__(self):\n",
        "        return len(self.records)\n",
        "    def __getitem__(self, idx):\n",
        "        record = self.records[idx]; text = str(record['text'])\n",
        "        inputs = self.tokenizer.encode_plus(text, add_special_tokens=True, max_length=self.cfg.max_token_len, padding='max_length', truncation=True, return_tensors='pt')\n",
        "        char_ids = [self.char2idx.get(c, self.char2idx['<unk>']) for c in text][:self.cfg.max_char_len]\n",
        "        padded_chars = char_ids + [0] * (self.cfg.max_char_len - len(char_ids))\n",
        "        return {\n",
        "            \"input_ids\": inputs['input_ids'].squeeze(), \"attention_mask\": inputs['attention_mask'].squeeze(),\n",
        "            \"char_ids\": torch.tensor(padded_chars, dtype=torch.long),\n",
        "            \"aux\": torch.tensor(compute_aux_features(text), dtype=torch.float32),\n",
        "            \"tfidf\": torch.tensor(self.tfidf_vectors[idx].toarray().squeeze(), dtype=torch.float32),\n",
        "            \"label\": torch.tensor(self.label_map[record['label']], dtype=torch.long)\n",
        "        }\n",
        "\n",
        "# ------------------------\n",
        "# Model Architecture\n",
        "# ------------------------\n",
        "class CharCNN(nn.Module):\n",
        "    def __init__(self, char_vocab_size, char_emb_dim, out_dim):\n",
        "        super().__init__(); self.char_emb = nn.Embedding(char_vocab_size, char_emb_dim, padding_idx=0); self.conv = nn.Conv1d(char_emb_dim, out_dim, kernel_size=3, padding=1)\n",
        "    def forward(self, x_char):\n",
        "        x = self.char_emb(x_char).transpose(1, 2); x = self.conv(x); return F.max_pool1d(x, x.size(2)).squeeze(2)\n",
        "\n",
        "class MurilHybridClassifier(nn.Module):\n",
        "    def __init__(self, char_vocab_size, num_labels, cfg):\n",
        "        super().__init__()\n",
        "        self.muril = AutoModel.from_pretrained(cfg.model_name)\n",
        "        self.bilstm = nn.LSTM(cfg.muril_hidden_size, cfg.lstm_hidden_dim // 2, num_layers=cfg.lstm_layers, bidirectional=True, batch_first=True)\n",
        "        self.char_cnn = CharCNN(char_vocab_size, cfg.char_emb_dim, cfg.char_out_dim)\n",
        "        self.tfidf_proj = nn.Linear(cfg.tfidf_max_features, cfg.tfidf_proj_dim)\n",
        "        self.aux_proj = nn.Linear(cfg.aux_dim, cfg.aux_dim)\n",
        "        classifier_input_dim = cfg.lstm_hidden_dim + cfg.char_out_dim + cfg.aux_dim + cfg.tfidf_proj_dim\n",
        "        self.classifier = nn.Sequential(nn.Linear(classifier_input_dim, 256), nn.ReLU(), nn.Dropout(cfg.dropout), nn.Linear(256, num_labels))\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, char_ids, aux, tfidf):\n",
        "        muril_output = self.muril(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        muril_embeddings = muril_output.last_hidden_state\n",
        "        _, (h_n, _) = self.bilstm(muril_embeddings)\n",
        "        hidden = torch.cat((h_n[-2,:,:], h_n[-1,:,:]), dim=1)\n",
        "        char_vec = self.char_cnn(char_ids)\n",
        "        aux_vec = F.relu(self.aux_proj(aux))\n",
        "        tfidf_vec = F.relu(self.tfidf_proj(tfidf))\n",
        "        combined_features = torch.cat([hidden, char_vec, aux_vec, tfidf_vec], dim=1)\n",
        "        return self.classifier(combined_features)\n",
        "\n",
        "# ------------------------\n",
        "# Training & Evaluation\n",
        "# ------------------------\n",
        "def evaluate(model, dataloader, device):\n",
        "    model.eval(); y_true, y_pred = [], []\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            logits = model(batch['input_ids'].to(device), batch['attention_mask'].to(device), batch['char_ids'].to(device), batch['aux'].to(device), batch['tfidf'].to(device))\n",
        "            y_true.extend(batch['label'].numpy()); y_pred.extend(torch.argmax(logits, dim=1).cpu().numpy())\n",
        "    return y_true, y_pred\n",
        "\n",
        "def train_loop(train_loader, val_loader, model, cfg, label_map):\n",
        "    device = cfg.device; model.to(device)\n",
        "    optimizer = torch.optim.AdamW([\n",
        "        {'params': model.muril.parameters(), 'lr': cfg.lr_muril},\n",
        "        {'params': model.bilstm.parameters(), 'lr': cfg.lr_head},\n",
        "        {'params': model.char_cnn.parameters(), 'lr': cfg.lr_head},\n",
        "        {'params': model.tfidf_proj.parameters(), 'lr': cfg.lr_head},\n",
        "        {'params': model.aux_proj.parameters(), 'lr': cfg.lr_head},\n",
        "        {'params': model.classifier.parameters(), 'lr': cfg.lr_head}\n",
        "    ])\n",
        "    criterion = nn.CrossEntropyLoss(); best_macro_f1 = -1.0\n",
        "    print(f\"\\n--- Starting Training on {device} ---\")\n",
        "    for epoch in range(cfg.epochs):\n",
        "        model.train()\n",
        "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{cfg.epochs}\")\n",
        "        for batch in pbar:\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(batch['input_ids'].to(device), batch['attention_mask'].to(device), batch['char_ids'].to(device), batch['aux'].to(device), batch['tfidf'].to(device))\n",
        "            loss = criterion(logits, batch['label'].to(device))\n",
        "            loss.backward(); optimizer.step(); pbar.set_postfix(loss=loss.item())\n",
        "        val_true, val_pred = evaluate(model, val_loader, device)\n",
        "        report = classification_report(val_true, val_pred, target_names=label_map.keys(), output_dict=True, zero_division=0)\n",
        "        macro_f1 = report['macro avg']['f1-score']\n",
        "        print(f\"Epoch {epoch+1} -> Val Macro F1: {macro_f1:.4f}\")\n",
        "        if macro_f1 > best_macro_f1:\n",
        "            best_macro_f1 = macro_f1; torch.save(model.state_dict(), os.path.join(cfg.output_dir, \"best_model.pt\")); print(f\"ðŸš€ New best model saved with Macro F1: {macro_f1:.4f}\")\n",
        "\n",
        "# ------------------------\n",
        "# Main Orchestration\n",
        "# ------------------------\n",
        "def main():\n",
        "    df = pd.read_csv(cfg.data_csv, sep='\\t', header=None, names=['label', 'text'], engine='python')\n",
        "    df.dropna(subset=['text', 'label'], inplace=True); df['text'] = df['text'].astype(str); df['label'] = df['label'].apply(normalize_label); df = df[df['label'] != 'Other']\n",
        "    label_counts = df['label'].value_counts(); classes_to_keep = label_counts[label_counts >= cfg.min_class_samples].index; df = df[df['label'].isin(classes_to_keep)].reset_index(drop=True)\n",
        "    label_map = {label: i for i, label in enumerate(df['label'].unique())}\n",
        "    data = df.to_dict(orient=\"records\"); labels = [label_map[r['label']] for r in data]\n",
        "    train_records, test_records, _, _ = train_test_split(data, labels, test_size=0.2, random_state=cfg.seed, stratify=labels)\n",
        "    train_records, val_records = train_test_split(train_records, test_size=0.15, random_state=cfg.seed, stratify=[label_map[r['label']] for r in train_records])\n",
        "\n",
        "    print(\"Fitting TF-IDF Vectorizer...\")\n",
        "    train_texts = [r['text'] for r in train_records]; tfidf_vectorizer = TfidfVectorizer(max_features=cfg.tfidf_max_features, ngram_range=(1, 2)); train_tfidf = tfidf_vectorizer.fit_transform(train_texts); val_tfidf = tfidf_vectorizer.transform([r['text'] for r in val_records]); test_tfidf = tfidf_vectorizer.transform([r['text'] for r in test_records])\n",
        "\n",
        "    print(f\"Loading MURIL tokenizer: {cfg.model_name}...\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(cfg.model_name)\n",
        "    char2idx = build_char_vocab([r['text'] for r in train_records])\n",
        "\n",
        "    train_ds = MurilHybridDataset(train_records, train_tfidf, label_map, char2idx, tokenizer, cfg)\n",
        "    val_ds = MurilHybridDataset(val_records, val_tfidf, label_map, char2idx, tokenizer, cfg)\n",
        "    test_ds = MurilHybridDataset(test_records, test_tfidf, label_map, char2idx, tokenizer, cfg)\n",
        "\n",
        "    train_loader = DataLoader(train_ds, batch_size=cfg.batch_size, shuffle=True, num_workers=2)\n",
        "    val_loader = DataLoader(val_ds, batch_size=cfg.batch_size * 2, shuffle=False, num_workers=2)\n",
        "    test_loader = DataLoader(test_ds, batch_size=cfg.batch_size * 2, shuffle=False, num_workers=2)\n",
        "\n",
        "    print(f\"Loading MURIL model: {cfg.model_name}... (This may take a moment)\")\n",
        "    # --- FIX WAS APPLIED HERE ---\n",
        "    model = MurilHybridClassifier(len(char2idx), len(label_map), cfg)\n",
        "    # --------------------------\n",
        "    train_loop(train_loader, val_loader, model, cfg, label_map)\n",
        "\n",
        "    print(\"\\n--- Final Test Set Evaluation ---\")\n",
        "    model.load_state_dict(torch.load(os.path.join(cfg.output_dir, \"best_model.pt\")))\n",
        "    test_true, test_pred = evaluate(model, test_loader, cfg.device)\n",
        "    print(classification_report(test_true, test_pred, target_names=label_map.keys()))\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NpCqFWItL9xk",
        "outputId": "334952e9-9f9d-46fa-c109-33abd0d0966d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting TF-IDF Vectorizer...\n",
            "Loading MURIL tokenizer: google/muril-base-cased...\n",
            "Loading MURIL model: google/muril-base-cased... (This may take a moment)\n",
            "\n",
            "--- Starting Training on cuda ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 224/224 [02:37<00:00,  1.43it/s, loss=0.696]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 -> Val Macro F1: 0.8462\n",
            "ðŸš€ New best model saved with Macro F1: 0.8462\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 224/224 [02:36<00:00,  1.43it/s, loss=0.0923]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 -> Val Macro F1: 0.8532\n",
            "ðŸš€ New best model saved with Macro F1: 0.8532\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 224/224 [02:36<00:00,  1.43it/s, loss=0.165]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 -> Val Macro F1: 0.8122\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 224/224 [02:35<00:00,  1.44it/s, loss=0.346]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 -> Val Macro F1: 0.8558\n",
            "ðŸš€ New best model saved with Macro F1: 0.8558\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 224/224 [02:36<00:00,  1.43it/s, loss=0.00552]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5 -> Val Macro F1: 0.8530\n",
            "\n",
            "--- Final Test Set Evaluation ---\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Positive       0.92      0.93      0.93      1582\n",
            "    Negative       0.78      0.76      0.77       520\n",
            "\n",
            "    accuracy                           0.89      2102\n",
            "   macro avg       0.85      0.84      0.85      2102\n",
            "weighted avg       0.89      0.89      0.89      2102\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Step 1: Install necessary libraries, including Hugging Face Transformers ---\n",
        "!pip install torch scikit-learn pandas tqdm transformers sentencepiece -q\n",
        "\n",
        "import os\n",
        "import time\n",
        "import random\n",
        "import json\n",
        "from collections import Counter\n",
        "from dataclasses import dataclass\n",
        "from typing import List\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "# ------------------------\n",
        "# Configuration\n",
        "# ------------------------\n",
        "@dataclass\n",
        "class CFG:\n",
        "    data_csv: str = \"mal_full_sentiment.tsv\"\n",
        "    output_dir: str = \"outputs_malayalam_muril_encoder\"\n",
        "    model_name: str = \"google/muril-base-cased\"\n",
        "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    min_class_samples: int = 3\n",
        "\n",
        "    # Padding / Length\n",
        "    max_token_len: int = 128\n",
        "\n",
        "    # TF-IDF Config\n",
        "    tfidf_max_features: int = 5000\n",
        "    tfidf_proj_dim: int = 64\n",
        "\n",
        "    # Model Architecture\n",
        "    muril_hidden_size: int = 768\n",
        "    aux_dim: int = 8\n",
        "    dropout: float = 0.3\n",
        "\n",
        "    # Training\n",
        "    epochs: int = 4 # Transformers fine-tune quickly\n",
        "    batch_size: int = 32\n",
        "    lr_muril: float = 2e-5\n",
        "    lr_head: float = 1e-3\n",
        "    seed: int = 42\n",
        "\n",
        "cfg = CFG()\n",
        "os.makedirs(cfg.output_dir, exist_ok=True)\n",
        "\n",
        "# ------------------------\n",
        "# Utilities & Preprocessing\n",
        "# ------------------------\n",
        "def seed_everything(seed=42):\n",
        "    random.seed(seed); os.environ['PYTHONHASHSEED'] = str(seed); np.random.seed(seed); torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "seed_everything(cfg.seed)\n",
        "\n",
        "def normalize_label(l):\n",
        "    s = str(l).lower()\n",
        "    if 'posit' in s: return 'Positive'\n",
        "    if 'negat' in s: return 'Negative'\n",
        "    if 'neu' in s or 'normal' in s: return 'Neutral'\n",
        "    return 'Other'\n",
        "\n",
        "def compute_aux_features(text: str) -> List[float]:\n",
        "    toks = text.split(); num_tokens = len(toks); num_chars = len(text)\n",
        "    return [\n",
        "        num_tokens, num_chars,\n",
        "        1.0 if any('\\u0D00' <= ch <= '\\u0D7F' for ch in text) else 0.0,\n",
        "        1.0 if any('a' <= ch.lower() <= 'z' for ch in text) else 0.0,\n",
        "        (sum(len(t) for t in toks) / num_tokens) if num_tokens > 0 else 0.0,\n",
        "        sum(1 for ch in text if ch.isupper()) / (num_chars + 1e-6),\n",
        "        sum(1 for ch in text if ch in '?!.,;:'),\n",
        "        sum(1 for ch in text if ord(ch) > 10000)\n",
        "    ]\n",
        "\n",
        "# ------------------------\n",
        "# Dataset Class - Simplified\n",
        "# ------------------------\n",
        "class MurilHybridDataset(Dataset):\n",
        "    def __init__(self, records, tfidf_vectors, label_map, tokenizer, cfg):\n",
        "        self.records = records\n",
        "        self.tfidf_vectors = tfidf_vectors\n",
        "        self.label_map = label_map\n",
        "        self.tokenizer = tokenizer\n",
        "        self.cfg = cfg\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.records)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        record = self.records[idx]\n",
        "        text = str(record['text'])\n",
        "\n",
        "        inputs = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.cfg.max_token_len,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": inputs['input_ids'].squeeze(),\n",
        "            \"attention_mask\": inputs['attention_mask'].squeeze(),\n",
        "            \"aux\": torch.tensor(compute_aux_features(text), dtype=torch.float32),\n",
        "            \"tfidf\": torch.tensor(self.tfidf_vectors[idx].toarray().squeeze(), dtype=torch.float32),\n",
        "            \"label\": torch.tensor(self.label_map[record['label']], dtype=torch.long)\n",
        "        }\n",
        "\n",
        "# ------------------------\n",
        "# Model Architecture - Simplified and Transformer-centric\n",
        "# ------------------------\n",
        "class MurilSentimentClassifier(nn.Module):\n",
        "    def __init__(self, num_labels, cfg):\n",
        "        super().__init__()\n",
        "        # Main MURIL model\n",
        "        self.muril = AutoModel.from_pretrained(cfg.model_name)\n",
        "\n",
        "        # Projection layers for our extra features\n",
        "        self.tfidf_proj = nn.Linear(cfg.tfidf_max_features, cfg.tfidf_proj_dim)\n",
        "        self.aux_proj = nn.Linear(cfg.aux_dim, cfg.aux_dim)\n",
        "\n",
        "        # Classifier head\n",
        "        classifier_input_dim = cfg.muril_hidden_size + cfg.tfidf_proj_dim + cfg.aux_dim\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(classifier_input_dim, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(cfg.dropout),\n",
        "            nn.Linear(256, num_labels)\n",
        "        )\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, aux, tfidf):\n",
        "        # Get output from MURIL\n",
        "        muril_output = self.muril(input_ids=input_ids, attention_mask=attention_mask)\n",
        "\n",
        "        # We use the pooler_output, which is the [CLS] token's embedding,\n",
        "        # designed for sentence-level classification.\n",
        "        cls_embedding = muril_output.pooler_output # Shape: (batch_size, 768)\n",
        "\n",
        "        # Process auxiliary features\n",
        "        aux_vec = F.relu(self.aux_proj(aux))\n",
        "        tfidf_vec = F.relu(self.tfidf_proj(tfidf))\n",
        "\n",
        "        # Combine all features and classify\n",
        "        combined_features = torch.cat([cls_embedding, aux_vec, tfidf_vec], dim=1)\n",
        "        return self.classifier(combined_features)\n",
        "\n",
        "# ------------------------\n",
        "# Training & Evaluation\n",
        "# ------------------------\n",
        "def evaluate(model, dataloader, device):\n",
        "    model.eval(); y_true, y_pred = [], []\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            logits = model(batch['input_ids'].to(device), batch['attention_mask'].to(device), batch['aux'].to(device), batch['tfidf'].to(device))\n",
        "            y_true.extend(batch['label'].numpy()); y_pred.extend(torch.argmax(logits, dim=1).cpu().numpy())\n",
        "    return y_true, y_pred\n",
        "\n",
        "def train_loop(train_loader, val_loader, model, cfg, label_map):\n",
        "    device = cfg.device; model.to(device)\n",
        "    optimizer = torch.optim.AdamW([\n",
        "        {'params': model.muril.parameters(), 'lr': cfg.lr_muril},\n",
        "        {'params': model.classifier.parameters(), 'lr': cfg.lr_head},\n",
        "        {'params': model.tfidf_proj.parameters(), 'lr': cfg.lr_head},\n",
        "        {'params': model.aux_proj.parameters(), 'lr': cfg.lr_head}\n",
        "    ])\n",
        "    criterion = nn.CrossEntropyLoss(); best_macro_f1 = -1.0\n",
        "    print(f\"\\n--- Starting Training on {device} ---\")\n",
        "    for epoch in range(cfg.epochs):\n",
        "        model.train()\n",
        "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{cfg.epochs}\")\n",
        "        for batch in pbar:\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(batch['input_ids'].to(device), batch['attention_mask'].to(device), batch['aux'].to(device), batch['tfidf'].to(device))\n",
        "            loss = criterion(logits, batch['label'].to(device))\n",
        "            loss.backward(); optimizer.step(); pbar.set_postfix(loss=loss.item())\n",
        "        val_true, val_pred = evaluate(model, val_loader, device)\n",
        "        report = classification_report(val_true, val_pred, target_names=label_map.keys(), output_dict=True, zero_division=0)\n",
        "        macro_f1 = report['macro avg']['f1-score']\n",
        "        print(f\"Epoch {epoch+1} -> Val Macro F1: {macro_f1:.4f}\")\n",
        "        if macro_f1 > best_macro_f1:\n",
        "            best_macro_f1 = macro_f1; torch.save(model.state_dict(), os.path.join(cfg.output_dir, \"best_model.pt\")); print(f\"ðŸš€ New best model saved with Macro F1: {macro_f1:.4f}\")\n",
        "\n",
        "# ------------------------\n",
        "# Main Orchestration\n",
        "# ------------------------\n",
        "def main():\n",
        "    df = pd.read_csv(cfg.data_csv, sep='\\t', header=None, names=['label', 'text'], engine='python')\n",
        "    df.dropna(subset=['text', 'label'], inplace=True); df['text'] = df['text'].astype(str); df['label'] = df['label'].apply(normalize_label); df = df[df['label'] != 'Other']\n",
        "    label_counts = df['label'].value_counts(); classes_to_keep = label_counts[label_counts >= cfg.min_class_samples].index; df = df[df['label'].isin(classes_to_keep)].reset_index(drop=True)\n",
        "    label_map = {label: i for i, label in enumerate(df['label'].unique())}\n",
        "    data = df.to_dict(orient=\"records\"); labels = [label_map[r['label']] for r in data]\n",
        "    train_records, test_records, _, _ = train_test_split(data, labels, test_size=0.2, random_state=cfg.seed, stratify=labels)\n",
        "    train_records, val_records = train_test_split(train_records, test_size=0.15, random_state=cfg.seed, stratify=[label_map[r['label']] for r in train_records])\n",
        "\n",
        "    print(\"Fitting TF-IDF Vectorizer...\")\n",
        "    train_texts = [r['text'] for r in train_records]; tfidf_vectorizer = TfidfVectorizer(max_features=cfg.tfidf_max_features, ngram_range=(1, 2)); train_tfidf = tfidf_vectorizer.fit_transform(train_texts); val_tfidf = tfidf_vectorizer.transform([r['text'] for r in val_records]); test_tfidf = tfidf_vectorizer.transform([r['text'] for r in test_records])\n",
        "\n",
        "    print(f\"Loading MURIL tokenizer: {cfg.model_name}...\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(cfg.model_name)\n",
        "\n",
        "    train_ds = MurilHybridDataset(train_records, train_tfidf, label_map, tokenizer, cfg)\n",
        "    val_ds = MurilHybridDataset(val_records, val_tfidf, label_map, tokenizer, cfg)\n",
        "    test_ds = MurilHybridDataset(test_records, test_tfidf, label_map, tokenizer, cfg)\n",
        "\n",
        "    train_loader = DataLoader(train_ds, batch_size=cfg.batch_size, shuffle=True, num_workers=2)\n",
        "    val_loader = DataLoader(val_ds, batch_size=cfg.batch_size * 2, shuffle=False, num_workers=2)\n",
        "    test_loader = DataLoader(test_ds, batch_size=cfg.batch_size * 2, shuffle=False, num_workers=2)\n",
        "\n",
        "    print(f\"Loading MURIL model: {cfg.model_name}... (This may take a moment)\")\n",
        "    model = MurilSentimentClassifier(len(label_map), cfg)\n",
        "    train_loop(train_loader, val_loader, model, cfg, label_map)\n",
        "\n",
        "    print(\"\\n--- Final Test Set Evaluation ---\")\n",
        "    model.load_state_dict(torch.load(os.path.join(cfg.output_dir, \"best_model.pt\")))\n",
        "    test_true, test_pred = evaluate(model, test_loader, cfg.device)\n",
        "    print(classification_report(test_true, test_pred, target_names=label_map.keys()))\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "At7HCEfnNHXZ",
        "outputId": "f2455d79-b15f-4669-f6e4-b20d7ac6b2e5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting TF-IDF Vectorizer...\n",
            "Loading MURIL tokenizer: google/muril-base-cased...\n",
            "Loading MURIL model: google/muril-base-cased... (This may take a moment)\n",
            "\n",
            "--- Starting Training on cuda ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 224/224 [02:29<00:00,  1.50it/s, loss=0.487]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 -> Val Macro F1: 0.7690\n",
            "ðŸš€ New best model saved with Macro F1: 0.7690\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 224/224 [02:27<00:00,  1.51it/s, loss=0.194]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 -> Val Macro F1: 0.8331\n",
            "ðŸš€ New best model saved with Macro F1: 0.8331\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 224/224 [02:27<00:00,  1.52it/s, loss=0.019]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 -> Val Macro F1: 0.8176\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 224/224 [02:27<00:00,  1.52it/s, loss=0.00402]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 -> Val Macro F1: 0.8306\n",
            "\n",
            "--- Final Test Set Evaluation ---\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Positive       0.94      0.86      0.89      1582\n",
            "    Negative       0.65      0.82      0.73       520\n",
            "\n",
            "    accuracy                           0.85      2102\n",
            "   macro avg       0.79      0.84      0.81      2102\n",
            "weighted avg       0.87      0.85      0.85      2102\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vanilla Fine Tuning MuRIL"
      ],
      "metadata": {
        "id": "W7428ZccvqZk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Step 1: Install necessary libraries ---\n",
        "!pip install torch scikit-learn pandas tqdm transformers sentencepiece -q\n",
        "\n",
        "import os\n",
        "import time\n",
        "import random\n",
        "from dataclasses import dataclass\n",
        "from typing import List\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "# ------------------------\n",
        "# Configuration\n",
        "# ------------------------\n",
        "@dataclass\n",
        "class CFG:\n",
        "    data_csv: str = \"mal_full_sentiment.tsv\"\n",
        "    output_dir: str = \"outputs_malayalam_muril_pure\"\n",
        "    model_name: str = \"google/muril-base-cased\"\n",
        "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    min_class_samples: int = 3\n",
        "\n",
        "    # Tokenizer/Padding\n",
        "    max_len: int = 128\n",
        "\n",
        "    # Model Architecture\n",
        "    dropout: float = 0.2\n",
        "\n",
        "    # Training\n",
        "    epochs: int = 4\n",
        "    batch_size: int = 32\n",
        "    lr_muril: float = 2e-5  # Standard learning rate for fine-tuning transformers\n",
        "    lr_head: float = 1e-3\n",
        "    seed: int = 42\n",
        "\n",
        "cfg = CFG()\n",
        "os.makedirs(cfg.output_dir, exist_ok=True)\n",
        "\n",
        "# ------------------------\n",
        "# Utilities\n",
        "# ------------------------\n",
        "def seed_everything(seed=42):\n",
        "    random.seed(seed); os.environ['PYTHONHASHSEED'] = str(seed); np.random.seed(seed); torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "seed_everything(cfg.seed)\n",
        "\n",
        "def normalize_label(l):\n",
        "    s = str(l).lower()\n",
        "    if 'posit' in s: return 'Positive'\n",
        "    if 'negat' in s: return 'Negative'\n",
        "    if 'neu' in s or 'normal' in s: return 'Neutral'\n",
        "    return 'Other'\n",
        "\n",
        "# ------------------------\n",
        "# Dataset Class - Simplified\n",
        "# ------------------------\n",
        "class MurilDataset(Dataset):\n",
        "    def __init__(self, records, label_map, tokenizer, cfg):\n",
        "        self.records = records\n",
        "        self.label_map = label_map\n",
        "        self.tokenizer = tokenizer\n",
        "        self.cfg = cfg\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.records)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        record = self.records[idx]\n",
        "        text = str(record['text'])\n",
        "\n",
        "        inputs = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.cfg.max_len,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": inputs['input_ids'].squeeze(),\n",
        "            \"attention_mask\": inputs['attention_mask'].squeeze(),\n",
        "            \"label\": torch.tensor(self.label_map[record['label']], dtype=torch.long)\n",
        "        }\n",
        "\n",
        "# ------------------------\n",
        "# Model Architecture - Simplified\n",
        "# ------------------------\n",
        "class MurilClassifier(nn.Module):\n",
        "    def __init__(self, num_labels, cfg):\n",
        "        super().__init__()\n",
        "        # Main MURIL model\n",
        "        self.muril = AutoModel.from_pretrained(cfg.model_name)\n",
        "\n",
        "        # Simple classifier head\n",
        "        self.dropout = nn.Dropout(cfg.dropout)\n",
        "        self.classifier = nn.Linear(self.muril.config.hidden_size, num_labels)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        muril_output = self.muril(input_ids=input_ids, attention_mask=attention_mask)\n",
        "\n",
        "        # Use the pooler_output, which is the [CLS] token's embedding\n",
        "        cls_embedding = muril_output.pooler_output\n",
        "\n",
        "        # Classification\n",
        "        pooled_output = self.dropout(cls_embedding)\n",
        "        return self.classifier(pooled_output)\n",
        "\n",
        "# ------------------------\n",
        "# Training & Evaluation\n",
        "# ------------------------\n",
        "def evaluate(model, dataloader, device):\n",
        "    model.eval(); y_true, y_pred = [], []\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            logits = model(batch['input_ids'].to(device), batch['attention_mask'].to(device))\n",
        "            y_true.extend(batch['label'].numpy()); y_pred.extend(torch.argmax(logits, dim=1).cpu().numpy())\n",
        "    return y_true, y_pred\n",
        "\n",
        "def train_loop(train_loader, val_loader, model, cfg, label_map):\n",
        "    device = cfg.device; model.to(device)\n",
        "    optimizer = torch.optim.AdamW([\n",
        "        {'params': model.muril.parameters(), 'lr': cfg.lr_muril},\n",
        "        {'params': model.classifier.parameters(), 'lr': cfg.lr_head}\n",
        "    ])\n",
        "    criterion = nn.CrossEntropyLoss(); best_macro_f1 = -1.0\n",
        "    print(f\"\\n--- Starting Training on {device} ---\")\n",
        "    for epoch in range(cfg.epochs):\n",
        "        model.train()\n",
        "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{cfg.epochs}\")\n",
        "        for batch in pbar:\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(batch['input_ids'].to(device), batch['attention_mask'].to(device))\n",
        "            loss = criterion(logits, batch['label'].to(device))\n",
        "            loss.backward(); optimizer.step(); pbar.set_postfix(loss=loss.item())\n",
        "        val_true, val_pred = evaluate(model, val_loader, device)\n",
        "        report = classification_report(val_true, val_pred, target_names=label_map.keys(), output_dict=True, zero_division=0)\n",
        "        macro_f1 = report['macro avg']['f1-score']\n",
        "        print(f\"Epoch {epoch+1} -> Val Macro F1: {macro_f1:.4f}\")\n",
        "        if macro_f1 > best_macro_f1:\n",
        "            best_macro_f1 = macro_f1; torch.save(model.state_dict(), os.path.join(cfg.output_dir, \"best_model.pt\")); print(f\"ðŸš€ New best model saved with Macro F1: {macro_f1:.4f}\")\n",
        "\n",
        "# ------------------------\n",
        "# Main Orchestration\n",
        "# ------------------------\n",
        "def main():\n",
        "    df = pd.read_csv(cfg.data_csv, sep='\\t', header=None, names=['label', 'text'], engine='python')\n",
        "    df.dropna(subset=['text', 'label'], inplace=True); df['text'] = df['text'].astype(str); df['label'] = df['label'].apply(normalize_label); df = df[df['label'] != 'Other']\n",
        "    label_counts = df['label'].value_counts(); classes_to_keep = label_counts[label_counts >= cfg.min_class_samples].index; df = df[df['label'].isin(classes_to_keep)].reset_index(drop=True)\n",
        "    label_map = {label: i for i, label in enumerate(df['label'].unique())}\n",
        "    data = df.to_dict(orient=\"records\"); labels = [label_map[r['label']] for r in data]\n",
        "    train_records, test_records, _, _ = train_test_split(data, labels, test_size=0.2, random_state=cfg.seed, stratify=labels)\n",
        "    train_records, val_records = train_test_split(train_records, test_size=0.15, random_state=cfg.seed, stratify=[label_map[r['label']] for r in train_records])\n",
        "\n",
        "    print(f\"Loading MURIL tokenizer: {cfg.model_name}...\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(cfg.model_name)\n",
        "\n",
        "    train_ds = MurilDataset(train_records, label_map, tokenizer, cfg)\n",
        "    val_ds = MurilDataset(val_records, label_map, tokenizer, cfg)\n",
        "    test_ds = MurilDataset(test_records, label_map, tokenizer, cfg)\n",
        "\n",
        "    train_loader = DataLoader(train_ds, batch_size=cfg.batch_size, shuffle=True, num_workers=2)\n",
        "    val_loader = DataLoader(val_ds, batch_size=cfg.batch_size * 2, shuffle=False, num_workers=2)\n",
        "    test_loader = DataLoader(test_ds, batch_size=cfg.batch_size * 2, shuffle=False, num_workers=2)\n",
        "\n",
        "    print(f\"Loading MURIL model: {cfg.model_name}... (This may take a moment)\")\n",
        "    model = MurilClassifier(len(label_map), cfg)\n",
        "    train_loop(train_loader, val_loader, model, cfg, label_map)\n",
        "\n",
        "    print(\"\\n--- Final Test Set Evaluation ---\")\n",
        "    model.load_state_dict(torch.load(os.path.join(cfg.output_dir, \"best_model.pt\")))\n",
        "    test_true, test_pred = evaluate(model, test_loader, cfg.device)\n",
        "    print(classification_report(test_true, test_pred, target_names=label_map.keys()))\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CGesObkBRVpS",
        "outputId": "05f5ff5b-2d9d-4d53-d72e-6ad4aec9fc4e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading MURIL tokenizer: google/muril-base-cased...\n",
            "Loading MURIL model: google/muril-base-cased... (This may take a moment)\n",
            "\n",
            "--- Starting Training on cuda ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 224/224 [02:27<00:00,  1.52it/s, loss=0.666]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 -> Val Macro F1: 0.6238\n",
            "ðŸš€ New best model saved with Macro F1: 0.6238\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 224/224 [02:27<00:00,  1.52it/s, loss=0.0553]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 -> Val Macro F1: 0.7525\n",
            "ðŸš€ New best model saved with Macro F1: 0.7525\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 224/224 [02:27<00:00,  1.52it/s, loss=0.0497]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 -> Val Macro F1: 0.8242\n",
            "ðŸš€ New best model saved with Macro F1: 0.8242\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 224/224 [02:27<00:00,  1.52it/s, loss=0.0564]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 -> Val Macro F1: 0.8430\n",
            "ðŸš€ New best model saved with Macro F1: 0.8430\n",
            "\n",
            "--- Final Test Set Evaluation ---\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Positive       0.90      0.95      0.92      1582\n",
            "    Negative       0.80      0.68      0.74       520\n",
            "\n",
            "    accuracy                           0.88      2102\n",
            "   macro avg       0.85      0.81      0.83      2102\n",
            "weighted avg       0.88      0.88      0.88      2102\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "HyperParameter Tuning MuRIL"
      ],
      "metadata": {
        "id": "df4B0zkMvctl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Step 1: Install necessary libraries ---\n",
        "!pip install torch scikit-learn pandas tqdm transformers sentencepiece -q\n",
        "\n",
        "import os\n",
        "import time\n",
        "import random\n",
        "from dataclasses import dataclass\n",
        "from typing import List\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from transformers import AutoTokenizer, AutoModel, get_linear_schedule_with_warmup\n",
        "\n",
        "# ------------------------\n",
        "# Configuration\n",
        "# ------------------------\n",
        "@dataclass\n",
        "class CFG:\n",
        "    data_csv: str = \"mal_full_sentiment.tsv\"\n",
        "    output_dir: str = \"outputs_malayalam_muril_advanced\"\n",
        "    model_name: str = \"google/muril-base-cased\"\n",
        "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    min_class_samples: int = 3\n",
        "\n",
        "    # Tokenizer/Padding\n",
        "    max_len: int = 128\n",
        "\n",
        "    # Model Architecture\n",
        "    dropout: float = 0.2\n",
        "\n",
        "    # --- MODIFIED: Advanced Training Config ---\n",
        "    epochs: int = 5 # Can increase to 5-6 with a good scheduler\n",
        "    batch_size: int = 32\n",
        "    lr_muril: float = 2e-5\n",
        "    lr_head: float = 1e-3\n",
        "    label_smoothing: float = 0.1 # New regularization technique\n",
        "    seed: int = 42\n",
        "\n",
        "cfg = CFG()\n",
        "os.makedirs(cfg.output_dir, exist_ok=True)\n",
        "\n",
        "# ------------------------\n",
        "# Utilities\n",
        "# ------------------------\n",
        "def seed_everything(seed=42):\n",
        "    random.seed(seed); os.environ['PYTHONHASHSEED'] = str(seed); np.random.seed(seed); torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "seed_everything(cfg.seed)\n",
        "\n",
        "def normalize_label(l):\n",
        "    s = str(l).lower()\n",
        "    if 'posit' in s: return 'Positive'\n",
        "    if 'negat' in s: return 'Negative'\n",
        "    if 'neu' in s or 'normal' in s: return 'Neutral'\n",
        "    return 'Other'\n",
        "\n",
        "# ------------------------\n",
        "# Dataset Class\n",
        "# ------------------------\n",
        "class MurilDataset(Dataset):\n",
        "    def __init__(self, records, label_map, tokenizer, cfg):\n",
        "        self.records = records\n",
        "        self.label_map = label_map\n",
        "        self.tokenizer = tokenizer\n",
        "        self.cfg = cfg\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.records)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        record = self.records[idx]\n",
        "        text = str(record['text'])\n",
        "        inputs = self.tokenizer.encode_plus(\n",
        "            text, add_special_tokens=True, max_length=self.cfg.max_len,\n",
        "            padding='max_length', truncation=True, return_tensors='pt'\n",
        "        )\n",
        "        return {\n",
        "            \"input_ids\": inputs['input_ids'].squeeze(),\n",
        "            \"attention_mask\": inputs['attention_mask'].squeeze(),\n",
        "            \"label\": torch.tensor(self.label_map[record['label']], dtype=torch.long)\n",
        "        }\n",
        "\n",
        "# ------------------------\n",
        "# Model Architecture\n",
        "# ------------------------\n",
        "class MurilClassifier(nn.Module):\n",
        "    def __init__(self, num_labels, cfg):\n",
        "        super().__init__()\n",
        "        self.muril = AutoModel.from_pretrained(cfg.model_name)\n",
        "        self.dropout = nn.Dropout(cfg.dropout)\n",
        "        self.classifier = nn.Linear(self.muril.config.hidden_size, num_labels)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        muril_output = self.muril(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        cls_embedding = muril_output.pooler_output\n",
        "        pooled_output = self.dropout(cls_embedding)\n",
        "        return self.classifier(pooled_output)\n",
        "\n",
        "# ------------------------\n",
        "# Training & Evaluation\n",
        "# ------------------------\n",
        "def evaluate(model, dataloader, device):\n",
        "    model.eval(); y_true, y_pred = [], []\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            logits = model(batch['input_ids'].to(device), batch['attention_mask'].to(device))\n",
        "            y_true.extend(batch['label'].numpy()); y_pred.extend(torch.argmax(logits, dim=1).cpu().numpy())\n",
        "    return y_true, y_pred\n",
        "\n",
        "def train_loop(train_loader, val_loader, model, cfg, label_map, class_weights):\n",
        "    device = cfg.device; model.to(device)\n",
        "    optimizer = torch.optim.AdamW([\n",
        "        {'params': model.muril.parameters(), 'lr': cfg.lr_muril},\n",
        "        {'params': model.classifier.parameters(), 'lr': cfg.lr_head}\n",
        "    ])\n",
        "\n",
        "    # --- NEW: Setup Scheduler and Weighted Loss Function ---\n",
        "    num_training_steps = len(train_loader) * cfg.epochs\n",
        "    num_warmup_steps = int(0.1 * num_training_steps)\n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "        optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_training_steps\n",
        "    )\n",
        "    weights_tensor = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
        "    criterion = nn.CrossEntropyLoss(weight=weights_tensor, label_smoothing=cfg.label_smoothing)\n",
        "    # --------------------------------------------------------\n",
        "\n",
        "    best_macro_f1 = -1.0\n",
        "    print(f\"\\n--- Starting Training on {device} ---\")\n",
        "    for epoch in range(cfg.epochs):\n",
        "        model.train()\n",
        "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{cfg.epochs}\")\n",
        "        for batch in pbar:\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(batch['input_ids'].to(device), batch['attention_mask'].to(device))\n",
        "            loss = criterion(logits, batch['label'].to(device))\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            scheduler.step() # Update the learning rate\n",
        "            pbar.set_postfix(loss=loss.item())\n",
        "        val_true, val_pred = evaluate(model, val_loader, device)\n",
        "        report = classification_report(val_true, val_pred, target_names=label_map.keys(), output_dict=True, zero_division=0)\n",
        "        macro_f1 = report['macro avg']['f1-score']\n",
        "        print(f\"Epoch {epoch+1} -> Val Macro F1: {macro_f1:.4f}\")\n",
        "        if macro_f1 > best_macro_f1:\n",
        "            best_macro_f1 = macro_f1; torch.save(model.state_dict(), os.path.join(cfg.output_dir, \"best_model.pt\")); print(f\"ðŸš€ New best model saved with Macro F1: {macro_f1:.4f}\")\n",
        "\n",
        "# ------------------------\n",
        "# Main Orchestration\n",
        "# ------------------------\n",
        "def main():\n",
        "    df = pd.read_csv(cfg.data_csv, sep='\\t', header=None, names=['label', 'text'], engine='python')\n",
        "    df.dropna(subset=['text', 'label'], inplace=True); df['text'] = df['text'].astype(str); df['label'] = df['label'].apply(normalize_label); df = df[df['label'] != 'Other']\n",
        "    label_counts = df['label'].value_counts(); classes_to_keep = label_counts[label_counts >= cfg.min_class_samples].index; df = df[df['label'].isin(classes_to_keep)].reset_index(drop=True)\n",
        "    label_map = {label: i for i, label in enumerate(sorted(df['label'].unique()))} # Sort labels for consistency\n",
        "    data = df.to_dict(orient=\"records\"); labels = [label_map[r['label']] for r in data]\n",
        "    train_records, test_records, train_labels, _ = train_test_split(data, labels, test_size=0.2, random_state=cfg.seed, stratify=labels)\n",
        "    train_records, val_records = train_test_split(train_records, test_size=0.15, random_state=cfg.seed, stratify=[label_map[r['label']] for r in train_records])\n",
        "\n",
        "    # --- NEW: Calculate Class Weights from the training set ---\n",
        "    train_labels_for_weights = [label_map[r['label']] for r in train_records]\n",
        "    class_weights = compute_class_weight(\n",
        "        class_weight='balanced',\n",
        "        classes=np.unique(train_labels_for_weights),\n",
        "        y=train_labels_for_weights\n",
        "    )\n",
        "    print(\"Calculated Class Weights:\", class_weights)\n",
        "    # --------------------------------------------------------\n",
        "\n",
        "    print(f\"Loading MURIL tokenizer: {cfg.model_name}...\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(cfg.model_name)\n",
        "\n",
        "    train_ds = MurilDataset(train_records, label_map, tokenizer, cfg)\n",
        "    val_ds = MurilDataset(val_records, label_map, tokenizer, cfg)\n",
        "    test_ds = MurilDataset(test_records, label_map, tokenizer, cfg)\n",
        "\n",
        "    train_loader = DataLoader(train_ds, batch_size=cfg.batch_size, shuffle=True, num_workers=2)\n",
        "    val_loader = DataLoader(val_ds, batch_size=cfg.batch_size * 2, shuffle=False, num_workers=2)\n",
        "    test_loader = DataLoader(test_ds, batch_size=cfg.batch_size * 2, shuffle=False, num_workers=2)\n",
        "\n",
        "    print(f\"Loading MURIL model: {cfg.model_name}... (This may take a moment)\")\n",
        "    model = MurilClassifier(len(label_map), cfg)\n",
        "    # --- MODIFIED: Pass class_weights to the training loop ---\n",
        "    train_loop(train_loader, val_loader, model, cfg, label_map, class_weights)\n",
        "\n",
        "    print(\"\\n--- Final Test Set Evaluation ---\")\n",
        "    model.load_state_dict(torch.load(os.path.join(cfg.output_dir, \"best_model.pt\")))\n",
        "    test_true, test_pred = evaluate(model, test_loader, cfg.device)\n",
        "    print(classification_report(test_true, test_pred, target_names=label_map.keys()))\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PFAqdEomUlW_",
        "outputId": "3cdff0d2-1e61-4087-9620-cbb7948cf1d5"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calculated Class Weights: [2.02036199 0.66443452]\n",
            "Loading MURIL tokenizer: google/muril-base-cased...\n",
            "Loading MURIL model: google/muril-base-cased... (This may take a moment)\n",
            "\n",
            "--- Starting Training on cuda ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 224/224 [02:27<00:00,  1.52it/s, loss=0.371]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 -> Val Macro F1: 0.7378\n",
            "ðŸš€ New best model saved with Macro F1: 0.7378\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 224/224 [02:27<00:00,  1.52it/s, loss=0.516]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 -> Val Macro F1: 0.8324\n",
            "ðŸš€ New best model saved with Macro F1: 0.8324\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 224/224 [02:27<00:00,  1.52it/s, loss=0.582]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 -> Val Macro F1: 0.8480\n",
            "ðŸš€ New best model saved with Macro F1: 0.8480\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 224/224 [02:27<00:00,  1.52it/s, loss=0.233]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 -> Val Macro F1: 0.8528\n",
            "ðŸš€ New best model saved with Macro F1: 0.8528\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 224/224 [02:27<00:00,  1.52it/s, loss=0.443]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5 -> Val Macro F1: 0.8528\n",
            "\n",
            "--- Final Test Set Evaluation ---\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.75      0.80      0.78       520\n",
            "    Positive       0.93      0.91      0.92      1582\n",
            "\n",
            "    accuracy                           0.89      2102\n",
            "   macro avg       0.84      0.86      0.85      2102\n",
            "weighted avg       0.89      0.89      0.89      2102\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "MuRIL Fine Tuning with K Folds cross-validation"
      ],
      "metadata": {
        "id": "-JF49iS5uw0O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Step 1: Install necessary libraries ---\n",
        "!pip install torch scikit-learn pandas tqdm transformers sentencepiece -q\n",
        "\n",
        "import os\n",
        "import time\n",
        "import random\n",
        "from dataclasses import dataclass\n",
        "from typing import List\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import StratifiedKFold # <-- NEW IMPORT\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from transformers import AutoTokenizer, AutoModel, get_linear_schedule_with_warmup\n",
        "\n",
        "# ------------------------\n",
        "# Configuration\n",
        "# ------------------------\n",
        "@dataclass\n",
        "class CFG:\n",
        "    data_csv: str = \"mal_full_sentiment.tsv\"\n",
        "    output_dir: str = \"outputs_malayalam_muril_ensembled\"\n",
        "    model_name: str = \"google/muril-base-cased\"\n",
        "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    min_class_samples: int = 3\n",
        "    n_folds: int = 5 # Number of models to train for the ensemble\n",
        "\n",
        "    # Tokenizer/Padding\n",
        "    max_len: int = 128\n",
        "\n",
        "    # Model Architecture\n",
        "    dropout: float = 0.2\n",
        "\n",
        "    # Training\n",
        "    epochs: int = 4\n",
        "    batch_size: int = 32\n",
        "    lr_muril: float = 2e-5\n",
        "    lr_head: float = 1e-3\n",
        "    label_smoothing: float = 0.1\n",
        "    seed: int = 42\n",
        "\n",
        "cfg = CFG()\n",
        "os.makedirs(cfg.output_dir, exist_ok=True)\n",
        "\n",
        "# ------------------------\n",
        "# Utilities, Dataset, Model (Same as before)\n",
        "# ------------------------\n",
        "def seed_everything(seed=42):\n",
        "    random.seed(seed); os.environ['PYTHONHASHSEED'] = str(seed); np.random.seed(seed); torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "seed_everything(cfg.seed)\n",
        "\n",
        "def normalize_label(l):\n",
        "    s = str(l).lower()\n",
        "    if 'posit' in s: return 'Positive'\n",
        "    if 'negat' in s: return 'Negative'\n",
        "    if 'neu' in s or 'normal' in s: return 'Neutral'\n",
        "    return 'Other'\n",
        "\n",
        "class MurilDataset(Dataset):\n",
        "    def __init__(self, records, label_map, tokenizer, cfg):\n",
        "        self.records, self.label_map, self.tokenizer, self.cfg = records, label_map, tokenizer, cfg\n",
        "    def __len__(self): return len(self.records)\n",
        "    def __getitem__(self, idx):\n",
        "        record = self.records[idx]; text = str(record['text'])\n",
        "        inputs = self.tokenizer.encode_plus(\n",
        "            text, add_special_tokens=True, max_length=self.cfg.max_len,\n",
        "            padding='max_length', truncation=True, return_tensors='pt'\n",
        "        )\n",
        "        return {\"input_ids\": inputs['input_ids'].squeeze(), \"attention_mask\": inputs['attention_mask'].squeeze(), \"label\": torch.tensor(self.label_map[record['label']], dtype=torch.long)}\n",
        "\n",
        "class MurilClassifier(nn.Module):\n",
        "    def __init__(self, num_labels, cfg):\n",
        "        super().__init__(); self.muril = AutoModel.from_pretrained(cfg.model_name); self.dropout = nn.Dropout(cfg.dropout); self.classifier = nn.Linear(self.muril.config.hidden_size, num_labels)\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        return self.classifier(self.dropout(self.muril(input_ids=input_ids, attention_mask=attention_mask).pooler_output))\n",
        "\n",
        "# ------------------------\n",
        "# Training & Evaluation - MODIFIED to save fold model\n",
        "# ------------------------\n",
        "def evaluate(model, dataloader, device):\n",
        "    model.eval(); y_true, y_pred, all_probs = [], [], []\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            logits = model(batch['input_ids'].to(device), batch['attention_mask'].to(device))\n",
        "            probs = torch.softmax(logits, dim=1).cpu().numpy()\n",
        "            y_true.extend(batch['label'].numpy()); y_pred.extend(np.argmax(probs, axis=1)); all_probs.append(probs)\n",
        "    return y_true, y_pred, np.concatenate(all_probs)\n",
        "\n",
        "def train_loop(train_loader, val_loader, model, cfg, label_map, class_weights, fold):\n",
        "    device = cfg.device; model.to(device)\n",
        "    optimizer = torch.optim.AdamW([{'params': model.muril.parameters(), 'lr': cfg.lr_muril}, {'params': model.classifier.parameters(), 'lr': cfg.lr_head}])\n",
        "    num_training_steps = len(train_loader) * cfg.epochs; num_warmup_steps = int(0.1 * num_training_steps)\n",
        "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_training_steps)\n",
        "    weights_tensor = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
        "    criterion = nn.CrossEntropyLoss(weight=weights_tensor, label_smoothing=cfg.label_smoothing)\n",
        "    best_macro_f1 = -1.0\n",
        "\n",
        "    print(f\"\\n--- Starting Training for Fold {fold+1}/{cfg.n_folds} on {device} ---\")\n",
        "    for epoch in range(cfg.epochs):\n",
        "        model.train()\n",
        "        pbar = tqdm(train_loader, desc=f\"Fold {fold+1} Epoch {epoch+1}/{cfg.epochs}\")\n",
        "        for batch in pbar:\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(batch['input_ids'].to(device), batch['attention_mask'].to(device))\n",
        "            loss = criterion(logits, batch['label'].to(device))\n",
        "            loss.backward(); optimizer.step(); scheduler.step(); pbar.set_postfix(loss=loss.item())\n",
        "\n",
        "        val_true, val_pred, _ = evaluate(model, val_loader, device)\n",
        "        report = classification_report(val_true, val_pred, target_names=label_map.keys(), output_dict=True, zero_division=0)\n",
        "        macro_f1 = report['macro avg']['f1-score']\n",
        "        print(f\"Fold {fold+1} Epoch {epoch+1} -> Val Macro F1: {macro_f1:.4f}\")\n",
        "        if macro_f1 > best_macro_f1:\n",
        "            best_macro_f1 = macro_f1\n",
        "            torch.save(model.state_dict(), os.path.join(cfg.output_dir, f\"best_model_fold_{fold}.pt\"))\n",
        "            print(f\"ðŸš€ New best model for Fold {fold+1} saved with Macro F1: {macro_f1:.4f}\")\n",
        "\n",
        "# ------------------------\n",
        "# Main Orchestration - MODIFIED for Cross-Validation\n",
        "# ------------------------\n",
        "def main():\n",
        "    df = pd.read_csv(cfg.data_csv, sep='\\t', header=None, names=['label', 'text'], engine='python')\n",
        "    df.dropna(subset=['text', 'label'], inplace=True); df['text'] = df['text'].astype(str); df['label'] = df['label'].apply(normalize_label); df = df[df['label'] != 'Other']\n",
        "    label_counts = df['label'].value_counts(); classes_to_keep = label_counts[label_counts >= cfg.min_class_samples].index; df = df[df['label'].isin(classes_to_keep)].reset_index(drop=True)\n",
        "    label_map = {label: i for i, label in enumerate(sorted(df['label'].unique()))}\n",
        "\n",
        "    # We will split into train and test once\n",
        "    data = df.to_dict(orient=\"records\")\n",
        "    labels = [label_map[r['label']] for r in data]\n",
        "    train_val_records, test_records, train_val_labels, test_labels = train_test_split(data, labels, test_size=0.15, random_state=cfg.seed, stratify=labels)\n",
        "\n",
        "    print(f\"Loading MURIL tokenizer: {cfg.model_name}...\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(cfg.model_name)\n",
        "\n",
        "    # --- NEW: Cross-Validation Loop ---\n",
        "    skf = StratifiedKFold(n_splits=cfg.n_folds, shuffle=True, random_state=cfg.seed)\n",
        "\n",
        "    # Convert records to numpy array for easy indexing by StratifiedKFold\n",
        "    train_val_records_np = np.array(train_val_records)\n",
        "    train_val_labels_np = np.array(train_val_labels)\n",
        "\n",
        "    for fold, (train_idx, val_idx) in enumerate(skf.split(train_val_records_np, train_val_labels_np)):\n",
        "        train_records_fold = train_val_records_np[train_idx].tolist()\n",
        "        val_records_fold = train_val_records_np[val_idx].tolist()\n",
        "\n",
        "        train_labels_fold = [label_map[r['label']] for r in train_records_fold]\n",
        "        class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(train_labels_fold), y=train_labels_fold)\n",
        "\n",
        "        train_ds = MurilDataset(train_records_fold, label_map, tokenizer, cfg)\n",
        "        val_ds = MurilDataset(val_records_fold, label_map, tokenizer, cfg)\n",
        "\n",
        "        train_loader = DataLoader(train_ds, batch_size=cfg.batch_size, shuffle=True, num_workers=2)\n",
        "        val_loader = DataLoader(val_ds, batch_size=cfg.batch_size * 2, shuffle=False, num_workers=2)\n",
        "\n",
        "        model = MurilClassifier(len(label_map), cfg)\n",
        "        train_loop(train_loader, val_loader, model, cfg, label_map, class_weights, fold)\n",
        "\n",
        "    # --- NEW: Final Evaluation with Ensembling ---\n",
        "    print(\"\\n--- Final Test Set Evaluation with Ensembling ---\")\n",
        "    test_ds = MurilDataset(test_records, label_map, tokenizer, cfg)\n",
        "    test_loader = DataLoader(test_ds, batch_size=cfg.batch_size * 2, shuffle=False, num_workers=2)\n",
        "\n",
        "    all_fold_probs = []\n",
        "    for fold in range(cfg.n_folds):\n",
        "        print(f\"Loading model from fold {fold+1} for inference...\")\n",
        "        model = MurilClassifier(len(label_map), cfg)\n",
        "        model.load_state_dict(torch.load(os.path.join(cfg.output_dir, f\"best_model_fold_{fold}.pt\")))\n",
        "        model.to(cfg.device)\n",
        "\n",
        "        _, _, fold_probs = evaluate(model, test_loader, cfg.device)\n",
        "        all_fold_probs.append(fold_probs)\n",
        "\n",
        "    # Average the probabilities from all models\n",
        "    ensembled_probs = np.mean(all_fold_probs, axis=0)\n",
        "    ensembled_preds = np.argmax(ensembled_probs, axis=1)\n",
        "\n",
        "    print(\"\\n--- Ensembled Classification Report ---\")\n",
        "    print(classification_report(test_labels, ensembled_preds, target_names=label_map.keys()))\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qVto21zwY8ht",
        "outputId": "d87487ce-f068-4d99-a7a4-234c896d855c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading MURIL tokenizer: google/muril-base-cased...\n",
            "\n",
            "--- Starting Training for Fold 1/5 on cuda ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fold 1 Epoch 1/4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 224/224 [02:27<00:00,  1.52it/s, loss=0.405]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1 Epoch 1 -> Val Macro F1: 0.6662\n",
            "ðŸš€ New best model for Fold 1 saved with Macro F1: 0.6662\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fold 1 Epoch 2/4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 224/224 [02:27<00:00,  1.52it/s, loss=0.292]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1 Epoch 2 -> Val Macro F1: 0.8191\n",
            "ðŸš€ New best model for Fold 1 saved with Macro F1: 0.8191\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fold 1 Epoch 3/4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 224/224 [02:27<00:00,  1.52it/s, loss=0.232]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1 Epoch 3 -> Val Macro F1: 0.8575\n",
            "ðŸš€ New best model for Fold 1 saved with Macro F1: 0.8575\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fold 1 Epoch 4/4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 224/224 [02:27<00:00,  1.52it/s, loss=0.365]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1 Epoch 4 -> Val Macro F1: 0.8596\n",
            "ðŸš€ New best model for Fold 1 saved with Macro F1: 0.8596\n",
            "\n",
            "--- Starting Training for Fold 2/5 on cuda ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fold 2 Epoch 1/4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 224/224 [02:27<00:00,  1.52it/s, loss=0.496]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 2 Epoch 1 -> Val Macro F1: 0.7761\n",
            "ðŸš€ New best model for Fold 2 saved with Macro F1: 0.7761\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fold 2 Epoch 2/4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 224/224 [02:27<00:00,  1.52it/s, loss=0.504]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 2 Epoch 2 -> Val Macro F1: 0.8190\n",
            "ðŸš€ New best model for Fold 2 saved with Macro F1: 0.8190\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fold 2 Epoch 3/4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 224/224 [02:27<00:00,  1.52it/s, loss=0.322]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 2 Epoch 3 -> Val Macro F1: 0.8349\n",
            "ðŸš€ New best model for Fold 2 saved with Macro F1: 0.8349\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fold 2 Epoch 4/4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 224/224 [02:27<00:00,  1.52it/s, loss=0.247]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 2 Epoch 4 -> Val Macro F1: 0.8369\n",
            "ðŸš€ New best model for Fold 2 saved with Macro F1: 0.8369\n",
            "\n",
            "--- Starting Training for Fold 3/5 on cuda ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fold 3 Epoch 1/4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 224/224 [02:27<00:00,  1.52it/s, loss=0.559]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 3 Epoch 1 -> Val Macro F1: 0.7607\n",
            "ðŸš€ New best model for Fold 3 saved with Macro F1: 0.7607\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fold 3 Epoch 2/4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 224/224 [02:27<00:00,  1.52it/s, loss=0.386]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 3 Epoch 2 -> Val Macro F1: 0.7830\n",
            "ðŸš€ New best model for Fold 3 saved with Macro F1: 0.7830\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fold 3 Epoch 3/4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 224/224 [02:27<00:00,  1.52it/s, loss=0.704]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 3 Epoch 3 -> Val Macro F1: 0.8419\n",
            "ðŸš€ New best model for Fold 3 saved with Macro F1: 0.8419\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fold 3 Epoch 4/4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 224/224 [02:27<00:00,  1.52it/s, loss=0.334]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 3 Epoch 4 -> Val Macro F1: 0.8427\n",
            "ðŸš€ New best model for Fold 3 saved with Macro F1: 0.8427\n",
            "\n",
            "--- Starting Training for Fold 4/5 on cuda ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fold 4 Epoch 1/4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 224/224 [02:27<00:00,  1.52it/s, loss=0.694]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 4 Epoch 1 -> Val Macro F1: 0.1984\n",
            "ðŸš€ New best model for Fold 4 saved with Macro F1: 0.1984\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fold 4 Epoch 2/4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 224/224 [02:27<00:00,  1.52it/s, loss=0.715]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 4 Epoch 2 -> Val Macro F1: 0.1984\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fold 4 Epoch 3/4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 224/224 [02:26<00:00,  1.53it/s, loss=0.753]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 4 Epoch 3 -> Val Macro F1: 0.1984\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fold 4 Epoch 4/4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 224/224 [02:26<00:00,  1.53it/s, loss=0.69]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 4 Epoch 4 -> Val Macro F1: 0.1984\n",
            "\n",
            "--- Starting Training for Fold 5/5 on cuda ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fold 5 Epoch 1/4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 224/224 [02:27<00:00,  1.52it/s, loss=0.601]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 5 Epoch 1 -> Val Macro F1: 0.6950\n",
            "ðŸš€ New best model for Fold 5 saved with Macro F1: 0.6950\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fold 5 Epoch 2/4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 224/224 [02:27<00:00,  1.52it/s, loss=0.433]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 5 Epoch 2 -> Val Macro F1: 0.7632\n",
            "ðŸš€ New best model for Fold 5 saved with Macro F1: 0.7632\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fold 5 Epoch 3/4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 224/224 [02:27<00:00,  1.52it/s, loss=0.267]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 5 Epoch 3 -> Val Macro F1: 0.8482\n",
            "ðŸš€ New best model for Fold 5 saved with Macro F1: 0.8482\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fold 5 Epoch 4/4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 224/224 [02:27<00:00,  1.52it/s, loss=0.217]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 5 Epoch 4 -> Val Macro F1: 0.8523\n",
            "ðŸš€ New best model for Fold 5 saved with Macro F1: 0.8523\n",
            "\n",
            "--- Final Test Set Evaluation with Ensembling ---\n",
            "Loading model from fold 1 for inference...\n",
            "Loading model from fold 2 for inference...\n",
            "Loading model from fold 3 for inference...\n",
            "Loading model from fold 4 for inference...\n",
            "Loading model from fold 5 for inference...\n",
            "\n",
            "--- Ensembled Classification Report ---\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.75      0.81      0.78       390\n",
            "    Positive       0.94      0.91      0.92      1187\n",
            "\n",
            "    accuracy                           0.89      1577\n",
            "   macro avg       0.84      0.86      0.85      1577\n",
            "weighted avg       0.89      0.89      0.89      1577\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "M2ZxQyjgcsgC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}