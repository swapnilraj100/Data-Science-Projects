{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "MuRIL Transformer Embeddings , TF-IDF, CharCNN, BiLSTM Hybrid Model\n",
        "\n"
      ],
      "metadata": {
        "id": "P_4vzstVsunW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Step 1: Install necessary libraries, including Hugging Face Transformers ---\n",
        "!pip install torch scikit-learn pandas tqdm transformers sentencepiece -q\n",
        "\n",
        "import os\n",
        "import time\n",
        "import random\n",
        "import json\n",
        "from collections import Counter\n",
        "from dataclasses import dataclass\n",
        "from typing import List\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from transformers import AutoTokenizer, AutoModel, get_linear_schedule_with_warmup\n",
        "from tqdm import tqdm # <--- FIX: Added the missing import for the progress bar\n",
        "\n",
        "# ------------------------\n",
        "# Configuration\n",
        "# ------------------------\n",
        "@dataclass\n",
        "class CFG:\n",
        "    data_csv: str = \"mal_full_sentiment.tsv\"\n",
        "    output_dir: str = \"outputs_malayalam_muril_hybrid_advanced\"\n",
        "    model_name: str = \"google/muril-base-cased\"\n",
        "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    min_class_samples: int = 3\n",
        "    max_token_len: int = 128\n",
        "    max_char_len: int = 256\n",
        "    tfidf_max_features: int = 5000\n",
        "    tfidf_proj_dim: int = 64\n",
        "    muril_hidden_size: int = 768\n",
        "    lstm_hidden_dim: int = 256\n",
        "    lstm_layers: int = 1\n",
        "    char_emb_dim: int = 50\n",
        "    char_out_dim: int = 100\n",
        "    aux_dim: int = 8\n",
        "    dropout: float = 0.4\n",
        "    epochs: int = 5\n",
        "    batch_size: int = 32\n",
        "    lr_muril: float = 2e-5\n",
        "    lr_recurrent: float = 1e-4\n",
        "    lr_head: float = 1e-3\n",
        "    label_smoothing: float = 0.1\n",
        "    seed: int = 42\n",
        "\n",
        "cfg = CFG()\n",
        "os.makedirs(cfg.output_dir, exist_ok=True)\n",
        "\n",
        "# ------------------------\n",
        "# Utilities & Preprocessing\n",
        "# ------------------------\n",
        "def seed_everything(seed=42):\n",
        "    random.seed(seed); os.environ['PYTHONHASHSEED'] = str(seed); np.random.seed(seed); torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "seed_everything(cfg.seed)\n",
        "\n",
        "def normalize_label(l):\n",
        "    s = str(l).lower()\n",
        "    if 'posit' in s: return 'Positive'\n",
        "    if 'negat' in s: return 'Negative'\n",
        "    if 'neu' in s or 'normal' in s: return 'Neutral'\n",
        "    return 'Other'\n",
        "\n",
        "def compute_aux_features(text: str) -> List[float]:\n",
        "    toks = text.split(); num_tokens = len(toks); num_chars = len(text)\n",
        "    return [num_tokens, num_chars, 1.0 if any('\\u0D00' <= ch <= '\\u0D7F' for ch in text) else 0.0, 1.0 if any('a' <= ch.lower() <= 'z' for ch in text) else 0.0, (sum(len(t) for t in toks) / num_tokens) if num_tokens > 0 else 0.0, sum(1 for ch in text if ch.isupper()) / (num_chars + 1e-6), sum(1 for ch in text if ch in '?!.,;:'), sum(1 for ch in text if ord(ch) > 10000)]\n",
        "\n",
        "# ------------------------\n",
        "# Dataset Class\n",
        "# ------------------------\n",
        "class MurilHybridDataset(Dataset):\n",
        "    def __init__(self, records, tfidf_vectors, label_map, char2idx, tokenizer, cfg):\n",
        "        self.records, self.tfidf_vectors, self.label_map, self.char2idx, self.tokenizer, self.cfg = records, tfidf_vectors, label_map, char2idx, tokenizer, cfg\n",
        "    def __len__(self): return len(self.records)\n",
        "    def __getitem__(self, idx):\n",
        "        record = self.records[idx]; text = str(record['text'])\n",
        "        inputs = self.tokenizer.encode_plus(text, add_special_tokens=True, max_length=self.cfg.max_token_len, padding='max_length', truncation=True, return_tensors='pt')\n",
        "        char_ids = [self.char2idx.get(c, self.char2idx['<unk>']) for c in text][:self.cfg.max_char_len]\n",
        "        padded_chars = char_ids + [0] * (self.cfg.max_char_len - len(char_ids))\n",
        "        return {\n",
        "            \"input_ids\": inputs['input_ids'].squeeze(), \"attention_mask\": inputs['attention_mask'].squeeze(),\n",
        "            \"char_ids\": torch.tensor(padded_chars, dtype=torch.long),\n",
        "            \"aux\": torch.tensor(compute_aux_features(text), dtype=torch.float32),\n",
        "            \"tfidf\": torch.tensor(self.tfidf_vectors[idx].toarray().squeeze(), dtype=torch.float32),\n",
        "            \"label\": torch.tensor(self.label_map[record['label']], dtype=torch.long)\n",
        "        }\n",
        "\n",
        "# ------------------------\n",
        "# Model Architecture\n",
        "# ------------------------\n",
        "class CharCNN(nn.Module):\n",
        "    def __init__(self, char_vocab_size, char_emb_dim, out_dim):\n",
        "        super().__init__(); self.char_emb = nn.Embedding(char_vocab_size, char_emb_dim, padding_idx=0); self.conv = nn.Conv1d(char_emb_dim, out_dim, kernel_size=3, padding=1)\n",
        "    def forward(self, x_char):\n",
        "        x = self.char_emb(x_char).transpose(1, 2); x = self.conv(x); return F.max_pool1d(x, x.size(2)).squeeze(2)\n",
        "\n",
        "class MurilHybridClassifier(nn.Module):\n",
        "    def __init__(self, char_vocab_size, num_labels, cfg):\n",
        "        super().__init__()\n",
        "        self.muril = AutoModel.from_pretrained(cfg.model_name)\n",
        "        self.bilstm = nn.LSTM(cfg.muril_hidden_size, cfg.lstm_hidden_dim // 2, num_layers=cfg.lstm_layers, bidirectional=True, batch_first=True)\n",
        "        self.char_cnn = CharCNN(char_vocab_size, cfg.char_emb_dim, cfg.char_out_dim)\n",
        "        self.tfidf_proj = nn.Linear(cfg.tfidf_max_features, cfg.tfidf_proj_dim)\n",
        "        self.aux_proj = nn.Linear(cfg.aux_dim, cfg.aux_dim)\n",
        "        classifier_input_dim = cfg.lstm_hidden_dim + cfg.char_out_dim + cfg.aux_dim + cfg.tfidf_proj_dim\n",
        "        self.classifier = nn.Sequential(nn.Linear(classifier_input_dim, 256), nn.ReLU(), nn.Dropout(cfg.dropout), nn.Linear(256, num_labels))\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, char_ids, aux, tfidf):\n",
        "        muril_embeddings = self.muril(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state\n",
        "        _, (h_n, _) = self.bilstm(muril_embeddings)\n",
        "        hidden = torch.cat((h_n[-2,:,:], h_n[-1,:,:]), dim=1)\n",
        "        char_vec = self.char_cnn(char_ids)\n",
        "        aux_vec = F.relu(self.aux_proj(aux))\n",
        "        tfidf_vec = F.relu(self.tfidf_proj(tfidf))\n",
        "        combined_features = torch.cat([hidden, char_vec, aux_vec, tfidf_vec], dim=1)\n",
        "        return self.classifier(combined_features)\n",
        "\n",
        "# ------------------------\n",
        "# Training & Evaluation\n",
        "# ------------------------\n",
        "def evaluate(model, dataloader, device):\n",
        "    model.eval(); y_true, y_pred = [], []\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            logits = model(batch['input_ids'].to(device), batch['attention_mask'].to(device), batch['char_ids'].to(device), batch['aux'].to(device), batch['tfidf'].to(device))\n",
        "            y_true.extend(batch['label'].numpy()); y_pred.extend(torch.argmax(logits, dim=1).cpu().numpy())\n",
        "    return y_true, y_pred\n",
        "\n",
        "def train_loop(train_loader, val_loader, model, cfg, label_map, class_weights):\n",
        "    device = cfg.device; model.to(device)\n",
        "    optimizer = torch.optim.AdamW([\n",
        "        {'params': model.muril.parameters(), 'lr': cfg.lr_muril},\n",
        "        {'params': model.bilstm.parameters(), 'lr': cfg.lr_recurrent},\n",
        "        {'params': model.char_cnn.parameters(), 'lr': cfg.lr_head},\n",
        "        {'params': model.tfidf_proj.parameters(), 'lr': cfg.lr_head},\n",
        "        {'params': model.aux_proj.parameters(), 'lr': cfg.lr_head},\n",
        "        {'params': model.classifier.parameters(), 'lr': cfg.lr_head}\n",
        "    ])\n",
        "    num_training_steps = len(train_loader) * cfg.epochs\n",
        "    num_warmup_steps = int(0.1 * num_training_steps)\n",
        "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_training_steps)\n",
        "    weights_tensor = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
        "    criterion = nn.CrossEntropyLoss(weight=weights_tensor, label_smoothing=cfg.label_smoothing)\n",
        "\n",
        "    best_macro_f1 = -1.0\n",
        "    print(f\"\\n--- Starting Advanced Training on {device} ---\")\n",
        "    for epoch in range(cfg.epochs):\n",
        "        model.train()\n",
        "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{cfg.epochs}\")\n",
        "        for batch in pbar:\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(batch['input_ids'].to(device), batch['attention_mask'].to(device), batch['char_ids'].to(device), batch['aux'].to(device), batch['tfidf'].to(device))\n",
        "            loss = criterion(logits, batch['label'].to(device))\n",
        "            loss.backward(); optimizer.step(); scheduler.step(); pbar.set_postfix(loss=loss.item())\n",
        "\n",
        "        val_true, val_pred = evaluate(model, val_loader, device)\n",
        "        report = classification_report(val_true, val_pred, target_names=label_map.keys(), output_dict=True, zero_division=0)\n",
        "        macro_f1 = report['macro avg']['f1-score']\n",
        "        print(f\"Epoch {epoch+1} -> Val Macro F1: {macro_f1:.4f}\")\n",
        "        if macro_f1 > best_macro_f1:\n",
        "            best_macro_f1 = macro_f1; torch.save(model.state_dict(), os.path.join(cfg.output_dir, \"best_model.pt\")); print(f\"🚀 New best model saved with Macro F1: {macro_f1:.4f}\")\n",
        "\n",
        "# ------------------------\n",
        "# Main Orchestration\n",
        "# ------------------------\n",
        "def main():\n",
        "    df = pd.read_csv(cfg.data_csv, sep='\\t', header=None, names=['label', 'text'], engine='python')\n",
        "    df.dropna(subset=['text', 'label'], inplace=True); df['text'] = df['text'].astype(str); df['label'] = df['label'].apply(normalize_label); df = df[df['label'] != 'Other']\n",
        "    label_counts = df['label'].value_counts(); classes_to_keep = label_counts[label_counts >= cfg.min_class_samples].index; df = df[df['label'].isin(classes_to_keep)].reset_index(drop=True)\n",
        "    label_map = {label: i for i, label in enumerate(sorted(df['label'].unique()))}\n",
        "\n",
        "    data = df.to_dict(orient=\"records\"); labels = [label_map[r['label']] for r in data]\n",
        "    train_records, test_records, _, _ = train_test_split(data, labels, test_size=0.2, random_state=cfg.seed, stratify=labels)\n",
        "    train_records, val_records = train_test_split(train_records, test_size=0.15, random_state=cfg.seed, stratify=[label_map[r['label']] for r in train_records])\n",
        "\n",
        "    train_labels_for_weights = [label_map[r['label']] for r in train_records]\n",
        "    class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(train_labels_for_weights), y=train_labels_for_weights)\n",
        "    print(\"Calculated Class Weights:\", class_weights)\n",
        "\n",
        "    print(\"Fitting TF-IDF Vectorizer...\")\n",
        "    train_texts = [r['text'] for r in train_records]; tfidf_vectorizer = TfidfVectorizer(max_features=cfg.tfidf_max_features, ngram_range=(1, 2)); train_tfidf = tfidf_vectorizer.fit_transform(train_texts); val_tfidf = tfidf_vectorizer.transform([r['text'] for r in val_records]); test_tfidf = tfidf_vectorizer.transform([r['text'] for r in test_records])\n",
        "\n",
        "    print(f\"Loading MURIL tokenizer: {cfg.model_name}...\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(cfg.model_name)\n",
        "    char2idx = {c: i+2 for i, c in enumerate(Counter(c for text in train_texts for c in text).keys())}\n",
        "    char2idx['<pad>'] = 0; char2idx['<unk>'] = 1\n",
        "\n",
        "    train_ds = MurilHybridDataset(train_records, train_tfidf, label_map, char2idx, tokenizer, cfg)\n",
        "    val_ds = MurilHybridDataset(val_records, val_tfidf, label_map, char2idx, tokenizer, cfg)\n",
        "    test_ds = MurilHybridDataset(test_records, test_tfidf, label_map, char2idx, tokenizer, cfg)\n",
        "\n",
        "    train_loader = DataLoader(train_ds, batch_size=cfg.batch_size, shuffle=True, num_workers=2)\n",
        "    val_loader = DataLoader(val_ds, batch_size=cfg.batch_size * 2, shuffle=False, num_workers=2)\n",
        "    test_loader = DataLoader(test_ds, batch_size=cfg.batch_size * 2, shuffle=False, num_workers=2)\n",
        "\n",
        "    print(f\"Loading MURIL model: {cfg.model_name}...\")\n",
        "    model = MurilHybridClassifier(len(char2idx), len(label_map), cfg)\n",
        "\n",
        "    train_loop(train_loader, val_loader, model, cfg, label_map, class_weights)\n",
        "\n",
        "    print(\"\\n--- Final Test Set Evaluation ---\")\n",
        "    model.load_state_dict(torch.load(os.path.join(cfg.output_dir, \"best_model.pt\")))\n",
        "    test_true, test_pred = evaluate(model, test_loader, cfg.device)\n",
        "    print(classification_report(test_true, test_pred, target_names=label_map.keys()))\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KP19hqxBIAto",
        "outputId": "6c267b79-0266-4684-989f-41fdc8822804"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calculated Class Weights: [2.02036199 0.66443452]\n",
            "Fitting TF-IDF Vectorizer...\n",
            "Loading MURIL tokenizer: google/muril-base-cased...\n",
            "Loading MURIL model: google/muril-base-cased...\n",
            "\n",
            "--- Starting Advanced Training on cuda ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/5: 100%|██████████| 224/224 [02:43<00:00,  1.37it/s, loss=0.482]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 -> Val Macro F1: 0.7840\n",
            "🚀 New best model saved with Macro F1: 0.7840\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/5: 100%|██████████| 224/224 [02:45<00:00,  1.36it/s, loss=0.639]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 -> Val Macro F1: 0.8444\n",
            "🚀 New best model saved with Macro F1: 0.8444\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/5: 100%|██████████| 224/224 [02:45<00:00,  1.35it/s, loss=0.701]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 -> Val Macro F1: 0.8549\n",
            "🚀 New best model saved with Macro F1: 0.8549\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/5: 100%|██████████| 224/224 [02:45<00:00,  1.35it/s, loss=0.266]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 -> Val Macro F1: 0.8570\n",
            "🚀 New best model saved with Macro F1: 0.8570\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/5: 100%|██████████| 224/224 [02:45<00:00,  1.35it/s, loss=0.59]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5 -> Val Macro F1: 0.8608\n",
            "🚀 New best model saved with Macro F1: 0.8608\n",
            "\n",
            "--- Final Test Set Evaluation ---\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.76      0.83      0.79       520\n",
            "    Positive       0.94      0.91      0.93      1582\n",
            "\n",
            "    accuracy                           0.89      2102\n",
            "   macro avg       0.85      0.87      0.86      2102\n",
            "weighted avg       0.90      0.89      0.89      2102\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "MuRIL Transformer Embeddings, CharCNN, TF-IDF, BiLSTM +Attention Hybrid Models"
      ],
      "metadata": {
        "id": "OwhTBAf5tJZ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Step 1: Install necessary libraries ---\n",
        "!pip install torch scikit-learn pandas tqdm transformers sentencepiece -q\n",
        "\n",
        "import os\n",
        "import time\n",
        "import random\n",
        "from dataclasses import dataclass\n",
        "from typing import List\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from transformers import AutoTokenizer, AutoModel, get_linear_schedule_with_warmup\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ------------------------\n",
        "# Configuration\n",
        "# ------------------------\n",
        "@dataclass\n",
        "class CFG:\n",
        "    data_csv: str = \"mal_full_sentiment.tsv\"\n",
        "    output_dir: str = \"outputs_malayalam_muril_attention_hybrid\"\n",
        "    model_name: str = \"google/muril-base-cased\"\n",
        "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    min_class_samples: int = 3\n",
        "    max_token_len: int = 128\n",
        "    max_char_len: int = 256\n",
        "    tfidf_max_features: int = 5000\n",
        "    tfidf_proj_dim: int = 64\n",
        "    muril_hidden_size: int = 768\n",
        "    lstm_hidden_dim: int = 256\n",
        "    lstm_layers: int = 1\n",
        "    char_emb_dim: int = 50\n",
        "    char_out_dim: int = 100\n",
        "    aux_dim: int = 8\n",
        "    dropout: float = 0.4\n",
        "    epochs: int = 5\n",
        "    batch_size: int = 32\n",
        "    lr_muril: float = 2e-5\n",
        "    lr_recurrent: float = 1e-4\n",
        "    lr_head: float = 1e-3\n",
        "    label_smoothing: float = 0.1\n",
        "    seed: int = 42\n",
        "\n",
        "cfg = CFG()\n",
        "os.makedirs(cfg.output_dir, exist_ok=True)\n",
        "\n",
        "# ------------------------\n",
        "# Utilities & Preprocessing\n",
        "# ------------------------\n",
        "def seed_everything(seed=42):\n",
        "    random.seed(seed); os.environ['PYTHONHASHSEED'] = str(seed); np.random.seed(seed); torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "seed_everything(cfg.seed)\n",
        "\n",
        "def normalize_label(l):\n",
        "    s = str(l).lower()\n",
        "    if 'posit' in s: return 'Positive'\n",
        "    if 'negat' in s: return 'Negative'\n",
        "    if 'neu' in s or 'normal' in s: return 'Neutral'\n",
        "    return 'Other'\n",
        "\n",
        "def compute_aux_features(text: str) -> List[float]:\n",
        "    toks = text.split(); num_tokens = len(toks); num_chars = len(text)\n",
        "    return [num_tokens, num_chars, 1.0 if any('\\u0D00' <= ch <= '\\u0D7F' for ch in text) else 0.0, 1.0 if any('a' <= ch.lower() <= 'z' for ch in text) else 0.0, (sum(len(t) for t in toks) / num_tokens) if num_tokens > 0 else 0.0, sum(1 for ch in text if ch.isupper()) / (num_chars + 1e-6), sum(1 for ch in text if ch in '?!.,;:'), sum(1 for ch in text if ord(ch) > 10000)]\n",
        "\n",
        "# ------------------------\n",
        "# Dataset Class\n",
        "# ------------------------\n",
        "class MurilHybridDataset(Dataset):\n",
        "    def __init__(self, records, tfidf_vectors, label_map, char2idx, tokenizer, cfg):\n",
        "        self.records, self.tfidf_vectors, self.label_map, self.char2idx, self.tokenizer, self.cfg = records, tfidf_vectors, label_map, char2idx, tokenizer, cfg\n",
        "    def __len__(self): return len(self.records)\n",
        "    def __getitem__(self, idx):\n",
        "        record = self.records[idx]; text = str(record['text'])\n",
        "        inputs = self.tokenizer.encode_plus(text, add_special_tokens=True, max_length=self.cfg.max_token_len, padding='max_length', truncation=True, return_tensors='pt')\n",
        "        char_ids = [self.char2idx.get(c, self.char2idx['<unk>']) for c in text][:self.cfg.max_char_len]\n",
        "        padded_chars = char_ids + [0] * (self.cfg.max_char_len - len(char_ids))\n",
        "        return {\n",
        "            \"input_ids\": inputs['input_ids'].squeeze(), \"attention_mask\": inputs['attention_mask'].squeeze(),\n",
        "            \"char_ids\": torch.tensor(padded_chars, dtype=torch.long),\n",
        "            \"aux\": torch.tensor(compute_aux_features(text), dtype=torch.float32),\n",
        "            \"tfidf\": torch.tensor(self.tfidf_vectors[idx].toarray().squeeze(), dtype=torch.float32),\n",
        "            \"label\": torch.tensor(self.label_map[record['label']], dtype=torch.long)\n",
        "        }\n",
        "\n",
        "# ------------------------\n",
        "# Model Architecture\n",
        "# ------------------------\n",
        "class CharCNN(nn.Module):\n",
        "    def __init__(self, char_vocab_size, char_emb_dim, out_dim):\n",
        "        super().__init__(); self.char_emb = nn.Embedding(char_vocab_size, char_emb_dim, padding_idx=0); self.conv = nn.Conv1d(char_emb_dim, out_dim, kernel_size=3, padding=1)\n",
        "    def forward(self, x_char):\n",
        "        x = self.char_emb(x_char).transpose(1, 2); x = self.conv(x); return F.max_pool1d(x, x.size(2)).squeeze(2)\n",
        "\n",
        "# --- NEW: Attention Mechanism ---\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.attention = nn.Linear(hidden_dim, 1, bias=False)\n",
        "\n",
        "    def forward(self, lstm_output, attention_mask):\n",
        "        # lstm_output shape: (B, T, H)\n",
        "        scores = self.attention(lstm_output).squeeze(-1)  # (B, T)\n",
        "        # Mask out the padding tokens so they don't get attention\n",
        "        scores = scores.masked_fill(attention_mask == 0, -1e9)\n",
        "        weights = torch.softmax(scores, dim=1)  # (B, T)\n",
        "        # Create context vector\n",
        "        context = torch.bmm(weights.unsqueeze(1), lstm_output).squeeze(1) # (B, H)\n",
        "        return context\n",
        "\n",
        "class MurilHybridClassifier(nn.Module):\n",
        "    def __init__(self, char_vocab_size, num_labels, cfg):\n",
        "        super().__init__()\n",
        "        self.muril = AutoModel.from_pretrained(cfg.model_name)\n",
        "        self.bilstm = nn.LSTM(cfg.muril_hidden_size, cfg.lstm_hidden_dim // 2, num_layers=cfg.lstm_layers, bidirectional=True, batch_first=True)\n",
        "        # --- NEW: Attention layer ---\n",
        "        self.attention = Attention(cfg.lstm_hidden_dim)\n",
        "        self.char_cnn = CharCNN(char_vocab_size, cfg.char_emb_dim, cfg.char_out_dim)\n",
        "        self.tfidf_proj = nn.Linear(cfg.tfidf_max_features, cfg.tfidf_proj_dim)\n",
        "        self.aux_proj = nn.Linear(cfg.aux_dim, cfg.aux_dim)\n",
        "        classifier_input_dim = cfg.lstm_hidden_dim + cfg.char_out_dim + cfg.aux_dim + cfg.tfidf_proj_dim\n",
        "        self.classifier = nn.Sequential(nn.Linear(classifier_input_dim, 256), nn.ReLU(), nn.Dropout(cfg.dropout), nn.Linear(256, num_labels))\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, char_ids, aux, tfidf):\n",
        "        muril_embeddings = self.muril(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state\n",
        "        lstm_output, _ = self.bilstm(muril_embeddings)\n",
        "        # --- NEW: Apply Attention mechanism ---\n",
        "        # We use the original attention_mask from the tokenizer\n",
        "        context_vector = self.attention(lstm_output, attention_mask)\n",
        "        # ------------------------------------\n",
        "        char_vec = self.char_cnn(char_ids)\n",
        "        aux_vec = F.relu(self.aux_proj(aux))\n",
        "        tfidf_vec = F.relu(self.tfidf_proj(tfidf))\n",
        "        combined_features = torch.cat([context_vector, char_vec, aux_vec, tfidf_vec], dim=1)\n",
        "        return self.classifier(combined_features)\n",
        "\n",
        "# ------------------------\n",
        "# Training & Evaluation (Same as before)\n",
        "# ------------------------\n",
        "def evaluate(model, dataloader, device):\n",
        "    model.eval(); y_true, y_pred = [], []\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            logits = model(batch['input_ids'].to(device), batch['attention_mask'].to(device), batch['char_ids'].to(device), batch['aux'].to(device), batch['tfidf'].to(device))\n",
        "            y_true.extend(batch['label'].numpy()); y_pred.extend(torch.argmax(logits, dim=1).cpu().numpy())\n",
        "    return y_true, y_pred\n",
        "\n",
        "def train_loop(train_loader, val_loader, model, cfg, label_map, class_weights):\n",
        "    device = cfg.device; model.to(device)\n",
        "    optimizer = torch.optim.AdamW([\n",
        "        {'params': model.muril.parameters(), 'lr': cfg.lr_muril},\n",
        "        {'params': model.bilstm.parameters(), 'lr': cfg.lr_recurrent},\n",
        "        {'params': model.attention.parameters(), 'lr': cfg.lr_head},\n",
        "        {'params': model.char_cnn.parameters(), 'lr': cfg.lr_head},\n",
        "        {'params': model.tfidf_proj.parameters(), 'lr': cfg.lr_head},\n",
        "        {'params': model.aux_proj.parameters(), 'lr': cfg.lr_head},\n",
        "        {'params': model.classifier.parameters(), 'lr': cfg.lr_head}\n",
        "    ])\n",
        "    num_training_steps = len(train_loader) * cfg.epochs\n",
        "    num_warmup_steps = int(0.1 * num_training_steps)\n",
        "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_training_steps)\n",
        "    weights_tensor = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
        "    criterion = nn.CrossEntropyLoss(weight=weights_tensor, label_smoothing=cfg.label_smoothing)\n",
        "    best_macro_f1 = -1.0\n",
        "    print(f\"\\n--- Starting Advanced Training on {device} ---\")\n",
        "    for epoch in range(cfg.epochs):\n",
        "        model.train()\n",
        "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{cfg.epochs}\")\n",
        "        for batch in pbar:\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(batch['input_ids'].to(device), batch['attention_mask'].to(device), batch['char_ids'].to(device), batch['aux'].to(device), batch['tfidf'].to(device))\n",
        "            loss = criterion(logits, batch['label'].to(device))\n",
        "            loss.backward(); optimizer.step(); scheduler.step(); pbar.set_postfix(loss=loss.item())\n",
        "\n",
        "        val_true, val_pred = evaluate(model, val_loader, device)\n",
        "        report = classification_report(val_true, val_pred, target_names=label_map.keys(), output_dict=True, zero_division=0)\n",
        "        macro_f1 = report['macro avg']['f1-score']\n",
        "        print(f\"Epoch {epoch+1} -> Val Macro F1: {macro_f1:.4f}\")\n",
        "        if macro_f1 > best_macro_f1:\n",
        "            best_macro_f1 = macro_f1; torch.save(model.state_dict(), os.path.join(cfg.output_dir, \"best_model.pt\")); print(f\"🚀 New best model saved with Macro F1: {macro_f1:.4f}\")\n",
        "\n",
        "# ------------------------\n",
        "# Main Orchestration\n",
        "# ------------------------\n",
        "def main():\n",
        "    df = pd.read_csv(cfg.data_csv, sep='\\t', header=None, names=['label', 'text'], engine='python')\n",
        "    df.dropna(subset=['text', 'label'], inplace=True); df['text'] = df['text'].astype(str); df['label'] = df['label'].apply(normalize_label); df = df[df['label'] != 'Other']\n",
        "    label_counts = df['label'].value_counts(); classes_to_keep = label_counts[label_counts >= cfg.min_class_samples].index; df = df[df['label'].isin(classes_to_keep)].reset_index(drop=True)\n",
        "    label_map = {label: i for i, label in enumerate(sorted(df['label'].unique()))}\n",
        "\n",
        "    data = df.to_dict(orient=\"records\"); labels = [label_map[r['label']] for r in data]\n",
        "    train_records, test_records, _, _ = train_test_split(data, labels, test_size=0.2, random_state=cfg.seed, stratify=labels)\n",
        "    train_records, val_records = train_test_split(train_records, test_size=0.15, random_state=cfg.seed, stratify=[label_map[r['label']] for r in train_records])\n",
        "\n",
        "    train_labels_for_weights = [label_map[r['label']] for r in train_records]\n",
        "    class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(train_labels_for_weights), y=train_labels_for_weights)\n",
        "    print(\"Calculated Class Weights:\", class_weights)\n",
        "\n",
        "    print(\"Fitting TF-IDF Vectorizer...\")\n",
        "    train_texts = [r['text'] for r in train_records]; tfidf_vectorizer = TfidfVectorizer(max_features=cfg.tfidf_max_features, ngram_range=(1, 2)); train_tfidf = tfidf_vectorizer.fit_transform(train_texts); val_tfidf = tfidf_vectorizer.transform([r['text'] for r in val_records]); test_tfidf = tfidf_vectorizer.transform([r['text'] for r in test_records])\n",
        "\n",
        "    print(f\"Loading MURIL tokenizer: {cfg.model_name}...\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(cfg.model_name)\n",
        "    char2idx = {c: i+2 for i, c in enumerate(Counter(c for text in train_texts for c in text).keys())}\n",
        "    char2idx['<pad>'] = 0; char2idx['<unk>'] = 1\n",
        "\n",
        "    train_ds = MurilHybridDataset(train_records, train_tfidf, label_map, char2idx, tokenizer, cfg)\n",
        "    val_ds = MurilHybridDataset(val_records, val_tfidf, label_map, char2idx, tokenizer, cfg)\n",
        "    test_ds = MurilHybridDataset(test_records, test_tfidf, label_map, char2idx, tokenizer, cfg)\n",
        "\n",
        "    train_loader = DataLoader(train_ds, batch_size=cfg.batch_size, shuffle=True, num_workers=2)\n",
        "    val_loader = DataLoader(val_ds, batch_size=cfg.batch_size * 2, shuffle=False, num_workers=2)\n",
        "    test_loader = DataLoader(test_ds, batch_size=cfg.batch_size * 2, shuffle=False, num_workers=2)\n",
        "\n",
        "    print(f\"Loading MURIL model: {cfg.model_name}...\")\n",
        "    model = MurilHybridClassifier(len(char2idx), len(label_map), cfg)\n",
        "\n",
        "    train_loop(train_loader, val_loader, model, cfg, label_map, class_weights)\n",
        "\n",
        "    print(\"\\n--- Final Test Set Evaluation ---\")\n",
        "    model.load_state_dict(torch.load(os.path.join(cfg.output_dir, \"best_model.pt\")))\n",
        "    test_true, test_pred = evaluate(model, test_loader, cfg.device)\n",
        "    print(classification_report(test_true, test_pred, target_names=label_map.keys()))\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vMMDM0l1dnfx",
        "outputId": "f43cd12b-7447-4059-ff69-c4342df182df"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calculated Class Weights: [2.02036199 0.66443452]\n",
            "Fitting TF-IDF Vectorizer...\n",
            "Loading MURIL tokenizer: google/muril-base-cased...\n",
            "Loading MURIL model: google/muril-base-cased...\n",
            "\n",
            "--- Starting Advanced Training on cuda ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/5: 100%|██████████| 224/224 [02:41<00:00,  1.38it/s, loss=0.517]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 -> Val Macro F1: 0.7937\n",
            "🚀 New best model saved with Macro F1: 0.7937\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/5: 100%|██████████| 224/224 [02:45<00:00,  1.36it/s, loss=0.357]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 -> Val Macro F1: 0.8502\n",
            "🚀 New best model saved with Macro F1: 0.8502\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/5: 100%|██████████| 224/224 [02:45<00:00,  1.36it/s, loss=0.43]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 -> Val Macro F1: 0.8509\n",
            "🚀 New best model saved with Macro F1: 0.8509\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/5: 100%|██████████| 224/224 [02:45<00:00,  1.35it/s, loss=0.212]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 -> Val Macro F1: 0.8673\n",
            "🚀 New best model saved with Macro F1: 0.8673\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/5: 100%|██████████| 224/224 [02:45<00:00,  1.36it/s, loss=0.614]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5 -> Val Macro F1: 0.8701\n",
            "🚀 New best model saved with Macro F1: 0.8701\n",
            "\n",
            "--- Final Test Set Evaluation ---\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.75      0.84      0.79       520\n",
            "    Positive       0.94      0.91      0.93      1582\n",
            "\n",
            "    accuracy                           0.89      2102\n",
            "   macro avg       0.85      0.87      0.86      2102\n",
            "weighted avg       0.90      0.89      0.89      2102\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "20UTj4dwmE_H"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}