{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "full pipeline (BiLSTM + FastText + Attention)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AJzkfmd08OJ5",
        "outputId": "a8b5a345-392c-485a-de0d-16017b2c6768"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Step 1: Loading and Cleaning Data ---\n",
            "Filtering out classes with less than 3 samples.\n",
            "‚úÖ Data loading complete. Kept 44019 records.\n",
            "\n",
            "--- Step 2: Splitting Data and Building Vocabulary ---\n",
            "‚úÖ Vocabulary built with 21993 unique tokens.\n",
            "\n",
            "--- Step 3: Training FastText Embeddings ---\n",
            "‚úÖ FastText model found at outputs_bilstm/fasttext_tamil.model, loading...\n",
            "\n",
            "--- Step 4: Building Model and DataLoaders ---\n",
            "‚è≥ Initializing embedding layer with FastText vectors...\n",
            "‚úÖ Found 21993/21993 words in FastText model.\n",
            "‚úÖ All setup complete. Starting training on cuda...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train E1/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 497/497 [00:11<00:00, 44.67it/s, loss=0.108]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 -> Val Macro F1: 0.4199\n",
            "üöÄ New best model saved with Macro F1: 0.4199\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train E2/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 497/497 [00:11<00:00, 44.07it/s, loss=0.0611]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2 -> Val Macro F1: 0.4175\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train E3/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 497/497 [00:11<00:00, 43.93it/s, loss=0.0434]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3 -> Val Macro F1: 0.4253\n",
            "üöÄ New best model saved with Macro F1: 0.4253\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train E4/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 497/497 [00:11<00:00, 44.61it/s, loss=0.0374]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4 -> Val Macro F1: 0.4415\n",
            "üöÄ New best model saved with Macro F1: 0.4415\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train E5/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 497/497 [00:11<00:00, 44.75it/s, loss=0.0207]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5 -> Val Macro F1: 0.4274\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train E6/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 497/497 [00:10<00:00, 45.49it/s, loss=0.0185]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 6 -> Val Macro F1: 0.4371\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train E7/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 497/497 [00:10<00:00, 45.47it/s, loss=0.0124]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 7 -> Val Macro F1: 0.4397\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train E8/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 497/497 [00:11<00:00, 44.80it/s, loss=0.0178]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 8 -> Val Macro F1: 0.4332\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train E9/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 497/497 [00:11<00:00, 44.72it/s, loss=0.0145]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 9 -> Val Macro F1: 0.4212\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train E10/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 497/497 [00:11<00:00, 44.82it/s, loss=0.00613]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 10 -> Val Macro F1: 0.4240\n",
            "\n",
            "--- Final Test Set Evaluation ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Classification Report:\n",
            "\n",
            "                precision    recall  f1-score   support\n",
            "\n",
            "Mixed_feelings     0.1996    0.3938    0.2649       739\n",
            "      Negative     0.3410    0.3954    0.3662       784\n",
            "      Positive     0.8133    0.5162    0.6316      3731\n",
            "     not-Tamil     0.3870    0.6837    0.4942       313\n",
            " unknown_state     0.3863    0.4903    0.4322      1036\n",
            "\n",
            "      accuracy                         0.4920      6603\n",
            "     macro avg     0.4255    0.4959    0.4378      6603\n",
            "  weighted avg     0.6014    0.4920    0.5212      6603\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            " [[ 291  145  144   23  136]\n",
            " [ 202  310  115   25  132]\n",
            " [ 752  323 1926  234  496]\n",
            " [  20   10   26  214   43]\n",
            " [ 193  121  157   57  508]]\n",
            "\n",
            "Total execution time: 2.09 minutes.\n"
          ]
        }
      ],
      "source": [
        "# Single Colab cell: full pipeline (BiLSTM + FastText) with corrected evaluate()\n",
        "# Step 1: Install necessary libraries (uncomment if running fresh)\n",
        "# !pip install torch scikit-learn gensim tensorboardX pandas tqdm -q\n",
        "\n",
        "import os\n",
        "import json\n",
        "import random\n",
        "from collections import Counter\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Dict, Tuple\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from gensim.models import FastText\n",
        "from tqdm import tqdm\n",
        "from tensorboardX import SummaryWriter\n",
        "\n",
        "# -------------------------\n",
        "# Config - edit these\n",
        "# -------------------------\n",
        "@dataclass\n",
        "class CFG:\n",
        "    data_csv: str = \"tamil_sentiment_full.csv\"\n",
        "    output_dir: str = \"outputs_bilstm\"\n",
        "    fasttext_model_path: str = \"outputs_bilstm/fasttext_tamil.model\"\n",
        "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    min_class_samples: int = 3\n",
        "\n",
        "    # Model\n",
        "    max_length: int = 100\n",
        "    embedding_dim: int = 300\n",
        "    hidden_dim: int = 256\n",
        "    n_layers: int = 2\n",
        "    dropout: float = 0.4\n",
        "\n",
        "    # Training\n",
        "    epochs: int = 10\n",
        "    batch_size: int = 64\n",
        "    lr: float = 1e-3\n",
        "    seed: int = 42\n",
        "\n",
        "    # Loss options\n",
        "    use_focal: bool = True\n",
        "    focal_gamma: float = 2.0  # fixed\n",
        "\n",
        "    # FastText\n",
        "    ft_dim: int = 300\n",
        "    ft_min_count: int = 2\n",
        "    ft_epochs: int = 10\n",
        "\n",
        "cfg = CFG()\n",
        "\n",
        "os.makedirs(cfg.output_dir, exist_ok=True)\n",
        "os.makedirs(\"data\", exist_ok=True)\n",
        "\n",
        "# -------------------------\n",
        "# Utilities & helpers\n",
        "# -------------------------\n",
        "def seed_everything(seed=42):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "\n",
        "seed_everything(cfg.seed)\n",
        "\n",
        "class Vocabulary:\n",
        "    def __init__(self, texts: List[str], min_freq=2):\n",
        "        self.word2idx = {'<pad>': 0, '<unk>': 1}\n",
        "        self.pad_token_id = 0; self.unk_token_id = 1\n",
        "        word_counts = Counter(word for text in texts for word in text.split())\n",
        "        for word, count in word_counts.items():\n",
        "            if count >= min_freq:\n",
        "                self.word2idx[word] = len(self.word2idx)\n",
        "    def __len__(self): return len(self.word2idx)\n",
        "    def text_to_sequence(self, text: str) -> List[int]:\n",
        "        return [self.word2idx.get(word, self.unk_token_id) for word in text.split()]\n",
        "\n",
        "class CommentsDataset(Dataset):\n",
        "    def __init__(self, records: List[Dict], label_map: Dict[str,int], vocab: Vocabulary, max_len=128):\n",
        "        self.vocab = vocab; self.max_len = max_len; self.label_map = label_map\n",
        "        self.samples = [(str(rec[\"text\"]), self.label_map[rec[\"label\"]]) for rec in records]\n",
        "    def __len__(self): return len(self.samples)\n",
        "    def __getitem__(self, idx):\n",
        "        text, label = self.samples[idx]\n",
        "        seq = self.vocab.text_to_sequence(text)\n",
        "        if len(seq) < self.max_len:\n",
        "            seq.extend([self.vocab.pad_token_id] * (self.max_len - len(seq)))\n",
        "        else:\n",
        "            seq = seq[:self.max_len]\n",
        "        return {\"text\": torch.tensor(seq, dtype=torch.long), \"label\": torch.tensor(label, dtype=torch.long)}\n",
        "\n",
        "def train_fasttext(sentences: List[str], save_path: str, dim=300, min_count=2, epochs=10):\n",
        "    if os.path.exists(save_path):\n",
        "        print(f\"‚úÖ FastText model found at {save_path}, loading...\")\n",
        "        return FastText.load(save_path)\n",
        "    print(f\"‚è≥ Training new FastText model for {epochs} epochs... (This can take 5-10 minutes)\")\n",
        "    tokenized = [s.split() for s in sentences]\n",
        "    ft = FastText(sentences=tokenized, vector_size=dim, window=5, min_count=min_count, workers=os.cpu_count(), epochs=epochs)\n",
        "    ft.save(save_path)\n",
        "    print(\"‚úÖ FastText model training complete.\")\n",
        "    return ft\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, hidden_dim):\n",
        "        super(Attention, self).__init__()\n",
        "        self.attn = nn.Linear(hidden_dim, 1, bias=False)\n",
        "    def forward(self, lstm_output):\n",
        "        attn_weights = F.softmax(self.attn(lstm_output), dim=1)\n",
        "        return torch.sum(attn_weights * lstm_output, dim=1)\n",
        "\n",
        "class BiLSTMAttentionClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, n_layers, num_labels, dropout):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim // 2, num_layers=n_layers, bidirectional=True, batch_first=True, dropout=dropout if n_layers > 1 else 0)\n",
        "        self.attention = Attention(hidden_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.fc = nn.Linear(hidden_dim, num_labels)\n",
        "    def init_weights_from_fasttext(self, ft_model, vocab):\n",
        "        print(\"‚è≥ Initializing embedding layer with FastText vectors...\")\n",
        "        weights_matrix = np.random.normal(size=(len(vocab), cfg.embedding_dim)).astype(np.float32)\n",
        "        found_count = 0\n",
        "        for word, i in vocab.word2idx.items():\n",
        "            if word in ft_model.wv:\n",
        "                weights_matrix[i] = ft_model.wv[word]\n",
        "                found_count += 1\n",
        "        self.embedding.weight.data.copy_(torch.from_numpy(weights_matrix))\n",
        "        print(f\"‚úÖ Found {found_count}/{len(vocab)} words in FastText model.\")\n",
        "    def forward(self, text):\n",
        "        embedded = self.embedding(text)\n",
        "        lstm_output, _ = self.lstm(embedded)\n",
        "        context_vector = self.attention(lstm_output)\n",
        "        return self.fc(self.dropout(context_vector)), None\n",
        "\n",
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, gamma=2.0, alpha=None, device='cpu'):\n",
        "        super().__init__()\n",
        "        self.gamma = gamma\n",
        "        self.alpha = torch.tensor(alpha, dtype=torch.float32).to(device) if alpha is not None else None\n",
        "    def forward(self, logits, targets):\n",
        "        ce = F.cross_entropy(logits, targets, reduction='none')\n",
        "        pt = torch.exp(-ce)\n",
        "        loss = ((1 - pt) ** self.gamma) * ce\n",
        "        if self.alpha is not None:\n",
        "            loss = self.alpha[targets] * loss\n",
        "        return loss.mean()\n",
        "\n",
        "# -------------------------\n",
        "# Corrected evaluate() -> returns 4 items\n",
        "# -------------------------\n",
        "def evaluate(model, dataloader, device, id2label):\n",
        "    \"\"\"\n",
        "    Returns:\n",
        "      - report (dict): sklearn classification_report as dict (output_dict=True)\n",
        "      - cm (np.array): confusion matrix\n",
        "      - y_true (list): ground-truth labels\n",
        "      - y_pred (list): predicted labels\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    y_true, y_pred = [], []\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(dataloader, desc=\"Eval\", leave=False):\n",
        "            text = batch['text'].to(device)\n",
        "            labels = batch['label'].cpu().numpy().tolist()\n",
        "            logits, _ = model(text)\n",
        "            preds = torch.argmax(logits, dim=1).cpu().numpy().tolist()\n",
        "            y_true.extend(labels)\n",
        "            y_pred.extend(preds)\n",
        "\n",
        "    report = classification_report(\n",
        "        y_true,\n",
        "        y_pred,\n",
        "        target_names=list(id2label.values()),\n",
        "        digits=4,\n",
        "        output_dict=True,\n",
        "        zero_division=0\n",
        "    )\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    return report, cm, y_true, y_pred\n",
        "\n",
        "# -------------------------\n",
        "# Training loop\n",
        "# -------------------------\n",
        "def train_loop(train_loader, val_loader, model, cfg, class_weights, id2label):\n",
        "    device = cfg.device\n",
        "    model.to(device)\n",
        "    print(f\"‚úÖ All setup complete. Starting training on {device}...\")\n",
        "    time.sleep(1)\n",
        "\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=cfg.lr)\n",
        "    alpha = class_weights / class_weights.sum()\n",
        "    criterion = FocalLoss(gamma=cfg.focal_gamma, alpha=alpha, device=cfg.device)\n",
        "\n",
        "    best_macro = -1.0\n",
        "    for epoch in range(cfg.epochs):\n",
        "        model.train()\n",
        "        pbar = tqdm(train_loader, desc=f\"Train E{epoch+1}/{cfg.epochs}\")\n",
        "        for batch in pbar:\n",
        "            text, labels = batch['text'].to(device), batch['label'].to(device)\n",
        "            optimizer.zero_grad()\n",
        "            logits, _ = model(text)\n",
        "            loss = criterion(logits, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            pbar.set_postfix(loss=loss.item())\n",
        "\n",
        "        report, _, _, _ = evaluate(model, val_loader, device, id2label)\n",
        "        macro_f1 = report['macro avg']['f1-score']\n",
        "        print(f\"Epoch {epoch+1} -> Val Macro F1: {macro_f1:.4f}\")\n",
        "        if macro_f1 > best_macro:\n",
        "            best_macro = macro_f1\n",
        "            torch.save({\"model_state_dict\": model.state_dict()}, os.path.join(cfg.output_dir, \"best_model.pt\"))\n",
        "            print(f\"üöÄ New best model saved with Macro F1: {macro_f1:.4f}\")\n",
        "    return os.path.join(cfg.output_dir, \"best_model.pt\")\n",
        "\n",
        "# -------------------------\n",
        "# Main pipeline\n",
        "# -------------------------\n",
        "def main():\n",
        "    start_time = time.time()\n",
        "    print(\"--- Step 1: Loading and Cleaning Data ---\")\n",
        "    df = pd.read_csv(cfg.data_csv, sep='\\\\t', header=None, names=['label', 'text'], engine='python')\n",
        "    df.dropna(subset=['text', 'label'], inplace=True)\n",
        "    df['text'] = df['text'].astype(str)\n",
        "\n",
        "    label_counts = df['label'].value_counts()\n",
        "    classes_to_keep = label_counts[label_counts >= cfg.min_class_samples].index.tolist()\n",
        "    if len(classes_to_keep) < len(label_counts):\n",
        "        print(f\"Filtering out classes with less than {cfg.min_class_samples} samples.\")\n",
        "        df = df[df['label'].isin(classes_to_keep)]\n",
        "    print(f\"‚úÖ Data loading complete. Kept {len(df)} records.\")\n",
        "\n",
        "    print(\"\\n--- Step 2: Splitting Data and Building Vocabulary ---\")\n",
        "    unique_labels = sorted(df['label'].unique())\n",
        "    label_map = {label: i for i, label in enumerate(unique_labels)}\n",
        "    id2label = {v: k for k, v in label_map.items()}\n",
        "\n",
        "    data = df.to_dict(orient=\"records\")\n",
        "    labels = [label_map[r['label']] for r in data]\n",
        "    train_idx, test_idx = train_test_split(range(len(data)), test_size=0.15, random_state=cfg.seed, stratify=labels)\n",
        "    train_idx, val_idx = train_test_split(train_idx, test_size=0.15, random_state=cfg.seed, stratify=[labels[i] for i in train_idx])\n",
        "    train_records, val_records, test_records = [data[i] for i in train_idx], [data[i] for i in val_idx], [data[i] for i in test_idx]\n",
        "\n",
        "    vocab = Vocabulary([r['text'] for r in train_records], min_freq=cfg.ft_min_count)\n",
        "    print(f\"‚úÖ Vocabulary built with {len(vocab)} unique tokens.\")\n",
        "\n",
        "    print(\"\\n--- Step 3: Training FastText Embeddings ---\")\n",
        "    ft_model = train_fasttext([r['text'] for r in data], cfg.fasttext_model_path, dim=cfg.ft_dim, min_count=cfg.ft_min_count, epochs=cfg.ft_epochs)\n",
        "\n",
        "    print(\"\\n--- Step 4: Building Model and DataLoaders ---\")\n",
        "    model = BiLSTMAttentionClassifier(vocab_size=len(vocab), embedding_dim=cfg.embedding_dim, hidden_dim=cfg.hidden_dim, n_layers=cfg.n_layers, num_labels=len(label_map), dropout=cfg.dropout)\n",
        "    model.init_weights_from_fasttext(ft_model, vocab)\n",
        "\n",
        "    train_ds = CommentsDataset(train_records, label_map, vocab, max_len=cfg.max_length)\n",
        "    val_ds = CommentsDataset(val_records, label_map, vocab, max_len=cfg.max_length)\n",
        "    train_loader = DataLoader(train_ds, batch_size=cfg.batch_size, shuffle=True, num_workers=0)\n",
        "    val_loader = DataLoader(val_ds, batch_size=cfg.batch_size*2, shuffle=False, num_workers=0)\n",
        "\n",
        "    cnt = Counter([label_map[r['label']] for r in train_records])\n",
        "    class_weights = np.array([cnt.get(i, 0) for i in range(len(label_map))])\n",
        "    class_weights = 1.0 / (class_weights + 1e-9)\n",
        "\n",
        "    # --- Step 5: Starting the Training Loop ---\n",
        "    best_ckpt = train_loop(train_loader, val_loader, model, cfg, class_weights=class_weights, id2label=id2label)\n",
        "\n",
        "    # --- Step 6: Final Evaluation ---\n",
        "    print(\"\\n--- Final Test Set Evaluation ---\")\n",
        "    if best_ckpt and os.path.exists(best_ckpt):\n",
        "        ckpt = torch.load(best_ckpt, map_location=cfg.device)\n",
        "        model.load_state_dict(ckpt['model_state_dict'])\n",
        "        test_ds = CommentsDataset(test_records, label_map, vocab, max_len=cfg.max_length)\n",
        "        test_loader = DataLoader(test_ds, batch_size=cfg.batch_size*2, num_workers=0)\n",
        "        report, cm, y_true, y_pred = evaluate(model, test_loader, cfg.device, id2label)\n",
        "\n",
        "        print(\"\\nClassification Report:\\n\")\n",
        "        print(classification_report(y_true, y_pred,target_names=list(id2label.values()),digits=4))\n",
        "\n",
        "        print(\"\\nConfusion Matrix:\\n\", cm)\n",
        "\n",
        "    total_time = time.time() - start_time\n",
        "    print(f\"\\nTotal execution time: {total_time/60:.2f} minutes.\")\n",
        "\n",
        "# --- Run the main function ---\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "CharCNN + FastText + BiLSTM + Attention + Aux features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X710ooN0-az_",
        "outputId": "9f3fa997-2d2b-448f-afc4-b236a12ac4b4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Records: 44019 labels: 5\n",
            "split sizes: 31803 5613 6603\n",
            "Vocab sizes: tokens 21997 chars 657\n",
            "Loading FastText from outputs_char_bilstm/fasttext.model\n",
            "Found 21997/21997 tokens in FastText.\n",
            "embedding dim: 300\n",
            "char_cnn out dim actual: 100\n",
            "logits shape (sanity): torch.Size([2, 5])\n",
            "Model trainable params: 7303301\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train E1/6: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 497/497 [07:03<00:00,  1.17it/s, loss=0.0879]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 -> Val Macro F1: 0.3860\n",
            "Saved outputs_char_bilstm/best_macro_0.3860.pt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train E2/6: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 497/497 [07:08<00:00,  1.16it/s, loss=0.0687]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2 -> Val Macro F1: 0.3646\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train E3/6: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 497/497 [07:08<00:00,  1.16it/s, loss=0.0893]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3 -> Val Macro F1: 0.4286\n",
            "Saved outputs_char_bilstm/best_macro_0.4286.pt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train E4/6: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 497/497 [07:08<00:00,  1.16it/s, loss=0.0952]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4 -> Val Macro F1: 0.4014\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train E5/6: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 497/497 [07:07<00:00,  1.16it/s, loss=0.0822]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5 -> Val Macro F1: 0.4619\n",
            "Saved outputs_char_bilstm/best_macro_0.4619.pt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train E6/6: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 497/497 [07:08<00:00,  1.16it/s, loss=0.0595]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 6 -> Val Macro F1: 0.4359\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Final Test Report:\n",
            "                precision    recall  f1-score   support\n",
            "\n",
            "Mixed_feelings     0.2225    0.2842    0.2496       739\n",
            "      Negative     0.3159    0.5536    0.4022       784\n",
            "      Positive     0.8604    0.5090    0.6396      3731\n",
            "     not-Tamil     0.4488    0.6997    0.5468       313\n",
            " unknown_state     0.3535    0.5425    0.4280      1036\n",
            "\n",
            "      accuracy                         0.5034      6603\n",
            "     macro avg     0.4402    0.5178    0.4532      6603\n",
            "  weighted avg     0.6253    0.5034    0.5302      6603\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            " [[ 210  251  131   19  128]\n",
            " [ 110  434   73   23  144]\n",
            " [ 481  479 1899  169  703]\n",
            " [   7   13   21  219   53]\n",
            " [ 136  197   83   58  562]]\n",
            "Saved test_preds.csv\n",
            "Done.\n"
          ]
        }
      ],
      "source": [
        "# Full corrected script: CharCNN + FastText + BiLSTM + Attention + Aux features\n",
        "# Requirements:\n",
        "# pip install torch scikit-learn gensim pandas tqdm\n",
        "\n",
        "import os, time, random, json\n",
        "from collections import Counter\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Dict\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from gensim.models import FastText\n",
        "from tqdm import tqdm\n",
        "\n",
        "# -------------------------\n",
        "# Config\n",
        "# -------------------------\n",
        "@dataclass\n",
        "class CFG:\n",
        "    data_csv: str = \"tamil_sentiment_full.csv\"   # tab-separated file: label \\t text\n",
        "    output_dir: str = \"outputs_char_bilstm\"\n",
        "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    min_class_samples: int = 3\n",
        "\n",
        "    # vocab / chars\n",
        "    min_token_freq: int = 2\n",
        "    max_chars_per_token: int = 12\n",
        "    min_char_freq: int = 1\n",
        "\n",
        "    # embedding / ft\n",
        "    ft_dim: int = 300\n",
        "    ft_min_count: int = 2\n",
        "    ft_epochs: int = 8\n",
        "    embedding_trainable: bool = True\n",
        "\n",
        "    # model\n",
        "    hidden_dim: int = 256\n",
        "    lstm_layers: int = 1\n",
        "    char_emb_dim: int = 50\n",
        "    char_out: int = 100\n",
        "    attn_dim: int = 128\n",
        "    aux_dim: int = 8      # number of auxiliary features\n",
        "    dropout: float = 0.3\n",
        "\n",
        "    # training\n",
        "    epochs: int = 6\n",
        "    batch_size: int = 64\n",
        "    lr_emb: float = 5e-5\n",
        "    lr_head: float = 1e-3\n",
        "    weight_decay: float = 1e-5\n",
        "    use_sampler: bool = False\n",
        "    use_focal: bool = True\n",
        "    focal_gamma: float = 2.0\n",
        "    seed: int = 42\n",
        "\n",
        "cfg = CFG()\n",
        "os.makedirs(cfg.output_dir, exist_ok=True)\n",
        "\n",
        "# -------------------------\n",
        "# Utilities\n",
        "# -------------------------\n",
        "def seed_everything(seed=42):\n",
        "    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed)\n",
        "seed_everything(cfg.seed)\n",
        "\n",
        "def read_data(path):\n",
        "    # expects tab separated with label \\t text\n",
        "    df = pd.read_csv(path, sep='\\t', header=None, names=['label','text'], engine='python')\n",
        "    df.dropna(subset=['text','label'], inplace=True)\n",
        "    df['text'] = df['text'].astype(str)\n",
        "    return df\n",
        "\n",
        "# -------------------------\n",
        "# Preprocessing / vocabs\n",
        "# -------------------------\n",
        "def build_token_vocab(texts: List[str], min_freq=2):\n",
        "    cnt = Counter()\n",
        "    for t in texts:\n",
        "        for w in t.split():\n",
        "            cnt[w] += 1\n",
        "    word2idx = {'<pad>':0, '<unk>':1}\n",
        "    for w,c in cnt.items():\n",
        "        if c>=min_freq:\n",
        "            word2idx[w] = len(word2idx)\n",
        "    return word2idx\n",
        "\n",
        "def build_char_vocab(texts: List[str], min_freq=1, max_chars=12):\n",
        "    cnt = Counter()\n",
        "    for t in texts:\n",
        "        for tok in t.split():\n",
        "            for ch in list(tok)[:max_chars]:\n",
        "                cnt[ch] += 1\n",
        "    char2idx = {'<pad>':0, '<unk>':1}\n",
        "    for ch,c in cnt.items():\n",
        "        if c>=min_freq:\n",
        "            char2idx[ch] = len(char2idx)\n",
        "    return char2idx\n",
        "\n",
        "def text_to_token_ids(text, word2idx, max_len):\n",
        "    ids = [word2idx.get(w, word2idx['<unk>']) for w in text.split()]\n",
        "    if len(ids) < max_len: ids += [word2idx['<pad>']] * (max_len - len(ids))\n",
        "    else: ids = ids[:max_len]\n",
        "    return ids\n",
        "\n",
        "def text_to_char_ids(text, char2idx, max_len_tokens, max_chars_per_token):\n",
        "    toks = text.split()\n",
        "    char_ids = []\n",
        "    for i in range(max_len_tokens):\n",
        "        if i < len(toks):\n",
        "            tok = toks[i][:max_chars_per_token]\n",
        "            ids = [char2idx.get(ch, char2idx['<unk>']) for ch in tok]\n",
        "            if len(ids) < max_chars_per_token:\n",
        "                ids += [char2idx['<pad>']] * (max_chars_per_token - len(ids))\n",
        "        else:\n",
        "            ids = [char2idx['<pad>']] * max_chars_per_token\n",
        "        char_ids.append(ids)\n",
        "    return char_ids  # shape (max_len_tokens, max_chars_per_token)\n",
        "\n",
        "# Auxiliary features generator (simple, extensible)\n",
        "def compute_aux_features(text):\n",
        "    toks = text.split()\n",
        "    num_tokens = len(toks)\n",
        "    num_chars = len(text)\n",
        "    emoji_count = sum(1 for ch in text if ord(ch) > 10000)  # crude heuristic\n",
        "    punct_count = sum(1 for ch in text if ch in '?!.,;:')\n",
        "    has_english = 1.0 if any('a' <= ch.lower() <= 'z' for ch in text) else 0.0\n",
        "    has_tamil = 1.0 if any('\\u0B80' <= ch <= '\\u0BFF' for ch in text) else 0.0\n",
        "    avg_token_len = (sum(len(t) for t in toks)/num_tokens) if num_tokens>0 else 0.0\n",
        "    cap_ratio = sum(1 for ch in text if ch.isupper()) / (num_chars+1)\n",
        "    return [num_tokens, num_chars, emoji_count, punct_count, has_english, has_tamil, avg_token_len, cap_ratio]\n",
        "\n",
        "# -------------------------\n",
        "# Dataset\n",
        "# -------------------------\n",
        "class CharTokenDataset(Dataset):\n",
        "    def __init__(self, records, label_map, word2idx, char2idx, max_len_tokens, max_chars_per_token, aux_dim):\n",
        "        self.records = records\n",
        "        self.label_map = label_map\n",
        "        self.word2idx = word2idx\n",
        "        self.char2idx = char2idx\n",
        "        self.max_len_tokens = max_len_tokens\n",
        "        self.max_chars_per_token = max_chars_per_token\n",
        "        self.aux_dim = aux_dim\n",
        "\n",
        "        self.samples = []\n",
        "        for r in records:\n",
        "            text = str(r['text'])\n",
        "            label = label_map[r['label']]\n",
        "            token_ids = text_to_token_ids(text, word2idx, max_len_tokens)\n",
        "            char_ids = text_to_char_ids(text, char2idx, max_len_tokens, max_chars_per_token)\n",
        "            aux = compute_aux_features(text)\n",
        "            aux = (aux + [0.0]*aux_dim)[:aux_dim]\n",
        "            self.samples.append((token_ids, char_ids, aux, label))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        token_ids, char_ids, aux, label = self.samples[idx]\n",
        "        token_ids = torch.tensor(token_ids, dtype=torch.long)\n",
        "        char_ids = torch.tensor(char_ids, dtype=torch.long)  # shape (T, C)\n",
        "        aux = torch.tensor(aux, dtype=torch.float32)\n",
        "        label = torch.tensor(label, dtype=torch.long)\n",
        "        return {\"token_ids\": token_ids, \"char_ids\": char_ids, \"aux\": aux, \"label\": label}\n",
        "\n",
        "# -------------------------\n",
        "# FastText training / loading\n",
        "# -------------------------\n",
        "def train_or_load_fasttext(sentences, path, dim=300, min_count=2, epochs=6):\n",
        "    if os.path.exists(path):\n",
        "        print(\"Loading FastText from\", path)\n",
        "        return FastText.load(path)\n",
        "    print(\"Training FastText...\")\n",
        "    tokenized = [s.split() for s in sentences]\n",
        "    ft = FastText(vector_size=dim, window=5, min_count=min_count, workers=os.cpu_count(), epochs=epochs)\n",
        "    ft.build_vocab(tokenized)\n",
        "    ft.train(tokenized, total_examples=len(tokenized), epochs=epochs)\n",
        "    ft.save(path)\n",
        "    print(\"Saved FastText at\", path)\n",
        "    return ft\n",
        "\n",
        "def build_embedding_matrix(word2idx, ft_model, dim):\n",
        "    V = len(word2idx)\n",
        "    mat = np.random.normal(scale=0.01, size=(V, dim)).astype(np.float32)\n",
        "    found = 0\n",
        "    for w,i in word2idx.items():\n",
        "        if w in ft_model.wv:\n",
        "            mat[i] = ft_model.wv[w]\n",
        "            found += 1\n",
        "    print(f\"Found {found}/{V} tokens in FastText.\")\n",
        "    return mat\n",
        "\n",
        "# -------------------------\n",
        "# Model components (robust CharCNN & BiLSTM)\n",
        "# -------------------------\n",
        "class CharCNN(nn.Module):\n",
        "    def __init__(self, char_vocab_size, char_emb_dim=50, out_dim=100, kernel_sizes=(3,4,5), dropout=0.1, max_chars=12):\n",
        "        super().__init__()\n",
        "        self.char_emb = nn.Embedding(char_vocab_size, char_emb_dim, padding_idx=0)\n",
        "        k = len(kernel_sizes)\n",
        "        base = out_dim // k\n",
        "        extras = out_dim - (base * k)\n",
        "        out_channels_list = [base + (1 if i < extras else 0) for i in range(k)]\n",
        "        self.convs = nn.ModuleList([\n",
        "            nn.Conv2d(in_channels=1, out_channels=out_channels_list[i], kernel_size=(kernel_sizes[i], char_emb_dim))\n",
        "            for i in range(k)\n",
        "        ])\n",
        "        self.out_dim_actual = sum(out_channels_list)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.max_chars = max_chars\n",
        "\n",
        "    def forward(self, x_char):\n",
        "        # x_char: (B, T, C)\n",
        "        B,T,C = x_char.size()\n",
        "        x = self.char_emb(x_char)           # (B, T, C, E)\n",
        "        x = x.view(B*T, C, -1).unsqueeze(1) # (B*T, 1, C, E)\n",
        "        conv_outs = []\n",
        "        for conv in self.convs:\n",
        "            o = conv(x)                     # (B*T, out_ch, L, 1)\n",
        "            o = F.relu(o.squeeze(-1))       # (B*T, out_ch, L)\n",
        "            o = F.max_pool1d(o, o.size(2)).squeeze(2)  # (B*T, out_ch)\n",
        "            conv_outs.append(o)\n",
        "        out = torch.cat(conv_outs, dim=1)   # (B*T, out_dim_actual)\n",
        "        out = out.view(B, T, -1)            # (B, T, out_dim_actual)\n",
        "        out = self.dropout(out)\n",
        "        return out\n",
        "\n",
        "class BiLSTMCharFastText(nn.Module):\n",
        "    def __init__(self, emb_matrix, char_vocab_size, cfg, num_labels):\n",
        "        super().__init__()\n",
        "        emb_matrix = torch.tensor(emb_matrix)\n",
        "        V, E = emb_matrix.shape\n",
        "        self.embedding = nn.Embedding.from_pretrained(emb_matrix, freeze=not cfg.embedding_trainable, padding_idx=0)\n",
        "        self.char_cnn = CharCNN(char_vocab_size, char_emb_dim=cfg.char_emb_dim, out_dim=cfg.char_out, dropout=cfg.dropout, max_chars=cfg.max_chars_per_token)\n",
        "        token_in_dim = E + self.char_cnn.out_dim_actual\n",
        "        self.bilstm = nn.LSTM(token_in_dim, cfg.hidden_dim//2, num_layers=cfg.lstm_layers, bidirectional=True, batch_first=True, dropout=cfg.dropout if cfg.lstm_layers>1 else 0)\n",
        "        self.attn_proj = nn.Linear(cfg.hidden_dim, cfg.attn_dim)\n",
        "        self.attn_v = nn.Linear(cfg.attn_dim, 1, bias=False)\n",
        "        self.aux_proj = nn.Linear(cfg.aux_dim, 32)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(cfg.hidden_dim + 32, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(cfg.dropout),\n",
        "            nn.Linear(256, num_labels)\n",
        "        )\n",
        "\n",
        "    def forward(self, token_ids, char_ids, aux):\n",
        "        emb = self.embedding(token_ids)                   # (B, T, E)\n",
        "        char_vec = self.char_cnn(char_ids)                # (B, T, char_out_actual)\n",
        "        x = torch.cat([emb, char_vec], dim=-1)            # (B, T, E + char_out_actual)\n",
        "        h, _ = self.bilstm(x)                             # (B, T, H)\n",
        "        a = torch.tanh(self.attn_proj(h))                 # (B, T, attn_dim)\n",
        "        scores = self.attn_v(a).squeeze(-1)               # (B, T)\n",
        "        mask = (token_ids != 0).float()                   # pad mask\n",
        "        scores = scores.masked_fill(mask==0, -1e9)\n",
        "        alpha = torch.softmax(scores, dim=1).unsqueeze(-1) # (B, T, 1)\n",
        "        pooled = (h * alpha).sum(dim=1)                   # (B, H)\n",
        "        aux_p = torch.relu(self.aux_proj(aux))            # (B, 32)\n",
        "        cat = torch.cat([pooled, aux_p], dim=1)           # (B, H+32)\n",
        "        logits = self.classifier(cat)                     # (B, num_labels)\n",
        "        return logits\n",
        "\n",
        "# -------------------------\n",
        "# Loss: Focal\n",
        "# -------------------------\n",
        "# Replace your current FocalLoss class with this version\n",
        "\n",
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, gamma=2.0, alpha=None):\n",
        "        super().__init__()\n",
        "        self.gamma = gamma\n",
        "        # store alpha as tensor on CPU for now; will move in forward\n",
        "        if alpha is not None:\n",
        "            self.alpha = torch.tensor(alpha, dtype=torch.float32)\n",
        "        else:\n",
        "            self.alpha = None\n",
        "\n",
        "    def forward(self, logits, targets):\n",
        "        \"\"\"\n",
        "        logits: (B, C)\n",
        "        targets: (B,) long, on some device (cpu/cuda)\n",
        "        \"\"\"\n",
        "        ce = F.cross_entropy(logits, targets, reduction='none')  # (B,)\n",
        "        pt = torch.exp(-ce)\n",
        "        loss = ((1 - pt) ** self.gamma) * ce\n",
        "\n",
        "        if self.alpha is not None:\n",
        "            # ensure alpha is on same device as targets before indexing\n",
        "            if self.alpha.device != targets.device:\n",
        "                self.alpha = self.alpha.to(targets.device)\n",
        "            loss = self.alpha[targets] * loss\n",
        "\n",
        "        return loss.mean()\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# Evaluate (returns y_true, y_pred)\n",
        "# -------------------------\n",
        "def evaluate(model, dataloader, device, id2label):\n",
        "    model.eval()\n",
        "    y_true, y_pred = [], []\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(dataloader, desc=\"Eval\", leave=False):\n",
        "            tokens = batch['token_ids'].to(device)\n",
        "            chars  = batch['char_ids'].to(device)\n",
        "            aux    = batch['aux'].to(device)\n",
        "            labels = batch['label'].cpu().numpy().tolist()\n",
        "            logits = model(tokens, chars, aux)\n",
        "            preds = torch.argmax(logits, dim=1).cpu().numpy().tolist()\n",
        "            y_true.extend(labels); y_pred.extend(preds)\n",
        "    report = classification_report(y_true, y_pred, target_names=list(id2label.values()), digits=4, output_dict=True, zero_division=0)\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    return report, cm, y_true, y_pred\n",
        "\n",
        "# -------------------------\n",
        "# Training loop\n",
        "# -------------------------\n",
        "def train(train_loader, val_loader, model, cfg, class_weights, id2label):\n",
        "    device = cfg.device\n",
        "    model.to(device)\n",
        "    emb_params = list(model.embedding.parameters())\n",
        "    other_params = [p for n,p in model.named_parameters() if not n.startswith('embedding.')]\n",
        "    optimizer = torch.optim.AdamW([\n",
        "        {\"params\": emb_params, \"lr\": cfg.lr_emb},\n",
        "        {\"params\": other_params, \"lr\": cfg.lr_head}\n",
        "    ], weight_decay=cfg.weight_decay)\n",
        "\n",
        "    if cfg.use_focal:\n",
        "        alpha = (1.0 / (class_weights + 1e-9))\n",
        "        alpha = alpha / alpha.sum()\n",
        "        criterion = FocalLoss(cfg.focal_gamma, alpha=alpha)\n",
        "    else:\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    best_ckpt = None\n",
        "    best_macro = -1.0\n",
        "    for epoch in range(cfg.epochs):\n",
        "        model.train()\n",
        "        pbar = tqdm(train_loader, desc=f\"Train E{epoch+1}/{cfg.epochs}\")\n",
        "        for batch in pbar:\n",
        "            tokens = batch['token_ids'].to(device)\n",
        "            chars  = batch['char_ids'].to(device)\n",
        "            aux    = batch['aux'].to(device)\n",
        "            labels = batch['label'].to(device)\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(tokens, chars, aux)\n",
        "            loss = criterion(logits, labels)\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "            pbar.set_postfix(loss=loss.item())\n",
        "        report, cm, _, _ = evaluate(model, val_loader, cfg.device, id2label)\n",
        "        macro_f1 = report['macro avg']['f1-score']\n",
        "        print(f\"Epoch {epoch+1} -> Val Macro F1: {macro_f1:.4f}\")\n",
        "        if macro_f1 > best_macro:\n",
        "            best_macro = macro_f1\n",
        "            best_ckpt = os.path.join(cfg.output_dir, f\"best_macro_{macro_f1:.4f}.pt\")\n",
        "            torch.save({\"model_state_dict\": model.state_dict(), \"cfg\": cfg.__dict__}, best_ckpt)\n",
        "            print(\"Saved\", best_ckpt)\n",
        "    return best_ckpt\n",
        "\n",
        "# -------------------------\n",
        "# Pipeline orchestration\n",
        "# -------------------------\n",
        "def main():\n",
        "    # load\n",
        "    df = read_data(cfg.data_csv)\n",
        "    # filter tiny classes\n",
        "    cnt = df['label'].value_counts()\n",
        "    keep = cnt[cnt >= cfg.min_class_samples].index.tolist()\n",
        "    if len(keep) < len(cnt):\n",
        "        df = df[df['label'].isin(keep)].reset_index(drop=True)\n",
        "    print(\"Records:\", len(df), \"labels:\", df['label'].nunique())\n",
        "\n",
        "    # label mapping\n",
        "    labels_unique = sorted(df['label'].unique())\n",
        "    label_map = {lab:i for i,lab in enumerate(labels_unique)}\n",
        "    id2label = {v:k for k,v in label_map.items()}\n",
        "\n",
        "    # splits\n",
        "    data = df.to_dict(orient='records')\n",
        "    lablist = [label_map[r['label']] for r in data]\n",
        "    train_idx, test_idx = train_test_split(range(len(data)), test_size=0.15, random_state=cfg.seed, stratify=lablist)\n",
        "    train_idx, val_idx = train_test_split(train_idx, test_size=0.15, random_state=cfg.seed, stratify=[lablist[i] for i in train_idx])\n",
        "    train_records = [data[i] for i in train_idx]\n",
        "    val_records   = [data[i] for i in val_idx]\n",
        "    test_records  = [data[i] for i in test_idx]\n",
        "    print(\"split sizes:\", len(train_records), len(val_records), len(test_records))\n",
        "\n",
        "    # vocabs\n",
        "    word2idx = build_token_vocab([r['text'] for r in train_records], cfg.min_token_freq)\n",
        "    char2idx = build_char_vocab([r['text'] for r in train_records], cfg.min_char_freq, cfg.max_chars_per_token)\n",
        "    print(\"Vocab sizes: tokens\", len(word2idx), \"chars\", len(char2idx))\n",
        "\n",
        "    # fasttext\n",
        "    ft_path = os.path.join(cfg.output_dir, \"fasttext.model\")\n",
        "    ft = train_or_load_fasttext([r['text'] for r in data], ft_path, dim=cfg.ft_dim, min_count=cfg.ft_min_count, epochs=cfg.ft_epochs)\n",
        "    emb_matrix = build_embedding_matrix(word2idx, ft, cfg.ft_dim)\n",
        "\n",
        "    # datasets\n",
        "    max_len = 64  # token length cap - tune as needed\n",
        "    train_ds = CharTokenDataset(train_records, label_map, word2idx, char2idx, max_len, cfg.max_chars_per_token, cfg.aux_dim)\n",
        "    val_ds   = CharTokenDataset(val_records, label_map, word2idx, char2idx, max_len, cfg.max_chars_per_token, cfg.aux_dim)\n",
        "    test_ds  = CharTokenDataset(test_records, label_map, word2idx, char2idx, max_len, cfg.max_chars_per_token, cfg.aux_dim)\n",
        "\n",
        "    if cfg.use_sampler:\n",
        "        labels = [s[3] for s in train_ds.samples]\n",
        "        cnts = Counter(labels)\n",
        "        sample_weights = [1.0 / cnts[l] for l in labels]\n",
        "        sampler = WeightedRandomSampler(sample_weights, num_samples=len(sample_weights), replacement=True)\n",
        "        train_loader = DataLoader(train_ds, batch_size=cfg.batch_size, sampler=sampler, num_workers=2)\n",
        "    else:\n",
        "        train_loader = DataLoader(train_ds, batch_size=cfg.batch_size, shuffle=True, num_workers=2)\n",
        "\n",
        "    val_loader = DataLoader(val_ds, batch_size=cfg.batch_size*2, shuffle=False, num_workers=2)\n",
        "    test_loader = DataLoader(test_ds, batch_size=cfg.batch_size*2, shuffle=False, num_workers=2)\n",
        "\n",
        "    # class weight vector (counts)\n",
        "    train_labels = [s[3] for s in train_ds.samples]\n",
        "    cnts = np.array([Counter(train_labels).get(i,0) for i in range(len(label_map))])\n",
        "    class_weights = cnts.astype(np.float32)\n",
        "\n",
        "    # model\n",
        "    model = BiLSTMCharFastText(emb_matrix, char_vocab_size=len(char2idx), cfg=cfg, num_labels=len(label_map))\n",
        "    # quick shape sanity check\n",
        "    print(\"embedding dim:\", emb_matrix.shape[1])\n",
        "    print(\"char_cnn out dim actual:\", model.char_cnn.out_dim_actual)\n",
        "    sample_token_ids = torch.zeros((2, 8), dtype=torch.long)\n",
        "    sample_char_ids = torch.zeros((2, 8, cfg.max_chars_per_token), dtype=torch.long)\n",
        "    sample_aux = torch.zeros((2, cfg.aux_dim), dtype=torch.float)\n",
        "    with torch.no_grad():\n",
        "        logits_shape = model(sample_token_ids, sample_char_ids, sample_aux).shape\n",
        "    print(\"logits shape (sanity):\", logits_shape)\n",
        "    print(\"Model trainable params:\", sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
        "\n",
        "    # train\n",
        "    best_ckpt = train(train_loader, val_loader, model, cfg, class_weights, id2label)\n",
        "\n",
        "    # test eval\n",
        "    if best_ckpt and os.path.exists(best_ckpt):\n",
        "        ckpt = torch.load(best_ckpt, map_location=cfg.device)\n",
        "        model.load_state_dict(ckpt['model_state_dict'])\n",
        "    report, cm, y_true, y_pred = evaluate(model, test_loader, cfg.device, id2label)\n",
        "    print(\"\\nFinal Test Report:\")\n",
        "    print(classification_report(y_true, y_pred, target_names=list(id2label.values()), digits=4))\n",
        "    print(\"\\nConfusion Matrix:\\n\", cm)\n",
        "\n",
        "    # save predictions csv for inspection\n",
        "    rows = []\n",
        "    for rec, yt, yp in zip(test_records, y_true, y_pred):\n",
        "        rows.append({\"text\": rec['text'], \"label\": id2label[yt], \"pred\": id2label[yp]})\n",
        "    pd.DataFrame(rows).to_csv(os.path.join(cfg.output_dir, \"test_preds.csv\"), index=False)\n",
        "    print(\"Saved test_preds.csv\")\n",
        "    print(\"Done.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "CharCNN + FastText + BiLSTM + Attention + Aux features + TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Y4KP5JwBc7f",
        "outputId": "aaaa85fc-c968-4155-f814-4e246bd62d99"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Records: 44019 labels: 5\n",
            "split sizes: 31803 5613 6603\n",
            "Computing TF-IDF features...\n",
            "TF-IDF matrix shape (train): (31803, 5000)\n",
            "Vocab sizes: tokens 21997 chars 657\n",
            "Training FastText...\n",
            "Saved FastText at outputs_char_bilstm_tfidf/fasttext.model\n",
            "Found 21997/21997 tokens in FastText.\n",
            "embedding dim: 300\n",
            "char_cnn out dim actual: 100\n",
            "logits shape (sanity): torch.Size([2, 5])\n",
            "Model trainable params: 7639749\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train E1/6: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 497/497 [07:02<00:00,  1.17it/s, loss=0.0734]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 -> Val Macro F1: 0.4474\n",
            "Saved outputs_char_bilstm_tfidf/best_macro_0.4474.pt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train E2/6: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 497/497 [07:09<00:00,  1.16it/s, loss=0.0515]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2 -> Val Macro F1: 0.4585\n",
            "Saved outputs_char_bilstm_tfidf/best_macro_0.4585.pt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train E3/6: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 497/497 [07:09<00:00,  1.16it/s, loss=0.0418]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3 -> Val Macro F1: 0.4445\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train E4/6: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 497/497 [07:09<00:00,  1.16it/s, loss=0.0526]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4 -> Val Macro F1: 0.4382\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train E5/6: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 497/497 [07:09<00:00,  1.16it/s, loss=0.0431]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5 -> Val Macro F1: 0.4228\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train E6/6: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 497/497 [07:08<00:00,  1.16it/s, loss=0.0288]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 6 -> Val Macro F1: 0.4244\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Final Test Report:\n",
            "                precision    recall  f1-score   support\n",
            "\n",
            "Mixed_feelings     0.2392    0.3288    0.2769       739\n",
            "      Negative     0.3090    0.6199    0.4124       784\n",
            "      Positive     0.8576    0.5615    0.6787      3731\n",
            "     not-Tamil     0.3902    0.7668    0.5172       313\n",
            " unknown_state     0.4372    0.4035    0.4197      1036\n",
            "\n",
            "      accuracy                         0.5273      6603\n",
            "     macro avg     0.4466    0.5361    0.4610      6603\n",
            "  weighted avg     0.6351    0.5273    0.5538      6603\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            " [[ 243  275  130   22   69]\n",
            " [ 118  486   76   23   81]\n",
            " [ 503  557 2095  213  363]\n",
            " [   7   15   26  240   25]\n",
            " [ 145  240  116  117  418]]\n",
            "Saved test_preds.csv\n",
            "Done.\n"
          ]
        }
      ],
      "source": [
        "# Full corrected script: CharCNN + FastText + BiLSTM + Attention + Aux features + TF-IDF\n",
        "# Requirements:\n",
        "# pip install torch scikit-learn gensim pandas tqdm\n",
        "\n",
        "import os, time, random, json\n",
        "from collections import Counter\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Dict\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer ### NEW ###\n",
        "from gensim.models import FastText\n",
        "from tqdm import tqdm\n",
        "\n",
        "# -------------------------\n",
        "# Config\n",
        "# -------------------------\n",
        "@dataclass\n",
        "class CFG:\n",
        "    data_csv: str = \"tamil_sentiment_full.csv\"  # tab-separated file: label \\t text\n",
        "    output_dir: str = \"outputs_char_bilstm_tfidf\"\n",
        "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    min_class_samples: int = 3\n",
        "\n",
        "    # vocab / chars\n",
        "    min_token_freq: int = 2\n",
        "    max_chars_per_token: int = 12\n",
        "    min_char_freq: int = 1\n",
        "\n",
        "    # embedding / ft\n",
        "    ft_dim: int = 300\n",
        "    ft_min_count: int = 2\n",
        "    ft_epochs: int = 8\n",
        "    embedding_trainable: bool = True\n",
        "\n",
        "    # model\n",
        "    hidden_dim: int = 256\n",
        "    lstm_layers: int = 1\n",
        "    char_emb_dim: int = 50\n",
        "    char_out: int = 100\n",
        "    attn_dim: int = 128\n",
        "    aux_dim: int = 8      # number of auxiliary features\n",
        "    tfidf_dim: int = 5000 ### NEW ###: Max features for TF-IDF\n",
        "    tfidf_proj_dim: int = 64 ### NEW ###: Dimension to project TF-IDF features to\n",
        "    dropout: float = 0.3\n",
        "\n",
        "    # training\n",
        "    epochs: int = 6\n",
        "    batch_size: int = 64\n",
        "    lr_emb: float = 5e-5\n",
        "    lr_head: float = 1e-3\n",
        "    weight_decay: float = 1e-5\n",
        "    use_sampler: bool = False\n",
        "    use_focal: bool = True\n",
        "    focal_gamma: float = 2.0\n",
        "    seed: int = 42\n",
        "\n",
        "cfg = CFG()\n",
        "os.makedirs(cfg.output_dir, exist_ok=True)\n",
        "\n",
        "# -------------------------\n",
        "# Utilities\n",
        "# -------------------------\n",
        "def seed_everything(seed=42):\n",
        "    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed)\n",
        "seed_everything(cfg.seed)\n",
        "\n",
        "def read_data(path):\n",
        "    # expects tab separated with label \\t text\n",
        "    df = pd.read_csv(path, sep='\\t', header=None, names=['label','text'], engine='python')\n",
        "    df.dropna(subset=['text','label'], inplace=True)\n",
        "    df['text'] = df['text'].astype(str)\n",
        "    return df\n",
        "\n",
        "# -------------------------\n",
        "# Preprocessing / vocabs\n",
        "# -------------------------\n",
        "def build_token_vocab(texts: List[str], min_freq=2):\n",
        "    cnt = Counter()\n",
        "    for t in texts:\n",
        "        for w in t.split():\n",
        "            cnt[w] += 1\n",
        "    word2idx = {'<pad>':0, '<unk>':1}\n",
        "    for w,c in cnt.items():\n",
        "        if c>=min_freq:\n",
        "            word2idx[w] = len(word2idx)\n",
        "    return word2idx\n",
        "\n",
        "def build_char_vocab(texts: List[str], min_freq=1, max_chars=12):\n",
        "    cnt = Counter()\n",
        "    for t in texts:\n",
        "        for tok in t.split():\n",
        "            for ch in list(tok)[:max_chars]:\n",
        "                cnt[ch] += 1\n",
        "    char2idx = {'<pad>':0, '<unk>':1}\n",
        "    for ch,c in cnt.items():\n",
        "        if c>=min_freq:\n",
        "            char2idx[ch] = len(char2idx)\n",
        "    return char2idx\n",
        "\n",
        "def text_to_token_ids(text, word2idx, max_len):\n",
        "    ids = [word2idx.get(w, word2idx['<unk>']) for w in text.split()]\n",
        "    if len(ids) < max_len: ids += [word2idx['<pad>']] * (max_len - len(ids))\n",
        "    else: ids = ids[:max_len]\n",
        "    return ids\n",
        "\n",
        "def text_to_char_ids(text, char2idx, max_len_tokens, max_chars_per_token):\n",
        "    toks = text.split()\n",
        "    char_ids = []\n",
        "    for i in range(max_len_tokens):\n",
        "        if i < len(toks):\n",
        "            tok = toks[i][:max_chars_per_token]\n",
        "            ids = [char2idx.get(ch, char2idx['<unk>']) for ch in tok]\n",
        "            if len(ids) < max_chars_per_token:\n",
        "                ids += [char2idx['<pad>']] * (max_chars_per_token - len(ids))\n",
        "        else:\n",
        "            ids = [char2idx['<pad>']] * max_chars_per_token\n",
        "        char_ids.append(ids)\n",
        "    return char_ids  # shape (max_len_tokens, max_chars_per_token)\n",
        "\n",
        "# Auxiliary features generator (simple, extensible)\n",
        "def compute_aux_features(text):\n",
        "    toks = text.split()\n",
        "    num_tokens = len(toks)\n",
        "    num_chars = len(text)\n",
        "    emoji_count = sum(1 for ch in text if ord(ch) > 10000)  # crude heuristic\n",
        "    punct_count = sum(1 for ch in text if ch in '?!.,;:')\n",
        "    has_english = 1.0 if any('a' <= ch.lower() <= 'z' for ch in text) else 0.0\n",
        "    has_tamil = 1.0 if any('\\u0B80' <= ch <= '\\u0BFF' for ch in text) else 0.0\n",
        "    avg_token_len = (sum(len(t) for t in toks)/num_tokens) if num_tokens>0 else 0.0\n",
        "    cap_ratio = sum(1 for ch in text if ch.isupper()) / (num_chars+1)\n",
        "    return [num_tokens, num_chars, emoji_count, punct_count, has_english, has_tamil, avg_token_len, cap_ratio]\n",
        "\n",
        "# -------------------------\n",
        "# Dataset\n",
        "# -------------------------\n",
        "class CharTokenDataset(Dataset): ### MODIFIED to include TF-IDF ###\n",
        "    def __init__(self, records, tfidf_matrix, label_map, word2idx, char2idx, max_len_tokens, max_chars_per_token, aux_dim):\n",
        "        self.records = records\n",
        "        self.tfidf_matrix = tfidf_matrix\n",
        "        self.label_map = label_map\n",
        "        self.word2idx = word2idx\n",
        "        self.char2idx = char2idx\n",
        "        self.max_len_tokens = max_len_tokens\n",
        "        self.max_chars_per_token = max_chars_per_token\n",
        "        self.aux_dim = aux_dim\n",
        "\n",
        "        self.samples = []\n",
        "        for i, r in enumerate(records):\n",
        "            text = str(r['text'])\n",
        "            label = label_map[r['label']]\n",
        "            token_ids = text_to_token_ids(text, word2idx, max_len_tokens)\n",
        "            char_ids = text_to_char_ids(text, char2idx, max_len_tokens, max_chars_per_token)\n",
        "            aux = compute_aux_features(text)\n",
        "            aux = (aux + [0.0]*aux_dim)[:aux_dim]\n",
        "            tfidf_vec = self.tfidf_matrix[i].toarray().squeeze()\n",
        "            self.samples.append((token_ids, char_ids, aux, tfidf_vec, label))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        token_ids, char_ids, aux, tfidf, label = self.samples[idx]\n",
        "        token_ids = torch.tensor(token_ids, dtype=torch.long)\n",
        "        char_ids = torch.tensor(char_ids, dtype=torch.long)\n",
        "        aux = torch.tensor(aux, dtype=torch.float32)\n",
        "        tfidf = torch.tensor(tfidf, dtype=torch.float32)\n",
        "        label = torch.tensor(label, dtype=torch.long)\n",
        "        return {\"token_ids\": token_ids, \"char_ids\": char_ids, \"aux\": aux, \"tfidf\": tfidf, \"label\": label}\n",
        "\n",
        "# -------------------------\n",
        "# FastText training / loading\n",
        "# -------------------------\n",
        "def train_or_load_fasttext(sentences, path, dim=300, min_count=2, epochs=6):\n",
        "    if os.path.exists(path):\n",
        "        print(\"Loading FastText from\", path)\n",
        "        return FastText.load(path)\n",
        "    print(\"Training FastText...\")\n",
        "    tokenized = [s.split() for s in sentences]\n",
        "    ft = FastText(vector_size=dim, window=5, min_count=min_count, workers=os.cpu_count(), epochs=epochs)\n",
        "    ft.build_vocab(tokenized)\n",
        "    ft.train(tokenized, total_examples=len(tokenized), epochs=epochs)\n",
        "    ft.save(path)\n",
        "    print(\"Saved FastText at\", path)\n",
        "    return ft\n",
        "\n",
        "def build_embedding_matrix(word2idx, ft_model, dim):\n",
        "    V = len(word2idx)\n",
        "    mat = np.random.normal(scale=0.01, size=(V, dim)).astype(np.float32)\n",
        "    found = 0\n",
        "    for w,i in word2idx.items():\n",
        "        if w in ft_model.wv:\n",
        "            mat[i] = ft_model.wv[w]\n",
        "            found += 1\n",
        "    print(f\"Found {found}/{V} tokens in FastText.\")\n",
        "    return mat\n",
        "\n",
        "# -------------------------\n",
        "# Model components (robust CharCNN & BiLSTM)\n",
        "# -------------------------\n",
        "class CharCNN(nn.Module):\n",
        "    def __init__(self, char_vocab_size, char_emb_dim=50, out_dim=100, kernel_sizes=(3,4,5), dropout=0.1, max_chars=12):\n",
        "        super().__init__()\n",
        "        self.char_emb = nn.Embedding(char_vocab_size, char_emb_dim, padding_idx=0)\n",
        "        k = len(kernel_sizes)\n",
        "        base = out_dim // k\n",
        "        extras = out_dim - (base * k)\n",
        "        out_channels_list = [base + (1 if i < extras else 0) for i in range(k)]\n",
        "        self.convs = nn.ModuleList([\n",
        "            nn.Conv2d(in_channels=1, out_channels=out_channels_list[i], kernel_size=(kernel_sizes[i], char_emb_dim))\n",
        "            for i in range(k)\n",
        "        ])\n",
        "        self.out_dim_actual = sum(out_channels_list)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.max_chars = max_chars\n",
        "\n",
        "    def forward(self, x_char):\n",
        "        # x_char: (B, T, C)\n",
        "        B,T,C = x_char.size()\n",
        "        x = self.char_emb(x_char)            # (B, T, C, E)\n",
        "        x = x.view(B*T, C, -1).unsqueeze(1) # (B*T, 1, C, E)\n",
        "        conv_outs = []\n",
        "        for conv in self.convs:\n",
        "            o = conv(x)                      # (B*T, out_ch, L, 1)\n",
        "            o = F.relu(o.squeeze(-1))      # (B*T, out_ch, L)\n",
        "            o = F.max_pool1d(o, o.size(2)).squeeze(2)  # (B*T, out_ch)\n",
        "            conv_outs.append(o)\n",
        "        out = torch.cat(conv_outs, dim=1)  # (B*T, out_dim_actual)\n",
        "        out = out.view(B, T, -1)           # (B, T, out_dim_actual)\n",
        "        out = self.dropout(out)\n",
        "        return out\n",
        "\n",
        "class BiLSTMCharFastText(nn.Module): ### MODIFIED to include TF-IDF ###\n",
        "    def __init__(self, emb_matrix, char_vocab_size, cfg, num_labels):\n",
        "        super().__init__()\n",
        "        emb_matrix = torch.tensor(emb_matrix)\n",
        "        V, E = emb_matrix.shape\n",
        "        self.embedding = nn.Embedding.from_pretrained(emb_matrix, freeze=not cfg.embedding_trainable, padding_idx=0)\n",
        "        self.char_cnn = CharCNN(char_vocab_size, char_emb_dim=cfg.char_emb_dim, out_dim=cfg.char_out, dropout=cfg.dropout, max_chars=cfg.max_chars_per_token)\n",
        "        token_in_dim = E + self.char_cnn.out_dim_actual\n",
        "        self.bilstm = nn.LSTM(token_in_dim, cfg.hidden_dim//2, num_layers=cfg.lstm_layers, bidirectional=True, batch_first=True, dropout=cfg.dropout if cfg.lstm_layers>1 else 0)\n",
        "        self.attn_proj = nn.Linear(cfg.hidden_dim, cfg.attn_dim)\n",
        "        self.attn_v = nn.Linear(cfg.attn_dim, 1, bias=False)\n",
        "        self.aux_proj = nn.Linear(cfg.aux_dim, 32)\n",
        "        self.tfidf_proj = nn.Linear(cfg.tfidf_dim, cfg.tfidf_proj_dim) ### NEW ###\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(cfg.hidden_dim + 32 + cfg.tfidf_proj_dim, 256), ### MODIFIED ###\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(cfg.dropout),\n",
        "            nn.Linear(256, num_labels)\n",
        "        )\n",
        "\n",
        "    def forward(self, token_ids, char_ids, aux, tfidf): ### MODIFIED ###\n",
        "        emb = self.embedding(token_ids)              # (B, T, E)\n",
        "        char_vec = self.char_cnn(char_ids)           # (B, T, char_out_actual)\n",
        "        x = torch.cat([emb, char_vec], dim=-1)       # (B, T, E + char_out_actual)\n",
        "        h, _ = self.bilstm(x)                        # (B, T, H)\n",
        "        a = torch.tanh(self.attn_proj(h))            # (B, T, attn_dim)\n",
        "        scores = self.attn_v(a).squeeze(-1)          # (B, T)\n",
        "        mask = (token_ids != 0).float()              # pad mask\n",
        "        scores = scores.masked_fill(mask==0, -1e9)\n",
        "        alpha = torch.softmax(scores, dim=1).unsqueeze(-1) # (B, T, 1)\n",
        "        pooled = (h * alpha).sum(dim=1)              # (B, H)\n",
        "        aux_p = torch.relu(self.aux_proj(aux))       # (B, 32)\n",
        "        tfidf_p = torch.relu(self.tfidf_proj(tfidf)) ### NEW ### (B, tfidf_proj_dim)\n",
        "        cat = torch.cat([pooled, aux_p, tfidf_p], dim=1)  ### MODIFIED ### (B, H + 32 + tfidf_proj_dim)\n",
        "        logits = self.classifier(cat)                # (B, num_labels)\n",
        "        return logits\n",
        "\n",
        "# -------------------------\n",
        "# Loss: Focal\n",
        "# -------------------------\n",
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, gamma=2.0, alpha=None):\n",
        "        super().__init__()\n",
        "        self.gamma = gamma\n",
        "        if alpha is not None:\n",
        "            self.alpha = torch.tensor(alpha, dtype=torch.float32)\n",
        "        else:\n",
        "            self.alpha = None\n",
        "\n",
        "    def forward(self, logits, targets):\n",
        "        ce = F.cross_entropy(logits, targets, reduction='none')\n",
        "        pt = torch.exp(-ce)\n",
        "        loss = ((1 - pt) ** self.gamma) * ce\n",
        "        if self.alpha is not None:\n",
        "            if self.alpha.device != targets.device:\n",
        "                self.alpha = self.alpha.to(targets.device)\n",
        "            loss = self.alpha[targets] * loss\n",
        "        return loss.mean()\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# Evaluate (returns y_true, y_pred)\n",
        "# -------------------------\n",
        "def evaluate(model, dataloader, device, id2label): ### MODIFIED to include TF-IDF ###\n",
        "    model.eval()\n",
        "    y_true, y_pred = [], []\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(dataloader, desc=\"Eval\", leave=False):\n",
        "            tokens = batch['token_ids'].to(device)\n",
        "            chars  = batch['char_ids'].to(device)\n",
        "            aux    = batch['aux'].to(device)\n",
        "            tfidf  = batch['tfidf'].to(device)\n",
        "            labels = batch['label'].cpu().numpy().tolist()\n",
        "            logits = model(tokens, chars, aux, tfidf)\n",
        "            preds = torch.argmax(logits, dim=1).cpu().numpy().tolist()\n",
        "            y_true.extend(labels); y_pred.extend(preds)\n",
        "    report = classification_report(y_true, y_pred, target_names=list(id2label.values()), digits=4, output_dict=True, zero_division=0)\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    return report, cm, y_true, y_pred\n",
        "\n",
        "# -------------------------\n",
        "# Training loop\n",
        "# -------------------------\n",
        "def train(train_loader, val_loader, model, cfg, class_weights, id2label): ### MODIFIED to include TF-IDF ###\n",
        "    device = cfg.device\n",
        "    model.to(device)\n",
        "    emb_params = list(model.embedding.parameters())\n",
        "    other_params = [p for n,p in model.named_parameters() if not n.startswith('embedding.')]\n",
        "    optimizer = torch.optim.AdamW([\n",
        "        {\"params\": emb_params, \"lr\": cfg.lr_emb},\n",
        "        {\"params\": other_params, \"lr\": cfg.lr_head}\n",
        "    ], weight_decay=cfg.weight_decay)\n",
        "\n",
        "    if cfg.use_focal:\n",
        "        alpha = (1.0 / (class_weights + 1e-9))\n",
        "        alpha = alpha / alpha.sum()\n",
        "        criterion = FocalLoss(cfg.focal_gamma, alpha=alpha)\n",
        "    else:\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    best_ckpt = None\n",
        "    best_macro = -1.0\n",
        "    for epoch in range(cfg.epochs):\n",
        "        model.train()\n",
        "        pbar = tqdm(train_loader, desc=f\"Train E{epoch+1}/{cfg.epochs}\")\n",
        "        for batch in pbar:\n",
        "            tokens = batch['token_ids'].to(device)\n",
        "            chars  = batch['char_ids'].to(device)\n",
        "            aux    = batch['aux'].to(device)\n",
        "            tfidf  = batch['tfidf'].to(device)\n",
        "            labels = batch['label'].to(device)\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(tokens, chars, aux, tfidf)\n",
        "            loss = criterion(logits, labels)\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "            pbar.set_postfix(loss=loss.item())\n",
        "        report, cm, _, _ = evaluate(model, val_loader, cfg.device, id2label)\n",
        "        macro_f1 = report['macro avg']['f1-score']\n",
        "        print(f\"Epoch {epoch+1} -> Val Macro F1: {macro_f1:.4f}\")\n",
        "        if macro_f1 > best_macro:\n",
        "            best_macro = macro_f1\n",
        "            best_ckpt = os.path.join(cfg.output_dir, f\"best_macro_{macro_f1:.4f}.pt\")\n",
        "            torch.save({\"model_state_dict\": model.state_dict(), \"cfg\": cfg.__dict__}, best_ckpt)\n",
        "            print(\"Saved\", best_ckpt)\n",
        "    return best_ckpt\n",
        "\n",
        "# -------------------------\n",
        "# Pipeline orchestration\n",
        "# -------------------------\n",
        "def main():\n",
        "    # load\n",
        "    df = read_data(cfg.data_csv)\n",
        "    # filter tiny classes\n",
        "    cnt = df['label'].value_counts()\n",
        "    keep = cnt[cnt >= cfg.min_class_samples].index.tolist()\n",
        "    if len(keep) < len(cnt):\n",
        "        df = df[df['label'].isin(keep)].reset_index(drop=True)\n",
        "    print(\"Records:\", len(df), \"labels:\", df['label'].nunique())\n",
        "\n",
        "    # label mapping\n",
        "    labels_unique = sorted(df['label'].unique())\n",
        "    label_map = {lab:i for i,lab in enumerate(labels_unique)}\n",
        "    id2label = {v:k for k,v in label_map.items()}\n",
        "\n",
        "    # splits\n",
        "    data = df.to_dict(orient='records')\n",
        "    lablist = [label_map[r['label']] for r in data]\n",
        "    train_idx, test_idx = train_test_split(range(len(data)), test_size=0.15, random_state=cfg.seed, stratify=lablist)\n",
        "    train_idx, val_idx = train_test_split(train_idx, test_size=0.15, random_state=cfg.seed, stratify=[lablist[i] for i in train_idx])\n",
        "    train_records = [data[i] for i in train_idx]\n",
        "    val_records   = [data[i] for i in val_idx]\n",
        "    test_records  = [data[i] for i in test_idx]\n",
        "    print(\"split sizes:\", len(train_records), len(val_records), len(test_records))\n",
        "\n",
        "    ### NEW ###: Compute TF-IDF features\n",
        "    print(\"Computing TF-IDF features...\")\n",
        "    tfidf_vectorizer = TfidfVectorizer(\n",
        "        max_features=cfg.tfidf_dim,\n",
        "        ngram_range=(1, 2),\n",
        "        token_pattern=r'(?u)\\b\\w+\\b'\n",
        "    )\n",
        "    train_texts = [r['text'] for r in train_records]\n",
        "    train_tfidf = tfidf_vectorizer.fit_transform(train_texts)\n",
        "    val_tfidf = tfidf_vectorizer.transform([r['text'] for r in val_records])\n",
        "    test_tfidf = tfidf_vectorizer.transform([r['text'] for r in test_records])\n",
        "    print(\"TF-IDF matrix shape (train):\", train_tfidf.shape)\n",
        "\n",
        "    # vocabs\n",
        "    word2idx = build_token_vocab([r['text'] for r in train_records], cfg.min_token_freq)\n",
        "    char2idx = build_char_vocab([r['text'] for r in train_records], cfg.min_char_freq, cfg.max_chars_per_token)\n",
        "    print(\"Vocab sizes: tokens\", len(word2idx), \"chars\", len(char2idx))\n",
        "\n",
        "    # fasttext\n",
        "    ft_path = os.path.join(cfg.output_dir, \"fasttext.model\")\n",
        "    ft = train_or_load_fasttext([r['text'] for r in data], ft_path, dim=cfg.ft_dim, min_count=cfg.ft_min_count, epochs=cfg.ft_epochs)\n",
        "    emb_matrix = build_embedding_matrix(word2idx, ft, cfg.ft_dim)\n",
        "\n",
        "    # datasets ### MODIFIED to pass TF-IDF matrices ###\n",
        "    max_len = 64  # token length cap - tune as needed\n",
        "    train_ds = CharTokenDataset(train_records, train_tfidf, label_map, word2idx, char2idx, max_len, cfg.max_chars_per_token, cfg.aux_dim)\n",
        "    val_ds   = CharTokenDataset(val_records, val_tfidf, label_map, word2idx, char2idx, max_len, cfg.max_chars_per_token, cfg.aux_dim)\n",
        "    test_ds  = CharTokenDataset(test_records, test_tfidf, label_map, word2idx, char2idx, max_len, cfg.max_chars_per_token, cfg.aux_dim)\n",
        "\n",
        "    if cfg.use_sampler:\n",
        "        labels = [s[4] for s in train_ds.samples] ### MODIFIED ###: Label is now the 5th element (index 4)\n",
        "        cnts = Counter(labels)\n",
        "        sample_weights = [1.0 / cnts[l] for l in labels]\n",
        "        sampler = WeightedRandomSampler(sample_weights, num_samples=len(sample_weights), replacement=True)\n",
        "        train_loader = DataLoader(train_ds, batch_size=cfg.batch_size, sampler=sampler, num_workers=2)\n",
        "    else:\n",
        "        train_loader = DataLoader(train_ds, batch_size=cfg.batch_size, shuffle=True, num_workers=2)\n",
        "\n",
        "    val_loader = DataLoader(val_ds, batch_size=cfg.batch_size*2, shuffle=False, num_workers=2)\n",
        "    test_loader = DataLoader(test_ds, batch_size=cfg.batch_size*2, shuffle=False, num_workers=2)\n",
        "\n",
        "    # class weight vector (counts)\n",
        "    train_labels = [s[4] for s in train_ds.samples] ### MODIFIED ###: Label is now the 5th element (index 4)\n",
        "    cnts = np.array([Counter(train_labels).get(i,0) for i in range(len(label_map))])\n",
        "    class_weights = cnts.astype(np.float32)\n",
        "\n",
        "    # model\n",
        "    model = BiLSTMCharFastText(emb_matrix, char_vocab_size=len(char2idx), cfg=cfg, num_labels=len(label_map))\n",
        "    # quick shape sanity check\n",
        "    print(\"embedding dim:\", emb_matrix.shape[1])\n",
        "    print(\"char_cnn out dim actual:\", model.char_cnn.out_dim_actual)\n",
        "    sample_token_ids = torch.zeros((2, 8), dtype=torch.long)\n",
        "    sample_char_ids = torch.zeros((2, 8, cfg.max_chars_per_token), dtype=torch.long)\n",
        "    sample_aux = torch.zeros((2, cfg.aux_dim), dtype=torch.float)\n",
        "    sample_tfidf = torch.zeros((2, cfg.tfidf_dim), dtype=torch.float) ### NEW ###\n",
        "    with torch.no_grad():\n",
        "        logits_shape = model(sample_token_ids, sample_char_ids, sample_aux, sample_tfidf).shape ### MODIFIED ###\n",
        "    print(\"logits shape (sanity):\", logits_shape)\n",
        "    print(\"Model trainable params:\", sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
        "\n",
        "    # train\n",
        "    best_ckpt = train(train_loader, val_loader, model, cfg, class_weights, id2label)\n",
        "\n",
        "    # test eval\n",
        "    if best_ckpt and os.path.exists(best_ckpt):\n",
        "        ckpt = torch.load(best_ckpt, map_location=cfg.device)\n",
        "        model.load_state_dict(ckpt['model_state_dict'])\n",
        "        report, cm, y_true, y_pred = evaluate(model, test_loader, cfg.device, id2label)\n",
        "        print(\"\\nFinal Test Report:\")\n",
        "        print(classification_report(y_true, y_pred, target_names=list(id2label.values()), digits=4))\n",
        "        print(\"\\nConfusion Matrix:\\n\", cm)\n",
        "\n",
        "        # save predictions csv for inspection\n",
        "        rows = []\n",
        "        for rec, yt, yp in zip(test_records, y_true, y_pred):\n",
        "            rows.append({\"text\": rec['text'], \"label\": id2label[yt], \"pred\": id2label[yp]})\n",
        "        pd.DataFrame(rows).to_csv(os.path.join(cfg.output_dir, \"test_preds.csv\"), index=False)\n",
        "        print(\"Saved test_preds.csv\")\n",
        "    print(\"Done.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p9HdzDK2PNAA"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
